[
  {
    "objectID": "SHARK.html",
    "href": "SHARK.html",
    "title": "Shark_tab",
    "section": "",
    "text": "Extra wizard…\nTo be continued"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "EDUCATION",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "resume.html#resume",
    "href": "resume.html#resume",
    "title": "EDUCATION",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "posts/stats_final/index.html",
    "href": "posts/stats_final/index.html",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "",
    "text": "Blue carbon is attracting massive attention from governments, investors, and companies needing carbon accounting practices. Mangrove forests can sequester almost ten times more than boreal, temperate, or tropical forests, thanks to the mangrove biological characteristics. That is one of the main reasons they received the nickname of “The Superheroes of Climate Change.”\nMoreover, carbon stocks are the total amount of organic carbon stored within a system and are comprised of carbon pools, which fall into different bins: living aboveground living (live plants, including epiphytes); dead aboveground biomass (e.g., fallen branches); living belowground biomass (e.g., roots); and belowground carbon (sediment organic matter).\nWith that in mind, it is critical to understand the main factors to calculate the total mangrove carbon stocks’ in restoration practices. Additionally, to analyze and calculate an appropriate amount of tree samples and plot sizes to estimate the amount of carbon storage in the area.\n\n\n\nThe more you know: Mangroves (2020)\n\n\n\n\nBlue carbon accounting is a new strategy with few practical examples and evidence worldwide. Some methodologies align with the Verra non-profit organization VCS (Verified Carbon Standard) that helps us describe and outline the correct procedures to quantify the net greenhouse gas emission reductions and removals resulting from project activities implemented to restore tidal wetlands. For this case, the provided data uses equations and formulas designed on the VCS0032 and VCS0033 methodologies, which are aligned with the VCS."
  },
  {
    "objectID": "posts/stats_final/index.html#introduction",
    "href": "posts/stats_final/index.html#introduction",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "",
    "text": "Blue carbon is attracting massive attention from governments, investors, and companies needing carbon accounting practices. Mangrove forests can sequester almost ten times more than boreal, temperate, or tropical forests, thanks to the mangrove biological characteristics. That is one of the main reasons they received the nickname of “The Superheroes of Climate Change.”\nMoreover, carbon stocks are the total amount of organic carbon stored within a system and are comprised of carbon pools, which fall into different bins: living aboveground living (live plants, including epiphytes); dead aboveground biomass (e.g., fallen branches); living belowground biomass (e.g., roots); and belowground carbon (sediment organic matter).\nWith that in mind, it is critical to understand the main factors to calculate the total mangrove carbon stocks’ in restoration practices. Additionally, to analyze and calculate an appropriate amount of tree samples and plot sizes to estimate the amount of carbon storage in the area.\n\n\n\nThe more you know: Mangroves (2020)\n\n\n\n\nBlue carbon accounting is a new strategy with few practical examples and evidence worldwide. Some methodologies align with the Verra non-profit organization VCS (Verified Carbon Standard) that helps us describe and outline the correct procedures to quantify the net greenhouse gas emission reductions and removals resulting from project activities implemented to restore tidal wetlands. For this case, the provided data uses equations and formulas designed on the VCS0032 and VCS0033 methodologies, which are aligned with the VCS."
  },
  {
    "objectID": "posts/stats_final/index.html#data",
    "href": "posts/stats_final/index.html#data",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Data",
    "text": "Data\nThe data we will analyze to answer our questions are actual field measurements from mangrove trees over four replantation sites (2010, 2016, 2017, 2020). This data was provided directly to me by Silvestrum Climate Associates ®. This data is confidential, and there is limited access to the site.\n\nLimitations, spatial and temporal features.\nThe dataset contains a record for each tree height and canopy. With those measurements, Silvestrum has calculated the total tree carbon in (kg) and the total tree carbon per hectare in (Mg/ha). In this analysis, we will assume that the correct protocols and equations were followed for this purpose, and this analysis is dependent from those results.\nRegarding the dataset, the above and below ground carbon storage was assessed using a nested sub-sampling protocol along the 25m2, 77m2, and 154m2 transect following the Kauffman and Donato procedure. The equations for above and below ground biomass are defined by Comley and McGuiness.\nFurthermore, seeking for data limitations this mathematical calculations made by Silvestrum is a significant one, as this observations do not have the actual carbon samples to be analyzed. Another limitation is the possible man errors from the field sampling and the natural growth and variability from the mangroves.\nTo get you familiarized, the table below shows the first 5 rows of the data we will be studying. It has a total of 10,826 rows and 11 columns.\n\n\n\nTable 1. Full Mangrove Data Set (head)\n\n\nplantation_year\nplot\nheight_cm\ncanopy_width_1_cm\ncanopy_width_2_cm\ncrown_area_m2\ncd_chatting_m\nchatting_agb_kg\nchatting_agb_mg_ha\ncomley_mc_guinness_planted_bgb_kg\ncomley_mc_guinness_planted_bgb_mg_ha\ntotal_tree_kg\ntotal_tree_mg_ha\nchatting_agc_kg_c\nchatting_agc_mg_c_ha\ncomley_mc_guinness_planted_bgc_kg_c\ncomley_mc_guinness_planted_bgc_mg_c_ha\ntotal_tree_kg_c\ntotal_tree_mg_c_ha\nplot_size_m2\n\n\n\n\n2017\n8\n112\n59\n57\n0.264\n0.58\n0.494\n0.032\n0.456\n0.030\n0.950\n0.062\n0.237\n0.015\n0.178\n0.012\n0.415\n0.027\n153.938\n\n\n2017\n2_new\n280\n90\n101\n0.716\n0.96\n1.436\n0.571\n1.326\n0.527\n2.762\n1.099\n0.689\n0.274\n0.517\n0.206\n1.206\n0.480\n25.133\n\n\n2017\n2_new\n275\n114\n119\n1.066\n1.17\n2.198\n0.874\n2.028\n0.807\n4.226\n1.681\n1.055\n0.420\n0.791\n0.315\n1.846\n0.734\n25.133\n\n\n2017\n2_new\n274\n95\n102\n0.762\n0.99\n1.534\n0.611\n1.416\n0.564\n2.951\n1.174\n0.737\n0.293\n0.552\n0.220\n1.289\n0.513\n25.133\n\n\n2017\n2_new\n264\n133\n104\n1.103\n1.19\n2.279\n0.907\n2.104\n0.837\n4.383\n1.744\n1.094\n0.435\n0.820\n0.326\n1.914\n0.762\n25.133\n\n\n2017\n2_new\n264\n133\n104\n1.103\n1.19\n2.279\n0.907\n2.104\n0.837\n4.383\n1.744\n1.094\n0.435\n0.820\n0.326\n1.914\n0.762\n25.133\n\n\n\n\n\n\n\n\n\n\nTable 2. Total mangrove Dataset summary\n\n\nplantation_year\nPlot Count\nSample Count\nTotal Carbon (mg/ha)\nTotal Carbon Variance (mg/ha)\n\n\n\n\n2010\n7\n746\n45.725\n0.1072349\n\n\n2017\n27\n4126\n242.993\n0.1302739\n\n\n2019\n8\n998\n22.994\n0.0185163\n\n\n2020\n46\n4956\n87.824\n0.0159968\n\n\n\n\n\n\n\nAs we can see in the table above, the 2017 plantation year has an exciting number of samples, different sample plot sizes, and high variation of total carbon in Mg per hectare. We will focus on this year to answer our questions.\nThe table #3 contains only information from the 2017 plantation year. We will use this filter to analyse the different questions, and interpret our results.\n\n\nShow code\nmangrove_2017 &lt;- mangrove_df |&gt; \n  filter(plantation_year == 2017) |&gt; \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha)\nmangrove_2017 &lt;- mangrove_2017[!(mangrove_2017$total_tree_mg_c_ha== 1.224),]\n\n#Statistical Table with the selected variables for question #1\nmangrove_plots_2017 &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nmangrove_plots_2017 %&gt;% \n  kbl(align=rep('c', 5),\n      caption =\"Table 3. Plot summary of the 2017 plantation year\",\n      position = \"left\") %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 3. Plot summary of the 2017 plantation year\n\n\nplot_size_m2\ntot_samples\ntot_plot_count\nmean_tree_c_ha\nsd_tree_c_ha\n\n\n\n\n25m2\n323\n4\n0.3708173\n0.3035103\n\n\n77m2\n336\n5\n0.0624196\n0.0651300\n\n\n154m2\n3466\n18\n0.0291466\n0.0297558\n\n\n\n\n\n\n\nShow code\n#Statistical table of each unique plot to analyse variability in the question #2\nplots_2017 &lt;- mangrove_2017 |&gt; \n  group_by(plot, plot_size_m2) |&gt; \n  summarise(sample_count = n(),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha))\n\n\nNow let us take a closer look at the effect of sample plot size and the total tree carbon (Mg/ha).\n\n\nShow code\n#Boxplot\nboxplot_2017 &lt;- mangrove_2017 |&gt;\n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_boxplot() +\n  labs(title = \"Tree Carbon vs Plot Type\",\n       subititle = \"Plantation Year - 2017\",\n       x = \"Plot Size (m2)\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") \n\nggplotly(boxplot_2017)"
  },
  {
    "objectID": "posts/stats_final/index.html#question-1",
    "href": "posts/stats_final/index.html#question-1",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 1",
    "text": "Question 1\nThe graph above shows a visual difference in carbon means between the 25m2 sample plot and the other two sample plots. So let us analyze the relationship between the 77m2 and the 154m2, with a hypothesis test and see if the means in the total carbon per tree (Mg/ha) are different.\nAnalysis: Hypothesis Testing between 77m2 vs 154m2\nThe Null Hypothesis - The Total Carbon per Tree (Mg/ha) mean in the Sample Size of 77m2 is no different from the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} = 0\\]\nAlternative Hypothesis- The Total Carbon per Tree (Mg/ha) in the sample size of 77m2 is different from the one the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} \\neq 0\\]\nCalculate the Point Estimate and the Standard Error\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\\]\n\n\nShow code\n#Calculate Point Estimate\npoint_est_2 = (mangrove_plots_2017$mean_tree_c_ha[2] - mangrove_plots_2017$mean_tree_c_ha[3])\n\n#Define the Standard Error\nn_77m = mangrove_plots_2017$tot_samples[2]\ns_77m =  mangrove_plots_2017$sd_tree_c_ha[2]\nn_154m = mangrove_plots_2017$tot_samples[3]\ns_154m =  mangrove_plots_2017$sd_tree_c_ha[3]\n\nprint(paste(\"Point Estimate:\", round(point_est_2,5)))\n\n\n[1] \"Point Estimate: 0.03327\"\n\n\nShow code\nSE_2 = as.numeric(sqrt(s_77m^2/n_77m + s_154m^2/n_154m))\nprint(paste(\"Standard Error:\", round(SE_2,5)))\n\n\n[1] \"Standard Error: 0.00359\"\n\n\nCalculate the Z-Score \\[z_{score}=\\frac{\\text { point estimate }-\\text { null value }}{S E}\\]\n\n\nShow code\nz_score_2 &lt;- (point_est_2 - 0) / SE_2\nz_score_2\n\n\n[1] 9.271106\n\n\nThe Z-Score will tell us that the the observed difference between a sample plot of 77m2 and 154m2 is 9.3 standard deviations above our null hypothesis of “zero difference” of our dependent variable in Tree Carbon (Mg/ha).\nCalculate the p-value and run a t - test\n\n\nShow code\noption2_ttest &lt;- t.test(mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969], mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938])\noption2_ttest\n\n\n\n    Welch Two Sample t-test\n\ndata:  mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969] and mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938]\nt = 9.2711, df = 348.68, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.02621446 0.04033169\nsample estimates:\n mean of x  mean of y \n0.06241964 0.02914657 \n\n\n\nResults:\nWith this results we can reject the null as the p-value is telling us there it exists a statistical significant difference between the means of carbon storage between the sample plot size of 77m2 to the sample size of 154m2.\nWe are 95% confident that the true value of the difference in tree carbon across the two plot sizes lies between 0.0262 - 0.0403 Mg per hectare.\nThis graph summarizes all the important take away.\n\n\nShow code\ncrit_val_2 = qnorm(0.025, lower.tail = F)\nci_lower_2 = round(point_est_2 - crit_val_2*SE_2, 3)\nci_upper_2 = round(point_est_2 + crit_val_2*SE_2, 2)\n\nmangrove_2017 |&gt;\n  filter(plot_size_m2 %in% c(76.969, 153.938)) |&gt; \n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_point(alpha = 0.5) +\n  stat_summary(fun= \"mean\", aes(shape= \"mean\"),color = \"darkblue\", geom = \"pointrange\",size = 1.5) +\n  labs(title = \"Tree mg C vs Plot Type\",\n       x = \"Plot Size\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_hline(aes(yintercept = 0.026, linetype = \"Lower CI\"), color = \"gray50\", size = .5) +\n  geom_hline(aes(yintercept = 0.04, linetype = \"Upper CI\"), color = \"gray10\", size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Confidence Intervals\", values = c(2, 2), \n                        guide = guide_legend(override.aes = list(color = c(\"gray50\", \"gray10\"))))\n\n\n\n\n\n\n\nImportant take away from Question 1\nAs we can see in the graph, both means are close to each other, but they are statistically significantly different. With this data analysis and results, we can help the actual client better understand their plot sampling sizes, concluding that smaller plot sizes are getting a larger carbon estimate."
  },
  {
    "objectID": "posts/stats_final/index.html#question-2",
    "href": "posts/stats_final/index.html#question-2",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 2:",
    "text": "Question 2:\nLet analyze why the smaller plots are getting a larger estimate of total carbon per hectare.\nFor this purpose, we will create a new table and graph to easily visualize what the density per plot type (Number of trees per m2) looks like:\n\n\nShow code\n#Changing the column to numeric\nmangrove_df$plot_size_m2_num &lt;- as.numeric(mangrove_df$plot_size_m2)\n\ndensity_df &lt;- read_csv(\"/Users/javipatron/Documents/MEDS/Courses/eds222/homework/eds222-finalproject/data/clean_monitoring_data_javier.csv\") |&gt; \n  clean_names() |&gt; \n  filter(plantation_year == 2017) |&gt; \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha) |&gt; \n  group_by(plot, plot_size_m2) |&gt; \n  summarise(sample_count = n(),\n            mean_hight = mean(height_cm),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(density = sample_count/plot_size_m2)\n\nhead(density_df) %&gt;% \n  kbl(align=rep('c', 5),\n      caption =\"Table 4. Density Table per Plot\",\n      position = \"left\") %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 4. Density Table per Plot\n\n\nplot\nplot_size_m2\nsample_count\nmean_hight\nmean_tree_c\nmean_carbon_mg_ha\nsd_tree_c\nsd_tree_c_ha\ndensity\n\n\n\n\n12\n153.938\n298\n96.80872\n0.3211477\n0.0208591\n0.2032983\n0.0132004\n1.935844\n\n\n13\n153.938\n183\n84.13115\n0.3955847\n0.0257268\n0.2680318\n0.0174041\n1.188790\n\n\n14\n153.938\n160\n108.31875\n0.5805625\n0.0377312\n0.3058017\n0.0198459\n1.039379\n\n\n16\n153.938\n288\n107.60069\n0.3394236\n0.0220590\n0.3061588\n0.0198987\n1.870883\n\n\n17\n153.938\n223\n105.67265\n0.3120404\n0.0202825\n0.2867950\n0.0186237\n1.448635\n\n\n18\n76.969\n91\n97.43956\n0.3579780\n0.0465385\n0.4200426\n0.0545699\n1.182294\n\n\n\n\n\n\n\nShow code\ndensity_graph &lt;- ggplot(density_df, aes(x= as.factor(plot_size_m2), y= density, fill = as.factor(plot_size_m2))) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Density of trees per plot size\",\n       x= \"Plot Size (m2)\",\n       y = \"Density (Tree/m2)\",\n       col = \"Plot Size (m2)\") +\n  scale_fill_discrete(name = \"Plot Type\")\n\nggplotly(density_graph)\n\n\n\n\n\n\nAs we can see in this graph, the plot sizes significantly differ in density (Number of trees per m2). Plots of 25 m2 have a density of 3.2 trees per m2, and plots of 77 m2 have a density of 0.873 trees per m2. To avoid possible bias, this is a point to highlight for future projects.\nNow, to visually show the effect of plot sizes on the total carbon per tree, let us take a deeper look at these two histograms.\nThis Histogram shows how much variance we have between our plots.\n\n\nShow code\nggplot(plots_2017, aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density)), color = \"gray20\", fill = \"darkolivegreen\", alpha = 0.7) +\n  geom_density(col = \"gray30\", alpha = 0.15, fill = \"green\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"All Plots\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Denisity Plots\")\n\n\n\n\n\nWe have a skew right histogram with a long tail. Now let us analyze this same plot, but separating per plot type, to see who is responsible for those outliers.\n\n\nShow code\nhistogram_plot &lt;- plots_2017 |&gt; \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |&gt; \nggplot(aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Per Plot\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Density Plots\")\n\nggplotly(histogram_plot)\n\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 25m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 25.133) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"red\", color = \"gray30\", alpha = 0.6) +\n  geom_density(col = \"gray30\", alpha = 0.3, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"25m2\")\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 77m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 76.969) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightgreen\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"77m2\")\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 154m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 153.938) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightblue\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"154m2\")\n\n\n\n\n\n\nImportant take away from Question 2:\nThe 25m2 plots have the highest carbon per hectare, a higher density of trees per m2, and a higher variance in their calculations. Overall, the plot of 25m2 has a more considerable amount of carbon stock per hectare, however, with high variability, which is not very confident for the methodologies and the Verified Carbon Standard protocols.\nAn important take away to ask and further analyse with Silvestrum, is why the 25m2 samples are having the highest trees records and the highest tree density in comparison to the 77m2 and 154m2."
  },
  {
    "objectID": "posts/stats_final/index.html#question-3",
    "href": "posts/stats_final/index.html#question-3",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 3:",
    "text": "Question 3:\nAs we can see a significant effect of sample plot size in the calculated Total Carbon per Tree, it would be helpful to put our shoes in the field, and estimate an appropriate sample size to determine the amount of carbon storage in this same area.\nFirst, does a larger number of samples decrease the variance in our calculations?\n\n\nShow code\nvariance_graph &lt;- ggplot(plots_2017, aes(x= sample_count, y = (sd_tree_c_ha^2))) +\n  geom_point(aes(color = plot_size_m2)) +\n  geom_smooth(method = lm,\n              color = \"cyan4\",\n              se = F) +\n  labs(title = \"Tree Carbon variance per Sample plot\",\n       x = \"Number of samples per Plot\",\n       y = \"Variance of Carbon in each plot\")\n\nggplotly(variance_graph)\n\n\n\n\n\n\nThis graph shows that a larger number of samples decrease the variance in our calculations. But is there a way we can define our “sweet spot” of sample sizes? In the case of the 154m2 the 3,466 did lower the variance, but it sounds like a lot of work!\nFurthermore, to correctly create a statistical approach, define an “ideal” sample count, and help guide sample sizes in future projects. We will use this data, and the pwr()` package to estimate the number of samples needed to obtain a high power of confidentiality. In other words, depending on the means overlaps for each plot size, we will need more or less samples to get the 95% confidence that we are looking for.\nFirst, we need to understand and visualize the overlaps that we currently have between our sample plots.\n\n\nShow code\n#Total Tree Carbon (mg/ha)\nhistogram_all &lt;- mangrove_2017 |&gt; \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Histogram of Carbon per Tree (Mg/ha)\",\n       subtitle = \"2017\")\n\nggplotly(histogram_all)\n\n\n\n\n\n\nThe histogram above shows the density between all tree measurements per plot size. The important part is to see the overlap between the sample types to calculate the power.\nAnother way to visualize this distribution of a single continuous variable of total tree carbon (Mg/ha) is by dividing the plot sizes into bins and using the frequency polygons geom_freqpoly()` function to display the counts with lines. Frequency polygons are more suitable when you want to compare the distribution across the levels of a categorical variable.\n\n\nShow code\n#Plotting just the lines\nggplot(mangrove_2017, aes(x=total_tree_mg_c_ha, color = plot_size_m2)) +\n  geom_freqpoly(aes(y = stat(density))) +\n  labs(title = \"Frequency Polygon\",\n    subtitle = \"Invisible Histogram to see overlaps between Sample Size\",\n       x = \"Total Carbon per Tree (Mg/ha)\",\n       y = \"Density\")\n\n\n\n\n\nThanks to the Frequency polygon graph, we can see clearly the overlap between 77m2 and 154m2 plot types, which are way more correlated than the overlap with 25m2.\nThirdly, we will use the pwr.t.test function to estimate the number of counts we need from each sample plot to have a power of 90%.\n\n\nShow code\n# Create the objects with the sd of each group\nsd_25m = mangrove_plots_2017[[1,5]]\nsd_77m = mangrove_plots_2017[[2,5]]\nsd_154m =  mangrove_plots_2017[[3,5]]\nmean_25m = mangrove_plots_2017[[1,4]]\nmean_77m = mangrove_plots_2017[[2,4]]\nmean_154m = mangrove_plots_2017[[3,4]]\n\n\n\nA) Power between 25m2 and 154m2.\n\n\nShow code\nmean_difference_a = mean_25m - mean_154m\nd25_154 = as.numeric(sqrt(sd_25m^2/2 + sd_154m^2/2))\n\neffect_size_a = mean_difference_a / d25_154\n\npower_test_1 &lt;- pwr.t.test(d = round(effect_size_a,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_1)\n\n\n\n     Two-sample t test power calculation \n\n              n = 11.46789\n              d = 1.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBecause the means between 25m2 and 154m2 are so different, we will only need twelve samples to have a power of 95%.\n\n\nB) Power between 77m2 and 154m2\n\n\nShow code\nmean_difference_b = mean_77m - mean_154m\nd77_154 = as.numeric(sqrt(sd_77m^2/2 + sd_154m^2/2))\neffect_size_b = mean_difference_b / d77_154\n\n\npower_test_2 &lt;- pwr.t.test(d = round(effect_size_b,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_2)\n\n\n\n     Two-sample t test power calculation \n\n              n = 60.64108\n              d = 0.66\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nShow code\nplot(power_test_2)\n\n\n\n\n\nIn this case, as the means between 77m2 and 154m2 are closer together, we can estimate that with Sixty samples, we could have a power of 95%."
  },
  {
    "objectID": "posts/stats_final/index.html#conclusion",
    "href": "posts/stats_final/index.html#conclusion",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Conclusion",
    "text": "Conclusion\nFurther research needs to be conducted with other mangrove reforestation sites to increase our confidentiality regarding the correct selection of plot sizes and tree measurements.\nIn order to increase our probabilities and reduce possible bias for future projects, its important to follow the VCS methodologies regarding uncertainty. A random stratified sampling could be a better approach than measuring every single tree in the reforestation. Nevertheless, this study supports the efforts put in place with the 77m2 and 154m2 sample plots as they show a general low variance in their calculations, and a significant lower amount of total measurement specially for the 77m2 plot, saving valuable time in the field. Assuming that all calculation are right in the initial provided data, this study successfully supports the initial question of what is an “ideal” number of samples to reduce extra effort, and still be certain of your estimates (see results of Question 3).  \nOverall, Mangrove forests are an incredible biological ecosystem; this type of small analysis and efforts definitely help the nature-based solutions get closer to unimaginable changes in future projects. As this analysis has proved, there are some questions being answered, but there are still some other important opportunities to be discovered regarding the structure of sampling according to the methodologies, which they seem to be the key factor for certain results.\nPS. Thanks Silvestrum Climate Associates for sharing this collection of data samples data from one of your real reforestation sites!"
  },
  {
    "objectID": "posts/stats_final/index.html#references",
    "href": "posts/stats_final/index.html#references",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "References",
    "text": "References\n\nVerraadmin (2020) First Blue Carbon Conservation Methodology expected to scale up finance for Coastal Restoration & Conservation Activities, Verra. Available at: https://verra.org/first-blue-carbon-conservation-methodology-expected-to-scale-up-finance-for-coastal-restoration-conservation-activities (Accessed: December 8, 2022). \nVM0033 methodology for tidal wetland and Seagrass Restoration, v2.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0033-methodology-for-tidal-wetland-and-seagrass-restoration-v2-0/ (Accessed: December 8, 2022). \nVM0032 methodology for the adoption of sustainable grasslands through adjustment of fire and grazing, v1.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0032-methodology-for-the-adoption-of-sustainable-grasslands-through-adjustment-of-fire-and-grazing-v1-0/ (Accessed: December 8, 2022). \nMcGuinness, K. (2005) Above- and below-ground biomass, and allometry, of four common northern ..., Research Gate. Available at: https://www.researchgate.net/publication/248899546_Above-_and_below-ground_biomass_and_allometry_of_four_common_northern_Australian_mangroves (Accessed: December 9, 2022).\nKauffman, and Donato, (2012) Center for International Forestry Research - CIFOR, Cifor. Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nCenter for International Forestry Research - CIFOR (no date). Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nThe more you know: Mangroves (2020) Ripley’s Aquarium of Canada. Available at: https://www.ripleyaquariums.com/canada/the-more-you-know-mangroves/ (Accessed: December 8, 2022).\n\n\nSupporting Figures\n\nTest the power results with random samples\nHere, we have created a power table with the means and SD of your random table and run a power analysis with the selected random samples to test my results\nTest your results 25m2 vs 154m2 by changing the number of samples!\n\n\nShow code\nsamples = 12\nrandom_table_test &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  slice_sample(n = samples)\n\npower_table_test &lt;- random_table_test |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_25m = power_table_test[[1,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_25m = power_table_test[[1,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_a = (random_mean_25m - random_mean_154m) / (as.numeric(sqrt(random_sd_25m^2/2 + random_sd_154m^2/2)))\n\n\nrandom_power_test &lt;- pwr.t.test(d = random_effect_size_a, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\nprint(effect_size_a)\n\n\n[1] 1.584427\n\n\nShow code\nprint(random_effect_size_a)\n\n\n[1] 2.574916\n\n\nShow code\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = mean_difference_a, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = (random_mean_25m - random_mean_154m), \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_a,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_25m - random_mean_154m),2), \"(Mg/ha)\", \"\\nPower:\", round(random_power_test$power,2)))\n\n\n\n\n\nTest your results 77m2 vs 154m2 with 50 samples\n\n\nShow code\nsamples = 60\nrandom_table_test &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  slice_sample(n = samples)\n\npower_table_test &lt;- random_table_test |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_77m = power_table_test[[2,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_77m = power_table_test[[2,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_b = (random_mean_77m - random_mean_154m) / (as.numeric(sqrt(random_sd_77m^2/2 + random_sd_154m^2/2)))\n\nrandom_power_test_b &lt;- pwr.t.test(d = random_effect_size_b, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\n\nprint(effect_size_b)\n\n\n[1] 0.6571471\n\n\nShow code\nprint(random_effect_size_b)\n\n\n[1] 0.9197665\n\n\nShow code\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = effect_size_b, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = random_effect_size_b, \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means Difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_b,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_77m - random_mean_154m),2), \"(Mg/ha)\",\"\\nPower:\", round(random_power_test_b$power,2)))\n\n\n\n\n\n\n\nC) Power of just 154m2\n\n\nShow code\nd_154 = as.numeric(sqrt(sd_154m^2/2))\neffect_size_c = mean_154m / d77_154\n\n\npower_test_c &lt;- pwr.t.test(d = round(effect_size_c,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_c)\n\n\n\n     Two-sample t test power calculation \n\n              n = 78.23115\n              d = 0.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nShow code\nplot(power_test_c)\n\n\n\n\n\n\n\nD) Power of just 77m2\n\n\nShow code\nd_77 = as.numeric(sqrt(sd_77m^2/2))\neffect_size_d = mean_77m / d_77\n\n\npower_test_d &lt;- pwr.t.test(d = round(effect_size_d,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_d)\n\n\n\n     Two-sample t test power calculation \n\n              n = 15.08404\n              d = 1.36\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nE) Power of just 25m2\n\n\nShow code\nd_25 = as.numeric(sqrt(sd_25m^2/2))\neffect_size_e = mean_25m / d_25\n\n\npower_test_e &lt;- pwr.t.test(d = round(effect_size_e,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_e)\n\n\n\n     Two-sample t test power calculation \n\n              n = 9.759221\n              d = 1.73\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "posts/texas_outage/index.html",
    "href": "posts/texas_outage/index.html",
    "title": "Texas Outage",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nThe tasks in this post are: - Estimating the number of homes in Houston that lost power as a result of the first two storms\n- Investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau.\n\n\n\nLoad vector/raster data\n\nSimple raster operations\n\nSimple vector operations\n\nSpatial joins"
  },
  {
    "objectID": "posts/texas_outage/index.html#overview",
    "href": "posts/texas_outage/index.html#overview",
    "title": "Texas Outage",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nThe tasks in this post are: - Estimating the number of homes in Houston that lost power as a result of the first two storms\n- Investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau.\n\n\n\nLoad vector/raster data\n\nSimple raster operations\n\nSimple vector operations\n\nSpatial joins"
  },
  {
    "objectID": "posts/texas_outage/index.html#data",
    "href": "posts/texas_outage/index.html#data",
    "title": "Texas Outage",
    "section": "Data",
    "text": "Data\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nRoads\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shape file of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouses\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "posts/texas_outage/index.html#assignment",
    "href": "posts/texas_outage/index.html#assignment",
    "title": "Texas Outage",
    "section": "Assignment",
    "text": "Assignment\n\nFind locations of blackouts\nFor improved computational efficiency and easier interoperability with sf, we will use the stars package for raster handling.\n\n\n\nShow code\n# Setting my filepaths\nrootdir &lt;- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndatatif &lt;- file.path(rootdir,\"data\",\"VNP46A1\")\ndata &lt;- file.path(rootdir,\"data\")\n\n#Creating the names for each file\nnightlight1 &lt;- 'VNP46A1.A2021038.h08v05.001.2021039064328.tif' \nnightlight2 &lt;- 'VNP46A1.A2021038.h08v06.001.2021039064329.tif' \nnightlight3 &lt;- 'VNP46A1.A2021047.h08v05.001.2021048091106.tif'\nnightlight4 &lt;- 'VNP46A1.A2021047.h08v06.001.2021048091105.tif'\n  \n#Downloading the raster data to a star object\none &lt;- read_stars(file.path(datatif, nightlight1))\ntwo &lt;- read_stars(file.path(datatif, nightlight2))\nthree &lt;- read_stars(file.path(datatif,nightlight3))\nfour &lt;- read_stars(file.path(datatif,nightlight4))\n\n\n\n\nShow code\n#Combine tiles to have the full size\nlights_off &lt;- st_mosaic(one,two)\nlights_on &lt;- st_mosaic(three,four)\n\nplot(lights_off, main= \"Satellite Image of Houston Feb 7th\")\n\n\ndownsample set to 6\n\n\n\n\n\nShow code\nplot(lights_on, main= \"Satellite Image of Houston Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nCreate a blackout mask\n\nFind the change in night lights intensity (presumably) caused by the storm\nResponse: The change on the mean from the second date (All lights) goes from 13.86 down 12.13939, on the first date (Outrage)\nReclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nAssign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\n\n\n\nShow code\n#Create a new raster that has the difference of the values from the Feb 7th (Lights Off) raster, and the 16th (Lights On) raster. This will just have the difference of the attribute value on each pixel\n\nraster_diff &lt;-  lights_off - lights_on\n\nplot(raster_diff, main= \"Difference in light intensity from Feb 7th and Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nVectorize the mask\n\nUsing st_as_sf() to vectorize the blackout mask\nFixing any invalid geometries by using st_make_valid\n\n\n\nShow code\n# Converts the non-spatial star object (.tif) file to an sf object. An sf object will have an organizing structure that will have the geom column and the layer as attribues\nblackout &lt;- st_as_sf(raster_diff)\nsummary(blackout)\n\n\n VNP46A1.A2021038.h08v05.001.2021039064328.tif          geometry    \n Min.   :  200.0                               POLYGON      :24950  \n 1st Qu.:  262.0                               epsg:4326    :    0  \n Median :  359.0                               +proj=long...:    0  \n Mean   :  543.8                                                    \n 3rd Qu.:  563.0                                                    \n Max.   :65512.0                                                    \n\n\n\n\nCrop the vectorized map to our region of interest.\n\nDefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nTurn these coordinates into a polygon using st_polygon\nConvert the polygon into a simple feature collection using st_sfc() and assign a CRS\n\nhint: because we are using this polygon to crop the night lights data it needs the same CRS\n\nCrop (spatially subset) the blackout mask to our region of interest \nRe-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nShow code\n#Create vectors for the desired CRS\nlon = c(-96.5, -96.5, -94.5, -94.5,-96.5)\nlat = c(29, 30.5, 30.5, 29, 29)\n\n\n#Create an array or matrix with those vectors \ncoordinates_array &lt;-cbind(lon,lat)\n\n#Creating a polygon with the st_polygon function but the coordinates_array has to be in the form of a list so the st_polygon can read it.\nhouston_polygon &lt;- st_polygon(list(coordinates_array))\n\n# Create a simple feature geometry list column, and add coordinate reference system so you can \"speak\" the same language than your recent blackout object\nhouston_geom &lt;- st_sfc(st_polygon(list(coordinates_array)), crs = 4326)\n\n# Indexing or Cropping the blackout sf object with just the houston geometery polygon. \n\nhouston_blackout_subset &lt;- blackout[houston_geom,]\n\n# Re-project the cropped blackout dataset with a new CRS (EPSG:3083) (NAD83 / Texas Centric Albers Equal Area)\nhouston_projection &lt;- st_transform(houston_blackout_subset,\"EPSG:3083\")\n\n\n\n\nShow code\nplot(houston_blackout_subset, main = \"Blackout pixels in Houston\")\n\n\n\n\n\n\n\nExclude highways from blackout mask\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\n\nDefine SQL query\nLoad just highway data from geopackage using st_read\nReproject data to EPSG:3083\nIdentify areas within 200m of all highways using st_buffer\nFind areas that experienced blackouts that are further than 200m from a highway\n\n\n\nShow code\n#Reading the data with format .gpkg\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(file.path(data, \"gis_osm_roads_free_1.gpkg\"), query = query)\n\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway''\nfrom data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n\nShow code\n#Transform that new sf object with the CRS we have been using, so we stay in the same space.\nhighways_3083 &lt;- st_transform(highways, \"EPSG:3083\")\n\n\n\n\nShow code\n#Create a buffer that contains all the roads, including the 200 meters across the roads.\n# BUT, to make a buffer object we need to first create a union of all those rows (highways), so we can use the st_buffer function...\nhighways_union &lt;- st_union(highways_3083)\nhighways_buffer &lt;- st_buffer(x = highways_union,\n          dist = 200)\n\nplot(highways_buffer, main = \"Highways in Houston\")\n\n\n\n\n\nShow code\n# Use the buffer to subtract those entire pixels or \"rows\" from the houston projection sf object.\nhouston_out &lt;- houston_projection[highways_buffer, op = st_disjoint]\nplot(houston_out,\n     main= \"Houston Without the Roadlights\")\n\n\n\n\n\n\n\nFind homes impacted by blackouts\n\nLoad buildings dataset using st_read and the following SQL query to select only residential buildings\nhint: reproject data to EPSG:3083\n\n\n\nShow code\n#Read the data\nquery2 &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings &lt;- st_read(file.path(data, \"gis_osm_buildings_a_free_1.gpkg\"), query = query2)\n\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')'\nfrom data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\n\n\nFind homes in blackout areas\n\nFilter to homes within blackout areas.\nTip: You can use [ ] option to subtract those geometries from the building sf object that do not correspond to the Houston area. Or you can use the st_filter.\nCount number of impacted homes.\nTip: The total number of impacted houses when using st_filter or [ ] is 139,148. This could vary depending on how the st_filter considers the limits and borders of the geometries. The default method when using st_filter is st_intersects.\n\n\n\nShow code\n#Count number of houses\ndim(homes_blackout_join)[1]\n\n\n[1] 139148\n\n\n\n\nInvestigate socioeconomic factors\nLoad ACS data\n\nUse st_read() to load the geodatabase layers\nGeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\nIncome data is stored in the X19_INCOME layer\nSelect the median income field B19013e1\nhint: reproject data to EPSG:3083\n\n\n\n\nShow code\n#Read the data and understand the data layers\nst_layers(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n\nDriver: OpenFileGDB \nAvailable layers:\n                            layer_name geometry_type features fields crs_name\n1                      X01_AGE_AND_SEX            NA     5265    719     &lt;NA&gt;\n2                             X02_RACE            NA     5265    433     &lt;NA&gt;\n3        X03_HISPANIC_OR_LATINO_ORIGIN            NA     5265    111     &lt;NA&gt;\n4                         X04_ANCESTRY            NA     5265    665     &lt;NA&gt;\n5         X05_FOREIGN_BORN_CITIZENSHIP            NA     5265   1765     &lt;NA&gt;\n6                   X06_PLACE_OF_BIRTH            NA     5265   1221     &lt;NA&gt;\n7                        X07_MIGRATION            NA     5265   1793     &lt;NA&gt;\n8                        X08_COMMUTING            NA     5265   2541     &lt;NA&gt;\n9  X09_CHILDREN_HOUSEHOLD_RELATIONSHIP            NA     5265    263     &lt;NA&gt;\n10      X10_GRANDPARENTS_GRANDCHILDREN            NA     5265    373     &lt;NA&gt;\n11    X11_HOUSEHOLD_FAMILY_SUBFAMILIES            NA     5265    781     &lt;NA&gt;\n12      X12_MARITAL_STATUS_AND_HISTORY            NA     5265    759     &lt;NA&gt;\n13                       X13_FERTILITY            NA     5265    399     &lt;NA&gt;\n14               X14_SCHOOL_ENROLLMENT            NA     5265    779     &lt;NA&gt;\n15          X15_EDUCATIONAL_ATTAINMENT            NA     5265    715     &lt;NA&gt;\n16         X16_LANGUAGE_SPOKEN_AT_HOME            NA     5265    871     &lt;NA&gt;\n17                         X17_POVERTY            NA     5265   3941     &lt;NA&gt;\n18                      X18_DISABILITY            NA     5265    893     &lt;NA&gt;\n19                          X19_INCOME            NA     5265   3045     &lt;NA&gt;\n20                        X20_EARNINGS            NA     5265   2185     &lt;NA&gt;\n21                  X21_VETERAN_STATUS            NA     5265    565     &lt;NA&gt;\n22                     X22_FOOD_STAMPS            NA     5265    243     &lt;NA&gt;\n23               X23_EMPLOYMENT_STATUS            NA     5265   1625     &lt;NA&gt;\n24         X25_HOUSING_CHARACTERISTICS            NA     5265   4415     &lt;NA&gt;\n25                X27_HEALTH_INSURANCE            NA     5265   1593     &lt;NA&gt;\n26       X28_COMPUTER_AND_INTERNET_USE            NA     5265    385     &lt;NA&gt;\n27           X29_VOTING_AGE_POPULATION            NA     5265     35     &lt;NA&gt;\n28                      X99_IMPUTATION            NA     5265    783     &lt;NA&gt;\n29             X24_INDUSTRY_OCCUPATION            NA     5265   2107     &lt;NA&gt;\n30                  X26_GROUP_QUARTERS            NA     5265      3     &lt;NA&gt;\n31                 TRACT_METADATA_2019            NA    35976      2     &lt;NA&gt;\n32         ACS_2019_5YR_TRACT_48_TEXAS Multi Polygon     5265     15    NAD83\n\n\n\n\nDetermine which census tracts experienced blackouts.\n\nJoin the income data to the census tract geometries\nhint: make sure to join by geometry ID\nSpatially join census tract data with buildings determined to be impacted by blackouts\nFind which census tracts had blackouts\n\n\n\nShow code\n#Create a big sf that has the income information and its geometry\nincome_geom &lt;- left_join(texas_geom, texas_income, by = \"GEOID_Data\")\n\n#Create an new sf that adds the income information to just the blackout sf object that we previously had. In this step I learned that if you use st_join will give you a different number and result than st_filter or [ ]\n\n#blackout_income_join &lt;- st_join(homes_blackout_join, income_geom)\nblackout_join &lt;- income_geom[homes_blackout_join,]\n\n#Print those Census Tracts a had blackouts. Im printing the number and the name\nlength(unique(blackout_join$NAMELSAD))\n\n\n[1] 711\n\n\nShow code\n#unique(blackout_join$NAMELSAD)\n\n\n\n\nCompare incomes of impacted tracts to unimpacted tracts.\n\nCreate a map of median income by census tract, designating which tracts had blackouts\nPlot the distribution of income in impacted and unimpacted tracts\nWrite approx. 100 words summarizing your results and discussing any limitations to this study\n\n\n\nShow code\n#Creating a list of tracts and counties that were affected by the blackout\nblackout_tracts &lt;- unique(blackout_join$TRACTCE)\nblackout_counties &lt;- unique(blackout_join$COUNTYFP)\n\n#Creating a Data Frames that includes only the rows that have the county affected by using the geom from blackout_tracts. One with the Counties and the other one with the Tracts.\ntracts_affected &lt;- income_geom |&gt; \n  filter(TRACTCE %in% blackout_tracts)\n\ncounties_affected &lt;- income_geom |&gt; \n  filter(COUNTYFP %in% blackout_counties)\n\n#Create a map were the base is the counties of Houston, then fill the color with the income_median column we created with \"B19013e1\", and then highlight the counties that were had building affected from our dataset\nmap &lt;- tm_shape(counties_affected) +\n  tm_fill(col = \"income_median\", palette = \"BrBG\") +\n  tm_borders() +\n  tm_shape(blackout_join) +\n  tm_fill(col = \"pink\", alpha= 0.3) +\n  tm_layout(legend.outside = T,\n            main.title = \"Median Income by Census Tract\",\n            frame = T,\n            title = \"*Affected areas in pink*\") +\n  tm_compass(type = \"arrow\", \n             position = c(\"left\", \"top\")) +\n  tm_scale_bar()\n\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nShow code\nmap\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\n\n\n\nShow code\n#Finding the difference in income between the affected tracts and unaffected.\n\n#First we need create two sf data frames to categorize if they were impacted or not by the blackout\n\nnot_impacted &lt;- anti_join(tracts_affected , as.data.frame(blackout_join)) |&gt; \n  mutate(impacted = \"no\")\n\n\nJoining with `by = join_by(STATEFP, COUNTYFP, TRACTCE, GEOID, NAME, NAMELSAD,\nMTFCC, FUNCSTAT, ALAND, AWATER, INTPTLAT, INTPTLON, Shape_Length, Shape_Area,\nGEOID_Data, income_median, Shape)`\n\n\nShow code\nimpacted &lt;- blackout_join |&gt; \n  mutate(impacted = \"yes\")\n\n#Second, we need to create a new data frame were the \"key\" is the same column name, so we will have which tracts were impacted and which ones were not.\ncombination &lt;- rbind(not_impacted, impacted) |&gt; \n  dplyr::select(income_median, impacted)\n\nsummary(combination)\n\n\n income_median      impacted                   Shape    \n Min.   : 13886   Length:741         MULTIPOLYGON :741  \n 1st Qu.: 43749   Class :character   epsg:3083    :  0  \n Median : 60414   Mode  :character   +proj=aea ...:  0  \n Mean   : 71330                                         \n 3rd Qu.: 89796                                         \n Max.   :250001                                         \n NA's   :25                                             \n\n\nShow code\n#Third, create a histogram and a box plot of that new column and analyse the income median \n\nggplot(combination, aes(x = income_median, fill= impacted)) +\n  geom_histogram()  +\n    labs(title = \"Distribution of Income\",\n       x = \"Income Median\",\n       y = \"Count\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 25 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nShow code\nggplot(combination, aes(y = income_median, fill = impacted)) +\n  geom_boxplot() +\n  labs(title = \"Blackout Impact by Income\",\n       x = \"Impacted\",\n       y = \"Income Median\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nShow code\n# Statistical comparison\nsummary(impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  13886   43749   60642   71462   90013  250001       3 \n\n\nShow code\nsummary(not_impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25755   42793   53442   59586   63670  134500      22 \n\n\nShow code\nt.test(impacted$income_median, not_impacted$income_median)\n\n\n\n    Welch Two Sample t-test\n\ndata:  impacted$income_median and not_impacted$income_median\nt = 0.9696, df = 7.2121, p-value = 0.3636\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -16915.65  40668.75\nsample estimates:\nmean of x mean of y \n 71462.30  59585.75"
  },
  {
    "objectID": "posts/texas_outage/index.html#summary",
    "href": "posts/texas_outage/index.html#summary",
    "title": "Texas Outage",
    "section": "Summary",
    "text": "Summary\nThe results show a high number people affected by this winter storm. According to the data, almost 140,000 houses/ building were impacted within the designated area of Houston showing an almost equal impact between high income census tracts and low income census tracts. Looking in detail at the plots, the distribution plot (Histogram), is a right skew, showing a higher density around the low income median, nevertheless in the summary results you can see that the median income for the not impacted census tracts is lower that the impacted census tract, showing that the effect of the storm affected almost equally everyone in the zone, regardless of the income. It is important to consider that we are not weighting each tract by impacted houses. Which could affect final results, and conclusions."
  },
  {
    "objectID": "posts/texas_outage/index.html#footnotes",
    "href": "posts/texas_outage/index.html#footnotes",
    "title": "Texas Outage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "posts/machine_learning/index.html",
    "href": "posts/machine_learning/index.html",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "In this lab, we will apply our machine learning knowledge to train models that can predict dissolved inorganic carbon concentrations in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). After developing our model, we will test its accuracy using a separate data set and compete against our classmates in a Kaggle competition. Here, you can find more details about the competition.\nDissolved inorganic carbon (DIC) is present in all natural waters. The concentration of DIC varies from less than 20 μM in acidic soft waters to more than 5000 μM in highly alkaline hard waters, but ranges between 100 and 1000 μM in most systems. DIC is usually the most abundant form of C in water. DIC consists of three main constituents: free CO2 (a gas), the bicarbonate ion (HCO3−), and the carbonate ion (CO32 −). Although CO2, like other gases, readily exchanges with the atmosphere, even the surface waters of most inland systems are far from equilibrium and are usually supersaturated with respect to the atmosphere. A number of factors cause this disequilibrium. Reference\nSome tasks will be covered in this lab are; explore the data, pre-processing, choose a model algorithm, tune relevant parameters with cross validation, and create my own prediction."
  },
  {
    "objectID": "posts/machine_learning/index.html#dataset-description",
    "href": "posts/machine_learning/index.html#dataset-description",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Dataset Description",
    "text": "Dataset Description\nThis data set was downloaded from the CalCOFI data portal. Bottle and cast data was downloaded and merged, then relevant variables were selected. I will use this data (train.csv) to train a model that will predict dissolved inorganic carbon (DIC) content in the water samples.\n\nFiles\n\ntrain.csv - the training set\ntest.csv - the test set (without the dic )\nsample_submission.csv - a sample submission file in the correct format Columns A database description is available here: https://calcofi.org/data/oceanographic-data/bottle-database/\n\n\n\nRead the data\n\n\nShow code\nnames(test)\n\n\n [1] \"lat_dec\"           \"lon_dec\"           \"no2u_m\"           \n [4] \"no3u_m\"            \"nh3u_m\"            \"r_temp\"           \n [7] \"r_depth\"           \"r_sal\"             \"r_dynht\"          \n[10] \"r_nuts\"            \"r_oxy_micromol_kg\" \"po4u_m\"           \n[13] \"si_o3u_m\"          \"ta1_x\"             \"salinity1\"        \n[16] \"temperature_deg_c\"\n\n\n\n\nDataset Variable Description\n\nlat_dec - Latitude North (Degrees N)\nlon_dec - Longitude in (-180 - 180 Degrees E or W)\nno2u_m - Micromoles nitrite per liter of seawater\nno3u_m - Micromoles nitrate per liter of seawater\nnh3u_m - Micromoles ammonia per liter of seawater\nr_temp - Reported (Potential) temperature in degrees (°C)\nr_depth - Reported Depth (from pressure) in meters (m)\nr_sal - Reported Salinity (from Specific Volume anomoly, (M^3/Kg)\nr_dynht - Reported Dynamic Height in units of dynamic meters (work per unit mass)\nr_nuts - Reported ammonium concentration\nr_oxy_micromol_kg - Reported Oxygen micro-moles/kilogram\npo4u_m - Micro-moles Phosphate per liter of seawater\nsi_o3u_m- Micro-moles Silicate per liter of seawater\nta1_x - Total Alkalinity micro-moles per kilogram solution\nsalinity1 - Salinity\ntemperature_deg_c Temperature in Celsius (°C)\ndic - Dissolved inorganic carbon (Outcome)\n\n\n\nCreate the folds for the cross validation\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into the number of groups you desire making them roughly in equal size. In this case we will split into 10 groups.\n\n\nShow code\nset.seed(1)\n\n#Split the data\ndata_split &lt;- initial_split(train, strata = \"dic\")\n\ndata_cv &lt;- train |&gt; \n  vfold_cv(v = 10)\n\n\n\n\nPre-processing\nFor the pre-processing step we will create a recipe to prepare and normalize our data so we can proceed with the model. In this case we are interested in predicting the outcome variable dic which is the Inorganic Carbon in micro-moles\n\n\nShow code\ndic_recipe &lt;- recipe(dic ~ .,\n                     data = train) |&gt;\n  step_normalize() |&gt; \n  prep() |&gt; \n  bake(new_data = train)"
  },
  {
    "objectID": "posts/machine_learning/index.html#select-our-model",
    "href": "posts/machine_learning/index.html#select-our-model",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Select our model",
    "text": "Select our model\nWe will go with the XGBoost model. For short is Extreme Gradient Boosting. This model combines the predictions of multiple decision tree models in a systematic way to improve the accuracy of the final prediction. XGBoost works by iteratively adding decision trees to the model, where each subsequent tree tries to correct the errors of the previous tree. This model is highly customizable and allows for the tuning of many different parameters, making it a popular choice among data scientists and machine learning practitioner thanks to its speed and accuracy.\nFor this lab purpose we will first create a model tuning the learn_rate. The learn rate controls the step size for each iteration and its crucial to find the optimal learning rate at the beginning to ensure that the model converges efficiently and effectively. Secondly we will create a second model grid, tuning the number of trees, tree_depth, min_n, loss_reduction. Tuning this features such as the number of trees, tree depth, min_n, and loss reduction, we can ensure that our model is robust and able to capture complex relationships within the data. These features help prevent over-fitting and under-fitting by controlling the complexity of the model. Thirdly we will move with the stochastic parameters such as sample_size, mtry, and stop_iter, which they are essential for controlling the randomness in the training process preventing over-fitting by introducing randomness in the model’s selection of features and observations during each iteration. This ensures that the model does not memorize the training data and can generalize well to unseen data.\n\nLearning Rate\nFollowing the XGBoost tuning strategy, first conduct tuning on just the learn_rate parameter:\n\n\nShow code\nfirst_model &lt;-parsnip::boost_tree( # Set the classification for the dic variable\n  trees = 1000,\n  learn_rate = tune(),\n) |&gt; \n  set_engine(\"xgboost\") |&gt; # Set the model that you want to use.\n  set_mode(\"regression\") # Set the mode depending on your outcome variable\n\n\nSet up a grid to tune our first model to set the object where the model runs all possible combinations of the specified hyper-parameter values. Then this grid is used to train and evaluate the model using each combination of hyper-parameters to determine which one results in the best performance on the validation set.\n\n\nShow code\nset.seed(1)\nfirst_grid &lt;- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n\nCreate a workflow for this process: workflow() is a function in the Tidy Models package in R that enables users to define and execute a pipeline of data pre-processing, modeling, and post-processing steps.\n\n\nShow code\nfirst_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(first_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nCreate a tuning process: Here we will tune the first grid with tune_grid. (tune_grid() runs a set of performance metrics (e.g. accuracy or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data.\n\n\nShow code\ndoParallel::registerDoParallel() # to build trees in parallel\n\nfirst_tuned &lt;- tune_grid(\n  object = first_workflow,\n  resamples = data_cv,\n  grid      = first_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\n\n\nShow code\n#first_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfirst_model_best &lt;- first_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\n\nkable(first_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model Result of the First tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model Result of the First tune\n\n\nlearn_rate\n.config\n\n\n\n\n0.021\nPreprocessor1_Model03"
  },
  {
    "objectID": "posts/machine_learning/index.html#tune-second-parameters",
    "href": "posts/machine_learning/index.html#tune-second-parameters",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Tune Second Parameters",
    "text": "Tune Second Parameters\nThe tune() function is used to perform hyper-parameter tuning for machine learning models, where the goal is to find the optimal values for one or more hyper-parameters that maximize the performance of the model on a given data set.\nThis three parameters control the complexity of the trees being built and the stopping criteria for each tree:\n\ntree_depth: This parameter specifies the maximum depth of each tree in the boosting process. Increasing the tree depth can lead to more complex models that may capture more intricate relationships in the data, but it can also increase the risk of over-fitting.\nmin_n: This parameter specifies the minimum number of observations required in each terminal node (leaf) of a tree. Setting a higher value for this parameter can prevent the model from fitting to noise or outlines, but it may result in less flexible models.\nloss_reduction: This parameter determines the minimum amount of loss reduction required to further split a node in the tree. A higher value for this parameter can result in less complex models and faster convergence, but it may result in a model that is under-fitting the data.\ntrees: This parameter control the number of trees used in the boosting process. Increasing the number of trees typically improves the performance of the model on the training set, but too many trees can lead to over-fitting.\n\n\n\nShow code\nsecond_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = tune(),\n  learn_rate = first_model_best$learn_rate,\n  min_n = tune(), \n  tree_depth = tune(), \n  loss_reduction = tune() \n  ) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate the workflow\n\n\nShow code\nsecond_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(second_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\nShow code\nsecond_grid &lt;- dials::grid_max_entropy(\n  min_n(),\n  tree_depth(),\n  loss_reduction(),\n  trees(), \n  size = 100) #GRID specs from the book\n\n\nUse the tune_grid to feed all alternatives and create the tuning process.\n\n\nShow code\ndoParallel::registerDoParallel() # this function helps to build trees in parallel\nseconed_tuned &lt;- tune_grid(\n  object = second_workflow,\n  resamples = data_cv,\n  grid      = second_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nShow code\n#seconed_tuned %&gt;% tune::show_best(metric = \"rmse\")\n\nsecond_model_best &lt;- seconed_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(second_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the second tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the second tune\n\n\ntrees\nmin_n\ntree_depth\nloss_reduction\n.config\n\n\n\n\n868\n3\n7\n5.601\nPreprocessor1_Model006\n\n\n\n\n\n\n\nIn this case the show_best() displays the optimal hyperparameters and the corresponding performance metric(s) from the tuning process."
  },
  {
    "objectID": "posts/machine_learning/index.html#tune-stochastic-parameters",
    "href": "posts/machine_learning/index.html#tune-stochastic-parameters",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Tune Stochastic Parameters",
    "text": "Tune Stochastic Parameters\nNow, for our third model we will randomly sample observations and predictors (features) at each iteration, which will help us to reduce overfitting and improve the generalization performance of the model. This process is called Stochastic Gradient Boosting (SGB) and we will tune tune() the hyperparameters of sample_size, mtry and stop_iter:\n\nsample_size: This argument specifies the fraction (or absolute number) of observations to be sampled at each iteration. The default value is 1, which corresponds to sampling all observations. Setting sample_size to a value less than 1 results in stochastic gradient boosting, where each iteration uses a randomly selected subset of the data.\nmtry: This argument specifies the number of randomly selected predictors (features) to be used at each split in the tree. The default value is NULL, which corresponds to using all predictors. Setting mtry to a value less than the total number of predictors results in stochastic feature selection, where each split uses a randomly selected subset of the predictors.\nstop_iter: This argument in boost_tree() specifies the stopping criterion for the boosting process. This hyperparameter determines when the boosting process should be terminated based on a certain criterion, such as the improvement in the loss function or the number of consecutive iterations with no improvement.\n\n\n\nShow code\nthird_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees,\n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth,\n  min_n = second_model_best$min_n,\n  loss_reduction = second_model_best$loss_reduction,\n  sample_size = tune(),\n  mtry = tune(),\n  stop_iter = tune()\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nSet up the new parameters with new range of sample proportions to improve our model\n\n\nShow code\nthird_params &lt;- dials::parameters(\n  stop_iter(c(5, 50)),\n  sample_size = sample_prop(c(0.4, 0.9)),\n  finalize(mtry(), train))\n\n\nCreate a workflow\n\n\nShow code\nthird_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(third_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid\n\n\nShow code\nthird_grid &lt;- dials::grid_max_entropy(third_params, size = 100) \n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nShow code\ndoParallel::registerDoParallel() # to build trees in parallel\n\nthird_tuned &lt;- tune_grid(\n  object = third_workflow,\n  resamples = data_cv,\n  grid      = third_grid ,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nShow code\n#third_tuned %&gt;% tune::show_best(metric = \"rmse\")\nthird_model_best &lt;- third_tuned %&gt;% tune::select_best(metric = \"rmse\")\nkable(third_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the Third Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the Third Tune\n\n\nmtry\nsample_size\nstop_iter\n.config\n\n\n\n\n6\n0.679\n11\nPreprocessor1_Model093\n\n\n\n\n\n\n\nAs we can see in the results above for the Stochastic Parameteres, which are this randomized selections to reduce overfitting, our new optimal parameter are:\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 6\n  trees = second_model_best$trees\n  min_n = second_model_best$min_n\n  tree_depth = second_model_best$tree_depth\n  learn_rate = first_model_best$learn_rate\n  loss_reduction = second_model_best$loss_reduction\n  sample_size = 0.678898499789648\n  stop_iter = 11\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/machine_learning/index.html#finalize-workflow-and-make-final-prediction",
    "href": "posts/machine_learning/index.html#finalize-workflow-and-make-final-prediction",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Finalize workflow and make final prediction",
    "text": "Finalize workflow and make final prediction\n\nAssemble your final workflow will all of your optimized parameters and do a final fit.\n\n\n\nShow code\nfinal_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees, \n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth, \n  min_n = second_model_best$min_n, \n  loss_reduction = second_model_best$loss_reduction, \n  sample_size = third_model_best$sample_size,\n  mtry = third_model_best$mtry,\n  stop_iter = third_model_best$stop_iter\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate your Workflow\n\n\nShow code\nfinal_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(final_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nShow code\n# Run your final model\nfinal_tuned &lt;- tune_grid(\n  object = final_workflow,\n  resamples = data_cv,\n  metrics   = metric_set(rmse))\n\n\nTake a look at the results\n\n\nShow code\n#final_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfinal_metrics &lt;- final_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(final_metrics,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Final Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Final Tune\n\n\n.config\n\n\n\n\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\nShow code\nfinal_boost &lt;- finalize_model(final_model,\n                             select_best(final_tuned))\nfinal_boost_fit &lt;- last_fit(final_boost, dic ~ ., data_split)"
  },
  {
    "objectID": "posts/machine_learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make",
    "href": "posts/machine_learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "How well did your model perform? What types of errors did it make?",
    "text": "How well did your model perform? What types of errors did it make?\nShow all metrics\n\n\nShow code\nfinal_metrics &lt;- final_boost_fit %&gt;% \n  collect_metrics(summarise = TRUE) %&gt;%\n  mutate(param_tuned = \"final\")"
  },
  {
    "objectID": "posts/machine_learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make-1",
    "href": "posts/machine_learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make-1",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "How well did your model perform? What types of errors did it make?",
    "text": "How well did your model perform? What types of errors did it make?\n\nRMSE: On average, the predicted DIC values from your model are off by about 5.610567 units of DIC. This means that if the actual DIC value is 100, your model might predict a value between 105.610567and 94.389433, with an average error of 5.610567 units. It’s important to note that this is an average error across all predictions, and there may be individual predictions that are much more or less accurate than this.\nR²: Your model explains 99.7510634% of the variance in DIC. This means that the model fits the data very well and can be used to make accurate predictions. A high R² value indicates that there is a strong linear relationship between the predictors and the outcome, and that the model is able to capture this relationship well."
  },
  {
    "objectID": "posts/machine_learning/index.html#now-lets-add-the-predicted-data-to-our-testing-data",
    "href": "posts/machine_learning/index.html#now-lets-add-the-predicted-data-to-our-testing-data",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "Now lets add the predicted data to our testing data:",
    "text": "Now lets add the predicted data to our testing data:\n\n\nShow code\ntest_id &lt;- read_csv(here::here(\"posts\", \"machine_learning\", \"data\", \"test.csv\")) |&gt; \n  select(id)\n\n\nRows: 485 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): id, Lat_Dec, Lon_Dec, NO2uM, NO3uM, NH3uM, R_TEMP, R_Depth, R_Sal,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow code\nresult &lt;- final_workflow |&gt; \n  fit(data = train) |&gt; \n  predict(new_data = test)\n\npredict_test &lt;- cbind(test, result) |&gt; \n  cbind(test_id) |&gt; \n  rename(DIC = .pred) |&gt; \n  select(c(id, DIC)) \n\n#This is the line where you can add your final results\n#write_csv(predict_test, \"final_results.csv\")\n\n\nCreate a graph and prints the result of RSME error (The lower the better), and the rsq value. This can be seen in the table above.\n\n\nShow code\nggplot(final_metrics, aes(x = .metric, y = .estimate, fill = .metric)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"Metric\", y = \"Value\", title = \"Model Performance\")"
  },
  {
    "objectID": "posts/eez_spatial/index.html",
    "href": "posts/eez_spatial/index.html",
    "title": "Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .2\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nSea surface temperature: 11-30 °C\nDepth: 0-70 meters below sea level\n\n\n\n\nCombining vector/raster data\nResampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "posts/eez_spatial/index.html#overview",
    "href": "posts/eez_spatial/index.html#overview",
    "title": "Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .2\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nSea surface temperature: 11-30 °C\nDepth: 0-70 meters below sea level\n\n\n\n\nCombining vector/raster data\nResampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "posts/eez_spatial/index.html#data",
    "href": "posts/eez_spatial/index.html#data",
    "title": "Marine Aquaculture",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\nPrepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\n\nLoad necessary packages and set path\n\nI recommend using the here package\n\nRead in the shape file for the West Coast EEZ (wc_regions_clean.shp)\nRead in SST rasters\n\naverage_annual_sst_2008.tif\naverage_annual_sst_2009.tif\naverage_annual_sst_2010.tif\naverage_annual_sst_2011.tif\naverage_annual_sst_2012.tif\n\nCombine SST rasters into a raster stack\nRead in bathymetry raster (depth.tif)\nCheck that data are in the same coordinate reference system\n\nReproject any data not in the same projection\n\n\n\n\n\nShow code\n# Setting my filepaths\nrootdir &lt;- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndata &lt;- file.path(rootdir,\"data\",\"assignment4\")\n\n# Creating the names for each file\nsst_2008 &lt;- 'average_annual_sst_2008.tif' \nsst_2009 &lt;- \"average_annual_sst_2009.tif\"\nsst_2010 &lt;- \"average_annual_sst_2010.tif\"\nsst_2011 &lt;- \"average_annual_sst_2011.tif\"\nsst_2012 &lt;- \"average_annual_sst_2012.tif\"\ndepth &lt;- \"depth.tif\"\n  \n# Downloading the raster data to a star object\nsst_2008 &lt;- rast(file.path(data, sst_2008))\nsst_2009 &lt;- rast(file.path(data, sst_2009))\nsst_2010 &lt;- rast(file.path(data, sst_2010))\nsst_2011 &lt;- rast(file.path(data, sst_2011))\nsst_2012 &lt;- rast(file.path(data, sst_2012))\nwc_regions &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `/Users/javipatron/Documents/MEDS/Courses/eds223/data/assignment4/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nShow code\ndepth &lt;- rast(file.path(data,depth))\n\n# Stack all the raster\nall_sst &lt;- c(sst_2008, sst_2009, sst_2010, sst_2011, sst_2012)\n\n\n# Check coordinate reference system\n#st_crs(wc_regions)\n#st_crs(depth)\n#st_crs(all_sst)\n\n\n# Set the new CRS\nall_sst &lt;- project(all_sst, \"EPSG:4326\")\nall_sst\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 5  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nnames       : averag~t_2008, averag~t_2009, averag~t_2010, averag~t_2011, averag~t_2012 \nmin values  :      278.8167,      278.0800,      279.9200,      278.8600,      278.1920 \nmax values  :      301.4321,      301.4475,      300.9486,      307.2733,      309.8502 \n\n\nPrint the West Cost Polygon Vector data to see how it looks like:\n\n\nShow code\ntm_shape(wc_regions) +\n  tm_polygons(col=\"rgn\",\n              palette= \"RdYlBu\",\n              legend.reverse = T,\n              title = \"EEZ West Coast Regions\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass()\n\n\nSome legend labels were too wide. These labels have been resized to 0.44, 0.45, 0.49. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\nShow code\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing"
  },
  {
    "objectID": "posts/eez_spatial/index.html#process-data",
    "href": "posts/eez_spatial/index.html#process-data",
    "title": "Marine Aquaculture",
    "section": "Process Data",
    "text": "Process Data\nNext, we need process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\n\nFind the mean SST from 2008-2012\nConvert SST data from Kelvin to Celsius\nCrop depth raster to match the extent of the SST raster\n\nnote: the resolutions of the SST and depth data do not match\n\nResample the depth data to match the resolution of the SST data using the nearest neighbor approach.\nCheck that the depth and SST match in resolution, extent, and coordinate reference system. Can the rasters be stacked?\n\n\n\nShow code\n#Finding the mean \nmean_sst &lt;- terra::app(all_sst, mean)\n\n#Converting to Celsius\nmean_sst_c &lt;- (mean_sst - 273.15)\n\n#Cropping the Depth to just the area of SST\ncrop_depth &lt;- crop(depth, mean_sst)\n\nclass(crop_depth)\n\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nShow code\n# Re-sample the depth with the needed resolution\nnew_depth &lt;- terra::resample(crop_depth, mean_sst_c, method = \"near\")\n\n# Stack both rasters and see if they match\nstack_depth_sst &lt;- c(mean_sst_c, new_depth)\n\n\n\nFind suitable locations\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\n\nReclassify SST and depth data into locations that are suitable for Lump sucker fish\n\nhint: set suitable values to 1 and unsuitable values to NA\n\nFind locations that satisfy both SST and depth conditions\n\nhint: create an overlay using the lapp() function multiplying cell values\n\n\n\n\n\nShow code\n#Create the matrix for Temperature between 11°C and 30°C\ntemp_vector &lt;- c(-Inf, 11, NA, \n                   11, 30, 1,\n                   30, Inf, NA)\n\ntemp_oysters_matrix &lt;- matrix(temp_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of °C\ntemp_oysters &lt;- classify(mean_sst_c, temp_oysters_matrix)\n\n\n#Create the matrix for Depth between 0 & -70\ndepth_vector &lt;- c(-Inf, -70, NA, \n                   -70, 0, 1,\n                   0, Inf, NA)\n\ndepth_oysters_matrix &lt;- matrix(depth_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of depth\ndepth_oysters &lt;- classify(new_depth, depth_oysters_matrix, include.lowest = T)\n\n# Combine the two raster\nmatrixes &lt;- c(depth_oysters, temp_oysters)\n\n# Rename the matrixes attributes\nnames(matrixes) &lt;- c(\"temp_matx\",\"depth_matx\")\n\n#Combine both matrixes\ncombined_matrix_stack &lt;- c(matrixes, stack_depth_sst)\n\n\n\n\nShow code\n#Find the locations where the oysters have a 1 in the pixel.\ncheck_condition &lt;- function(x,y){\n  return(x * y)\n   }\n\ntemp_conditions &lt;- lapp(combined_matrix_stack[[c(1,3)]], fun = check_condition)\ndepth_conditions &lt;- lapp(combined_matrix_stack[[c(2,4)]], fun = check_condition)\n\ntm_shape(temp_conditions) +\n  tm_raster(title = \"Sea Temp °C\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\nShow code\ntm_shape(depth_conditions) +\n  tm_raster(title = \"Depth (Meters)\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\nNow lets create the mask of both conditions\n\n\nShow code\nsuitable_conditions &lt;- lapp(matrixes[[c(1,2)]], fun = check_condition)\nprint(suitable_conditions)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : lyr1 \nmin value   :    1 \nmax value   :    1"
  },
  {
    "objectID": "posts/eez_spatial/index.html#determine-the-most-suitable-eez",
    "href": "posts/eez_spatial/index.html#determine-the-most-suitable-eez",
    "title": "Marine Aquaculture",
    "section": "Determine the most suitable EEZ",
    "text": "Determine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nSelect suitable cells within West Coast EEZs\nFind area of grid cells\nFind the total suitable area within each EEZ\n\nhint: it might be helpful to rasterize the EEZ data\n\nFind the percentage of each zone that is suitable\n\nhint it might be helpful to join the suitable area by region onto the EEZ vector data\n\n\n\n\nShow code\ncell_ezz &lt;- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n\nrast_ezz &lt;- rasterize(wc_regions, suitable_conditions, field= 'rgn')\nmask_ezz &lt;-  mask(rast_ezz, suitable_conditions)\nsuitable_area &lt;- zonal(cell_ezz, mask_ezz, sum )\n\njoined_area &lt;-  left_join(wc_regions, suitable_area, by = 'rgn') |&gt; \n  mutate(area_suitkm2 = area,\n         percentage = (area_suitkm2 / area_km2) * 100,\n         .before = geometry)"
  },
  {
    "objectID": "posts/eez_spatial/index.html#visualize-results",
    "href": "posts/eez_spatial/index.html#visualize-results",
    "title": "Marine Aquaculture",
    "section": "Visualize results",
    "text": "Visualize results\nNow that we have results, we need to present them!\nCreate the following maps:\n\nTotal suitable area by region\nPercent suitable area by region\n\n\n\nShow code\ntm_shape(joined_area) +\n  tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Total Suitable area per EEZ for: Oysters\"),\n            frame = T)\n\n\n\n\n\n\n\n\n\nShow code\ntm_shape(joined_area) +\n  tm_polygons(col = \"percentage\", palette = \"RdYlBu\", legend.reverse = T,\n              title = \"Percent (%)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Suitable Area per EEZ for: Oysters \"),\n            frame = T)"
  },
  {
    "objectID": "posts/eez_spatial/index.html#conclusion",
    "href": "posts/eez_spatial/index.html#conclusion",
    "title": "Marine Aquaculture",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see in the maps above the most suitable areas for the oysters are the northerner regions with a 3 to 3.5 % of the entire EEZ region. As the assignments states the sea surface temperature that the oysters like are between the 11-30 °C, and for the depth is between 0-70 meters below sea level, which are pretty specific. Thus, this results can give us hints on how the Economic Exclusive Zones help those species, and we can compare to other EEZ or other species of the US West Coast."
  },
  {
    "objectID": "posts/eez_spatial/index.html#broaden-your-workflow",
    "href": "posts/eez_spatial/index.html#broaden-your-workflow",
    "title": "Marine Aquaculture",
    "section": "Broaden your workflow!",
    "text": "Broaden your workflow!\nNow that you’ve worked through the solution for one group of species, let’s update your workflow to work for other species. Please create a function that would allow you to reproduce your results for other species. Your function should be able to do the following:\n\n\nAccept temperature and depth ranges and species name as inputs\n\nCreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\nRun your function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\n\n\nShow code\nfind_my_happy_place &lt;- function(species = \"NAME\", min_temp = 5, max_temp = 30, min_depth = 0, max_depth = -5468) {\n  temp_vector &lt;- c(-Inf, min_temp, NA, min_temp, max_temp, 1,max_temp, Inf, NA)\n  temp_matrix &lt;- matrix(temp_vector, ncol= 3, byrow = T)\n  temp_condition &lt;- classify(mean_sst_c, temp_matrix)\n  depth_vector &lt;- c(-Inf, max_depth, NA, max_depth, min_depth, 1, min_depth, Inf, NA)\n  depth_matrix &lt;- matrix(depth_vector, ncol= 3, byrow = T)\n  depth_condition &lt;- classify(new_depth, depth_matrix, include.lowest = T)\n  mix_rasters &lt;- c(depth_condition, temp_condition)\n  suitable_conditions &lt;- lapp(mix_rasters[[c(1,2)]], fun = check_condition)\n  cell_ezz &lt;- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n  rast_ezz &lt;- rasterize(wc_regions, suitable_conditions, field= 'rgn')\n  mask_ezz &lt;-  mask(rast_ezz, suitable_conditions)\n  suitable_area &lt;- zonal(cell_ezz, mask_ezz, sum )\n  joined_area &lt;-  left_join(wc_regions, suitable_area, by = 'rgn') |&gt;\n    mutate(happy_area_km2 = area,\n           \"happy_(%)\" = (happy_area_km2 / area_km2) * 100,\n           .before = geometry) |&gt; \n    arrange(desc(happy_area_km2))\n  map &lt;- tmap_arrange(tm_shape(joined_area) +\n                        tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Total Suitable area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T),\n                        tm_shape(joined_area) +\n                        tm_polygons(col = \"happy_(%)\", palette = \"RdYlBu\", legend.reverse = T,\n                                    title = \"Percent (%)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Suitable Area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T))\n  print(joined_area[c(\"rgn\", \"rgn_key\", \"area_km2\", \"happy_area_km2\", \"happy_(%)\",\"geometry\")])\n  print(paste(\"*Conclusion:* For the species\", species, \"the most suitable region is\", joined_area$rgn[1], \"with\", round(joined_area$happy_area_km2[1],2), \"km2 of 'happy' area.\"))\n  \n  map\n}\n\n\n\nNow Test your function\nREMEMBER:\n1. The species name has to be in quotes. The Default value is “NAME”.\n2. The depth has to include the negative sign for the maximum depth.\nDefault; Min: 0, Max: -5468.\n3. The Temperature is in °C .\nDefault; Min: 5, Max: 30).\n\n\nShow code\nfind_my_happy_place()\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2 happy_(%)\n1 Southern California    CA-S 206860.78      205730.95  99.45382\n2  Central California    CA-C 202738.33      201478.95  99.37882\n3              Oregon      OR 179994.06      178650.46  99.25353\n4 Northern California    CA-N 164378.81      162975.61  99.14636\n5          Washington      WA  66898.31       64695.95  96.70790\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-123.4318 4...\n4 MULTIPOLYGON (((-124.2102 4...\n5 MULTIPOLYGON (((-122.7675 4...\n[1] \"*Conclusion:* For the species NAME the most suitable region is Southern California with 205730.95 km2 of 'happy' area.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Try your species!! \nfind_my_happy_place(\"Turtle\", 13, 28, 0, -290)\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2  happy_(%)\n1 Southern California    CA-S 206860.78     10890.3024 5.26455645\n2  Central California    CA-C 202738.33       360.9648 0.17804468\n3          Washington      WA  66898.31        29.1610 0.04359004\n4              Oregon      OR 179994.06             NA         NA\n5 Northern California    CA-N 164378.81             NA         NA\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-122.7675 4...\n4 MULTIPOLYGON (((-123.4318 4...\n5 MULTIPOLYGON (((-124.2102 4...\n[1] \"*Conclusion:* For the species Turtle the most suitable region is Southern California with 10890.3 km2 of 'happy' area.\""
  },
  {
    "objectID": "posts/eez_spatial/index.html#footnotes",
    "href": "posts/eez_spatial/index.html#footnotes",
    "title": "Marine Aquaculture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).↩︎\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).↩︎\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "posts/blog_example_1/index.html",
    "href": "posts/blog_example_1/index.html",
    "title": "Blog Post Title",
    "section": "",
    "text": "Hi this my first class blog post, and this is my first graph in R.\nSHARK\n\n\n\n\n\nIn this page will also help me visualize my references, cites, changes, and edits to official posts in my webpage.\nHere is some more text. Cite: (Csik 2022)\nWhen you want to reference stuff. Then in this same pager click in “Insert v”, then click in citation… then copy and paste the url\n\n\n\n\nReferences\n\nCsik, Samantha. 2022. “Adding a Blog to Your Existing Quarto Website.” October 24, 2022. https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/.\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Patrón, Javier},\n  title = {Blog {Post} {Title}},\n  date = {2022-10-24},\n  url = {https://javipatron.github.io/posts/blog_example_1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrón, Javier. 2022. “Blog Post Title.” October 24, 2022.\nhttps://javipatron.github.io/posts/blog_example_1/."
  },
  {
    "objectID": "posts/ethics-post/index.html",
    "href": "posts/ethics-post/index.html",
    "title": "Project Chaac Dives into the Pool of Ethical Sense",
    "section": "",
    "text": "Data is a new language in which we read, write, and transform the figures our eyes cannot naturally see, hidden in our surrounding reality. However, thanks to the technological revolution, those “natural numbers” can now be deciphered and manipulated by people who are fortunate and deft enough to speak and understand such language. \nRephrasing a definition of data coined by Stefania Milan “Data is seen as an avenue to revert or challenge our dominant understandings of the world, (re)creating conditions of possibility for counter-imaginaries and social justice claims to emerge.” With that in mind, In this blog post, I will discuss how data experts would be able to generate more consciousness on ethics and bias while developing a more sensitive touch in their daily work as environmental data scientists. To help express these thoughts on paper, I will use an actual environmental project held in the Yucatán Peninsula of Mexico and discuss how responsible and equitable data scientists should behave when exercising the power of data to design a world case example out of this project. \nMy musings expressed in this post are meant to awaken present and future “data evangelists”, motivating them to nourish a better sense of community within their field, and understanding the substantial change that both environment and society will experience through Project Chaac. This reflection will be based on the skills and foundations I have acquired throughout the course of Ethics and Bias in Environmental Data Science at the Bren School of Environmental Science & Management. \nAt its core, Project Chaac seeks to protect, conserve, and restore a territory of 42,676 hectares of degraded mangroves in the southern Mexican state of Yucatán. The project involves a significant amount of team management that must be aligned with overall environmental impact. Worldwide, most of the lands and territories targeted for greenhouse gas (GHG) mitigation action overlap with areas customarily held by indigenous groups, local communities, and Afro-descendant peoples. \nMoreover, most of the lands and forests targeted for nature-based greenhouse gas removal and offset, like Project Chaac, are located within areas where the State’s customary rights of communities have yet to be recognized (Reference two). Therefore, I will briefly describe some difficulties and potential roadblocks involving ethics and bias that the project might face in Mexico, since its complexity resides in the entangled path of multiple factors and circumstances at constant play. Metaphorically speaking, this project is a living, growing vine, in which the roots are essential; biodiversity stands for the open minds involved; key nutrients represent human values, and the environment as a whole is the high frequency vibe of all the passionate individuals that will provide a firm and rich soil to plant the seed of change. Finally, we will analyze how this tiny seed can become a fully grown, jade-green turquoise plant, as astonishing as the Strongylodon Macrobotrys.\n\n\n\nImage 1: How does a seed germinate?\n\n\nAs with all plants, a careful assessment of the seed germination and the creation of concise strategies will have an exponential impact on the growth and direction of the project. Here are the three axes on which I foresee true potential for building solid bridges between ethics and bias resides:\n\nData Justice.- Satellite images are used to select the pieces of land with the highest potential for mangrove restoration in the area. Therefore an equitable allocation of carbon credits rights, land use, and business strategy must be well cemented among all players involved: Federal and local authorities, as well as the private sector, along with the rural communities called ejidos, as well as the territories inhabited by Afro-descendants. (Reference four)\nProject Development.- The actual value of the land comes when you dip your hands in the dirt while sweating big fat droplets on the rich soil, learning and adapting to your surroundings. In this case, the project implies complicated engineering analysis to build an efficient network of hydrological channels in the near future, in order to embrace a healthy and protected ecosystem in the restored mangrove lands.\n\n\n\n\nImage 2: Hydrological construction and rehabilitation of canals. Photo: Primary Production Laboratory (CINVESTAV- IPN)\n\n\n\nLegal Feasibility.- Carbon accounting in developing countries is facing difficult but also exciting times. Agreements regarding carbon rights will encounter critical roadblocks, and hence need to be exceptionally well-planned. Additionally, with climate change hot on our heels and the Paris Agreement goals swiftly approaching, countries and corporations are increasing their participation to meet their emission reduction targets and net-zero commitments 5. Keeping that in mind, embracing and understanding Mexican laws will become a crucial element for the success of the project.\n\nNow, I would like to define what Blue Carbon is and why it is so important. A newly developed concept, it refers to the carbon stocks sequestered in any coastal ecosystem, like mangroves, sea grasses, and salt marshes. Mangroves are considered one of the most productive and biologically complex ecosystems on earth; that is why they are also regarded as Nature’s “superheroes” fighting against climate change. \nHowever, over the last few years, mangroves have been deforested at an alarming rate, due to increased coastal development and other anthropogenic factors. Thus, conservation and restoration of these coastal ecosystems are essential both to sustain the natural environment and to assure their decisive impact on our world. As with coral reefs, mangrove forests allow the development of true environmental health by fostering fisheries and healthy coastal ecosystems, as well as by providing significant protection from natural disasters.\nNext, I will introduce the topic of Data Justice and why it is relevant to this project and blog post. Data Justice happens whenever data relies on insufficient recognition to understand real community needs. It is an approach that addresses new ways of data collection and dissemination of crucial facts which have been invisibilized in the past, thus harming marginalized communities (Reference six). Therefore, it plays an essential role in carbon-credit adjudications. In environmental science, carbon stocks represent the total amount of organic carbon stored within an environmental system, so the correct manipulation of data is key to quantifying such processes. Hence, data scientists play a vital role in elegant calculations in the pursuit of data justice.\nRegarding carbon rights in Mexico, I am presenting two tables to describe some general carbon trade legal agreements. Carbon rights are used to describe a number of different tradeable GHG prerogatives. In other words, it is the given right to benefit from sequestered carbon and/or reduced greenhouse gas emissions.” (Reference two)\n\n\nShow code\n# Add the tables in a nice format\n\n\n\nTable 1. Legal frameworks to support carbon-linked transactions.\n\n\n\n\n\n\n\n\n\n\nCountry\nLaws for securing community tenure to forests?\nEstablished a national legal framework for carbon trade?\nDefined carbon rights?\nAre carbon rights linked to tenure?\nDo communities have carbon rights?\n\n\n\n\nMexico\nYes\nPartial\nInconclusive or undefined\nInconclusive or undefined\nInconclusive or undefined\n\n\n\n\nTable 2. Legal recognition of community land.\n\n\n\n\n\n\n\n\nCountry\nCountry area where rights of Indigenous  peoples, local communities, and Afro-descendants are legally recognized\nCountry area where rights of Indigenous peoples, local communities, and Afro-descendants are not legally recognized\nTotal percent of land held by Indigenous peoples, local communities, and Afro-descendants\n\n\n\n\nMexico\n52 %\n0.50%\n52.5 %\n\n\n\nAs the tables clearly depict, Mexico has an inconclusive law on carbon rights. However, almost half of its territory is legally recognized when referring to its indigenous peoples, local communities, and Afro-descendants, which is today a leading example among developing countries.\nSo now we have an overview of the project’s objectives. However, how and where does data manipulation impact its success? \nCurrently, the CINVESTAV (The Center for Research and Advanced Studies of the National Polytechnic Institute), a non-governmental Mexican research institution, has settled and defined the actual conditions of the mangroves by using satellite imagery. After collecting images from the Sentinel-2A 7 from March 2021 to June 2022, and by using a powerful environmental data science tool called NDVI (Normalized Difference Vegetation Index), they were able to quantify and define the areas with higher and lower vegetation density health. In other words, they designated the patches of land in specific areas of interest, where vegetation reflects green back to space (meaning healthy mangroves), comparing such areas against those showing degraded or deforested vegetation, (which reflected longer color wavelengths).\n\n\n\n\nImage 3: NDVI Formula\n\n\nBy using the NDVI tool, a language that only a few people know about (remember “potential bias”?), the CINVESTAV team defined those sections of land with a higher probability of mangrove reforestation along with the potential for the development of a potential carbon accounting business. As you can imagine, such results designate an important plant tutor, which sets the standard for launching critical collaborative agreements with land owners, state governments, private companies, and ejidos.\nNowadays, technology has become a potent influential tool across the globe, but it can also be scary and threatening to those who do not understand it. Consequently, I invite all involved parties, especially data scientists, to think outside the box from the very beginning and bring the terminology of Data Justice into play. \nIn contrast, if a project lacks a strategy of environmental data justice or fails to recognize an equitable carbon right among indigenous people and/or local communities, it will surely generate a negative domino effect with huge implications, leaving long-lasting scars for all future environmental nature-based initiatives.\nThroughout the course of Ethics and Bias, I learned the importance of becoming data-sensitive. I am one of those scientists who are fortunate enough to speak the language of data, having developed a sense for reading and hearing his surroundings in order to find a deeper connection with their heart and soul, ensuring that the local wisdom moves from one generation to the next, honoring its ancient knowledge as an invaluable heritage. I wish to become a key player in communicating the importance of this work, quickly generating new allies within the communities. Undoubtedly, data sensitivity is the vital foundation for building a solid ethical environment.\nUnfortunately, only some countries explicitly recognize community carbon rights, and even fewer have tested established rules’ operational and political feasibility. However, Project Chaac has all the tools to become a world case example of a well-organized project, although a topnotch level of data sensitivity, knowledge on carbon rights, data justice, and firm management must be assured to bring all the pieces together.\nOverall, the success of this mangrove restoration project depends heavily on a vigorous commitment between all players involved: Collaborators, government agents, lawyers, field engineers, data scientists, and ejido leaders, among others. These 40,000 hectares of land are heavily threatened by tourism and pressure from the governmental instances. Yet, I am certain that Project Chaac represents a golden opportunity to show the world how a nature-based solution with a well-cemented ethical culture is definitely possible. All of the aforementioned factors will empower our incipient vine to evolve into an emerald wonder that will make the world’s eyes widen in awe, aiming to the skies.\n\n\n\nImage 4: The beautiful blue jade vine (Strongylodon Macrobotrys)\n\n\nReferences:\n1. Stefania, S. (2019) Full article: Exploring Data Justice: Conceptions, applications and ..., Exploring Data Justice: Available at: https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1606268 (Accessed: December 8, 2022).\n2. Initiative, A.R.and R. (2020) Rights-based conservation: The path to preserving Earth’s biological and cultural diversity?, Rights + Resources. Available at: https://rightsandresources.org/publication/rights-based-conservation/ (Accessed: December 7, 2022).\n3. Strongylodon macrobotrys (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Strongylodon_macrobotrys (Accessed: December 7, 2022).\n4. Ejido (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Ejido (Accessed: December 7, 2022).\n5. Hood, C. (2020) Completing the Paris ‘rulebook’: Key article 6 issues, Center for Climate and Energy Solutions. Available at: https://www.c2es.org/document/completing-the-paris-rulebook-key-article-6-issues/ (Accessed: December 7, 2022).\n6. Taylor, L. (2017). What is Data Justice? The case for connecting digital rights and freedoms globally. Big Data & Society.https://doi.org/10.1177/2053951717736335.\n7. Home (no date) Sentinel. Available at: https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a (Accessed: December 7, 2022).\n\n\n\n\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Patrón, Javier},\n  title = {Project {Chaac} {Dives} into the {Pool} of {Ethical} {Sense}},\n  date = {2022-12-19},\n  url = {https://github.com/javipatron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrón, Javier. 2022. “Project Chaac Dives into the Pool of\nEthical Sense.” December 19, 2022. https://github.com/javipatron."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Javier Patrón",
    "section": "",
    "text": "Data Science Master´s Student\n\n\nEnvironmental Engineer"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blogs",
    "section": "",
    "text": "Inorganic Carbon in Machine Leaning\n\n\n\nMEDS\n\n\nOcean\n\n\nR\n\n\n\nUsing data from the California Cooperative Oceanic Fisheries Investigations program (CalCOFI) we will train machine learning models for predicting dissolved inorganic carbon…\n\n\n\nJavier Patrón\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Chaac Dives into the Pool of Ethical Sense\n\n\n\nMEDS\n\n\nMangroves\n\n\n\nAn Ethics and Bias thought\n\n\n\nJavier Patrón\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Aquaculture\n\n\n\nMEDS\n\n\nSpatial\n\n\nOcean\n\n\nR\n\n\n\nDetermining which Exclusive Economic Zones (EEZ) on the US West Coast are best suited to developing marine aquaculture for ocean species like Oysters.\n\n\n\nJavier Patrón\n\n\nDec 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAn Statistical Mangrove Analysis Post\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\nMangroves\n\n\n\nA blue carbon analysis based on the skills learned from the course of Statistics for Environmental Data Science at the Bren School of Environmental Science & Management\n\n\n\nJavier Patrón\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas Outage\n\n\n\nMEDS\n\n\nSpatial\n\n\nR\n\n\n\nAnalyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area\n\n\n\nJavier Patrón\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Title\n\n\n\nMEDS\n\n\nR\n\n\nPython\n\n\nOcean\n\n\n\nWhat my first post looks like\n\n\n\nJavier Patrón\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "BIO",
    "section": "",
    "text": "After graduating from Universidad Iberoamericana as a Mechanical and Electrical Engineer in Mexico City, I started my professional career as a Field Engineer at General Electric. I was the pioneer commissioner and field engineer for the smart lighting product development in the Latin America region. After 5 years, the GE Current smart controls and energy management software grew significantly and I ended up as the team leader for the global commissioning and tech-support team.\nThen in 2021, I shifted my career to become the team headman and navigator of a racing sailboat to cross the Pacific. During this time, I volunteered in a coral restoration project in Fiji, where the collection of data was proven to be key for effective restoration practices. This ocean crossing adventure was a life-changing experience and fueled my enthusiasm to apply for the Masters of Environmental Data Science at the Bren School UCSB.\nI´m an energetic and proactive individual. My vision as an Environmental Data Scientist is to empower high-end technology companies by improving their monitoring practices on data collection and drawing actionable insights to foster a positive environmental impact. Ultimately, I aspire to work on natural climate solutions, carbon offset, and MPA restoration and conservation.\n\n\n\nImage: Coral Farming. Mamanuca Islands, Fiji"
  }
]