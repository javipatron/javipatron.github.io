[
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "BIO",
    "section": "",
    "text": "After graduating from Universidad Iberoamericana as a Mechanical and Electrical Engineer in Mexico City, I started my professional career as a Field Engineer at General Electric. I was the pioneer commissioner and field engineer for the smart lighting product development in the Latin America region. After 5 years, the GE Current smart controls and energy management software grew significantly and I ended up as the team leader for the global commissioning and tech-support team.\nThen in 2021, I shifted my career to become the team manager and navigator of a racing sailboat to cross the Pacific. This incredible ocean crossing not only transformed my life but also intensified my passion to pursue a Master’s degree in Environmental Data Science at the renowned Bren School of Environmental Science & Management at UC Santa Barbara.\nAs an individual, I embody energy and proactiveness, and now, as an Environmental Data Scientist, my vision is to empower cutting-edge technology companies by enhancing their data collection and analysis practices, enabling them to derive actionable insights and make a positive impact on the environment. My main passion and career vision is to actively contribute and empower solutions that make a positive impact. This involves engaging in nature-based approaches such as GHG accounting, monitoring carbon stock changes, promoting mangrove restoration and conservation for blue carbon initiatives, and studying the relationships between coastal ecosystems like mangroves, coral reefs, kelp forests and marine protected areas (MPAs) in addressing climate change and its effects.\nAs Sylvia Earle once said; “The ocean is in trouble, but together, we can turn the tide. We have a choice: to be the generation that destroys the ocean or the generation that saves it. Let’s choose wisely.”\n\n\n\nImage: Coral Farming. Mamanuca Islands, Fiji"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/machine-learning/index.html",
    "href": "posts/machine-learning/index.html",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "In this lab, we will apply our machine learning knowledge to train models that can predict dissolved inorganic carbon concentrations in water samples collected by the California Cooperative Oceanic Fisheries Investigations program (CalCOFI). After developing our model, we will test its accuracy using a separate data set and compete against our classmates in a Kaggle competition. Here, you can find more details about the competition.\n\n\n\n\n\nDissolved inorganic carbon (DIC) is present in all natural waters. The concentration of DIC varies from less than 20 μM in acidic soft waters to more than 5000 μM in highly alkaline hard waters, but ranges between 100 and 1000 μM in most systems. DIC is usually the most abundant form of C in water. DIC consists of three main constituents: free CO2 (a gas), the bicarbonate ion (HCO3−), and the carbonate ion (CO32 −). Although CO2, like other gases, readily exchanges with the atmosphere, even the surface waters of most inland systems are far from equilibrium and are usually supersaturated with respect to the atmosphere. A number of factors cause this disequilibrium. Reference\nSome tasks will be covered in this lab are; explore the data, pre-processing, choose a model algorithm, tune relevant parameters with cross validation, and create my own prediction.\n\n\nThis data set was downloaded from the CalCOFI data portal. Bottle and cast data was downloaded and merged, then relevant variables were selected. I will use this data (train.csv) to train a model that will predict dissolved inorganic carbon (DIC) content in the water samples.\n\n\n\ntrain.csv - the training set\ntest.csv - the test set (without the dic )\nsample_submission.csv - a sample submission file in the correct format Columns A database description is available here: https://calcofi.org/data/oceanographic-data/bottle-database/\n\n\n\n\n\n\nCode\nnames(test)\n\n\n [1] \"lat_dec\"           \"lon_dec\"           \"no2u_m\"           \n [4] \"no3u_m\"            \"nh3u_m\"            \"r_temp\"           \n [7] \"r_depth\"           \"r_sal\"             \"r_dynht\"          \n[10] \"r_nuts\"            \"r_oxy_micromol_kg\" \"po4u_m\"           \n[13] \"si_o3u_m\"          \"ta1_x\"             \"salinity1\"        \n[16] \"temperature_deg_c\"\n\n\n\n\n\n\nlat_dec - Latitude North (Degrees N)\nlon_dec - Longitude in (-180 - 180 Degrees E or W)\nno2u_m - Micromoles nitrite per liter of seawater\nno3u_m - Micromoles nitrate per liter of seawater\nnh3u_m - Micromoles ammonia per liter of seawater\nr_temp - Reported (Potential) temperature in degrees (°C)\nr_depth - Reported Depth (from pressure) in meters (m)\nr_sal - Reported Salinity (from Specific Volume anomoly, (M^3/Kg)\nr_dynht - Reported Dynamic Height in units of dynamic meters (work per unit mass)\nr_nuts - Reported ammonium concentration\nr_oxy_micromol_kg - Reported Oxygen micro-moles/kilogram\npo4u_m - Micro-moles Phosphate per liter of seawater\nsi_o3u_m- Micro-moles Silicate per liter of seawater\nta1_x - Total Alkalinity micro-moles per kilogram solution\nsalinity1 - Salinity\ntemperature_deg_c Temperature in Celsius (°C)\ndic - Dissolved inorganic carbon (Outcome)\n\n\n\n\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into the number of groups you desire making them roughly in equal size. In this case we will split into 10 groups.\n\n\nCode\nset.seed(1)\n\n#Split the data\ndata_split &lt;- initial_split(train, strata = \"dic\")\n\ndata_cv &lt;- train |&gt; \n  vfold_cv(v = 10)\n\n\n\n\n\nFor the pre-processing step we will create a recipe to prepare and normalize our data so we can proceed with the model. In this case we are interested in predicting the outcome variable dic which is the Inorganic Carbon in micro-moles\n\n\nCode\ndic_recipe &lt;- recipe(dic ~ .,\n                     data = train) |&gt;\n  step_normalize() |&gt; \n  prep() |&gt; \n  bake(new_data = train)\n\n\n\n\n\n\nWe will go with the XGBoost model. For short is Extreme Gradient Boosting. This model combines the predictions of multiple decision tree models in a systematic way to improve the accuracy of the final prediction. XGBoost works by iteratively adding decision trees to the model, where each subsequent tree tries to correct the errors of the previous tree. This model is highly customizable and allows for the tuning of many different parameters, making it a popular choice among data scientists and machine learning practitioner thanks to its speed and accuracy.\nFor this lab purpose we will first create a model tuning the learn_rate. The learn rate controls the step size for each iteration and its crucial to find the optimal learning rate at the beginning to ensure that the model converges efficiently and effectively. Secondly we will create a second model grid, tuning the number of trees, tree_depth, min_n, loss_reduction. Tuning this features such as the number of trees, tree depth, min_n, and loss reduction, we can ensure that our model is robust and able to capture complex relationships within the data. These features help prevent over-fitting and under-fitting by controlling the complexity of the model. Thirdly we will move with the stochastic parameters such as sample_size, mtry, and stop_iter, which they are essential for controlling the randomness in the training process preventing over-fitting by introducing randomness in the model’s selection of features and observations during each iteration. This ensures that the model does not memorize the training data and can generalize well to unseen data.\n\n\nFollowing the XGBoost tuning strategy, first conduct tuning on just the learn_rate parameter:\n\n\nCode\nfirst_model &lt;-parsnip::boost_tree( # Set the classification for the dic variable\n  trees = 1000,\n  learn_rate = tune(),\n) |&gt; \n  set_engine(\"xgboost\") |&gt; # Set the model that you want to use.\n  set_mode(\"regression\") # Set the mode depending on your outcome variable\n\n\nSet up a grid to tune our first model to set the object where the model runs all possible combinations of the specified hyper-parameter values. Then this grid is used to train and evaluate the model using each combination of hyper-parameters to determine which one results in the best performance on the validation set.\n\n\nCode\nset.seed(1)\nfirst_grid &lt;- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n\nCreate a workflow for this process: workflow() is a function in the Tidy Models package in R that enables users to define and execute a pipeline of data pre-processing, modeling, and post-processing steps.\n\n\nCode\nfirst_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(first_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nCreate a tuning process: Here we will tune the first grid with tune_grid. (tune_grid() runs a set of performance metrics (e.g. accuracy or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data.\n\n\nCode\ndoParallel::registerDoParallel() # to build trees in parallel\n\nfirst_tuned &lt;- tune_grid(\n  object = first_workflow,\n  resamples = data_cv,\n  grid      = first_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\n\n\nCode\n#first_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfirst_model_best &lt;- first_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\n\nkable(first_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model Result of the First tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model Result of the First tune\n\n\nlearn_rate\n.config\n\n\n\n\n0.021\nPreprocessor1_Model03\n\n\n\n\n\n\n\n\n\n\n\nThe tune() function is used to perform hyper-parameter tuning for machine learning models, where the goal is to find the optimal values for one or more hyper-parameters that maximize the performance of the model on a given data set.\nThis three parameters control the complexity of the trees being built and the stopping criteria for each tree:\n\ntree_depth: This parameter specifies the maximum depth of each tree in the boosting process. Increasing the tree depth can lead to more complex models that may capture more intricate relationships in the data, but it can also increase the risk of over-fitting.\nmin_n: This parameter specifies the minimum number of observations required in each terminal node (leaf) of a tree. Setting a higher value for this parameter can prevent the model from fitting to noise or outlines, but it may result in less flexible models.\nloss_reduction: This parameter determines the minimum amount of loss reduction required to further split a node in the tree. A higher value for this parameter can result in less complex models and faster convergence, but it may result in a model that is under-fitting the data.\ntrees: This parameter control the number of trees used in the boosting process. Increasing the number of trees typically improves the performance of the model on the training set, but too many trees can lead to over-fitting.\n\n\n\nCode\nsecond_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = tune(),\n  learn_rate = first_model_best$learn_rate,\n  min_n = tune(), \n  tree_depth = tune(), \n  loss_reduction = tune() \n  ) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate the workflow\n\n\nCode\nsecond_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(second_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\nCode\nsecond_grid &lt;- dials::grid_max_entropy(\n  min_n(),\n  tree_depth(),\n  loss_reduction(),\n  trees(), \n  size = 100) #GRID specs from the book\n\n\nUse the tune_grid to feed all alternatives and create the tuning process.\n\n\nCode\ndoParallel::registerDoParallel() # this function helps to build trees in parallel\nseconed_tuned &lt;- tune_grid(\n  object = second_workflow,\n  resamples = data_cv,\n  grid      = second_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nCode\n#seconed_tuned %&gt;% tune::show_best(metric = \"rmse\")\n\nsecond_model_best &lt;- seconed_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(second_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the second tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the second tune\n\n\ntrees\nmin_n\ntree_depth\nloss_reduction\n.config\n\n\n\n\n868\n3\n7\n5.601\nPreprocessor1_Model006\n\n\n\n\n\n\n\nIn this case the show_best() displays the optimal hyperparameters and the corresponding performance metric(s) from the tuning process.\n\n\n\nNow, for our third model we will randomly sample observations and predictors (features) at each iteration, which will help us to reduce overfitting and improve the generalization performance of the model. This process is called Stochastic Gradient Boosting (SGB) and we will tune tune() the hyperparameters of sample_size, mtry and stop_iter:\n\nsample_size: This argument specifies the fraction (or absolute number) of observations to be sampled at each iteration. The default value is 1, which corresponds to sampling all observations. Setting sample_size to a value less than 1 results in stochastic gradient boosting, where each iteration uses a randomly selected subset of the data.\nmtry: This argument specifies the number of randomly selected predictors (features) to be used at each split in the tree. The default value is NULL, which corresponds to using all predictors. Setting mtry to a value less than the total number of predictors results in stochastic feature selection, where each split uses a randomly selected subset of the predictors.\nstop_iter: This argument in boost_tree() specifies the stopping criterion for the boosting process. This hyperparameter determines when the boosting process should be terminated based on a certain criterion, such as the improvement in the loss function or the number of consecutive iterations with no improvement.\n\n\n\nCode\nthird_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees,\n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth,\n  min_n = second_model_best$min_n,\n  loss_reduction = second_model_best$loss_reduction,\n  sample_size = tune(),\n  mtry = tune(),\n  stop_iter = tune()\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nSet up the new parameters with new range of sample proportions to improve our model\n\n\nCode\nthird_params &lt;- dials::parameters(\n  stop_iter(c(5, 50)),\n  sample_size = sample_prop(c(0.4, 0.9)),\n  finalize(mtry(), train))\n\n\nCreate a workflow\n\n\nCode\nthird_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(third_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid\n\n\nCode\nthird_grid &lt;- dials::grid_max_entropy(third_params, size = 100) \n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nCode\ndoParallel::registerDoParallel() # to build trees in parallel\n\nthird_tuned &lt;- tune_grid(\n  object = third_workflow,\n  resamples = data_cv,\n  grid      = third_grid ,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nCode\n#third_tuned %&gt;% tune::show_best(metric = \"rmse\")\nthird_model_best &lt;- third_tuned %&gt;% tune::select_best(metric = \"rmse\")\nkable(third_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the Third Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the Third Tune\n\n\nmtry\nsample_size\nstop_iter\n.config\n\n\n\n\n6\n0.679\n11\nPreprocessor1_Model093\n\n\n\n\n\n\n\nAs we can see in the results above for the Stochastic Parameteres, which are this randomized selections to reduce overfitting, our new optimal parameter are:\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 6\n  trees = second_model_best$trees\n  min_n = second_model_best$min_n\n  tree_depth = second_model_best$tree_depth\n  learn_rate = first_model_best$learn_rate\n  loss_reduction = second_model_best$loss_reduction\n  sample_size = 0.678898499789648\n  stop_iter = 11\n\nComputational engine: xgboost \n\n\n\n\n\n\nAssemble your final workflow will all of your optimized parameters and do a final fit.\n\n\n\nCode\nfinal_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees, \n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth, \n  min_n = second_model_best$min_n, \n  loss_reduction = second_model_best$loss_reduction, \n  sample_size = third_model_best$sample_size,\n  mtry = third_model_best$mtry,\n  stop_iter = third_model_best$stop_iter\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate your Workflow\n\n\nCode\nfinal_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(final_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nCode\n# Run your final model\nfinal_tuned &lt;- tune_grid(\n  object = final_workflow,\n  resamples = data_cv,\n  metrics   = metric_set(rmse))\n\n\nTake a look at the results\n\n\nCode\n#final_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfinal_metrics &lt;- final_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(final_metrics,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Final Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Final Tune\n\n\n.config\n\n\n\n\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\nCode\nfinal_boost &lt;- finalize_model(final_model,\n                             select_best(final_tuned))\nfinal_boost_fit &lt;- last_fit(final_boost, dic ~ ., data_split)\n\n\n\n\n\nShow all metrics\n\n\nCode\nfinal_metrics &lt;- final_boost_fit %&gt;% \n  collect_metrics(summarise = TRUE) %&gt;%\n  mutate(param_tuned = \"final\")\n\n\n\n\n\n\nRMSE: On average, the predicted DIC values from your model are off by about 5.610567 units of DIC. This means that if the actual DIC value is 100, your model might predict a value between 105.610567and 94.389433, with an average error of 5.610567 units. It’s important to note that this is an average error across all predictions, and there may be individual predictions that are much more or less accurate than this.\nR²: Your model explains 99.7510634% of the variance in DIC. This means that the model fits the data very well and can be used to make accurate predictions. A high R² value indicates that there is a strong linear relationship between the predictors and the outcome, and that the model is able to capture this relationship well.\n\n\n\n\n\n\nCode\ntest_id &lt;- read_csv(\"/Users/javipatron/Documents/MEDS/Personal/personal_webpage/javipatron.github.io/posts/machine-learning/data/test.csv\") |&gt; \n  select(id)\n\n\nRows: 485 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): id, Lat_Dec, Lon_Dec, NO2uM, NO3uM, NH3uM, R_TEMP, R_Depth, R_Sal,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nresult &lt;- final_workflow |&gt; \n  fit(data = train) |&gt; \n  predict(new_data = test)\n\npredict_test &lt;- cbind(test, result) |&gt; \n  cbind(test_id) |&gt; \n  rename(DIC = .pred) |&gt; \n  select(c(id, DIC)) \n\n#This is the line where you can add your final results\n#write_csv(predict_test, \"final_results.csv\")\n\n\nCreate a graph and prints the result of RSME error (The lower the better), and the rsq value. This can be seen in the table above.\n\n\nCode\nggplot(final_metrics, aes(x = .metric, y = .estimate, fill = .metric)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"Metric\", y = \"Value\", title = \"Model Performance\")"
  },
  {
    "objectID": "posts/machine-learning/index.html#dataset-description",
    "href": "posts/machine-learning/index.html#dataset-description",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "This data set was downloaded from the CalCOFI data portal. Bottle and cast data was downloaded and merged, then relevant variables were selected. I will use this data (train.csv) to train a model that will predict dissolved inorganic carbon (DIC) content in the water samples.\n\n\n\ntrain.csv - the training set\ntest.csv - the test set (without the dic )\nsample_submission.csv - a sample submission file in the correct format Columns A database description is available here: https://calcofi.org/data/oceanographic-data/bottle-database/\n\n\n\n\n\n\nCode\nnames(test)\n\n\n [1] \"lat_dec\"           \"lon_dec\"           \"no2u_m\"           \n [4] \"no3u_m\"            \"nh3u_m\"            \"r_temp\"           \n [7] \"r_depth\"           \"r_sal\"             \"r_dynht\"          \n[10] \"r_nuts\"            \"r_oxy_micromol_kg\" \"po4u_m\"           \n[13] \"si_o3u_m\"          \"ta1_x\"             \"salinity1\"        \n[16] \"temperature_deg_c\"\n\n\n\n\n\n\nlat_dec - Latitude North (Degrees N)\nlon_dec - Longitude in (-180 - 180 Degrees E or W)\nno2u_m - Micromoles nitrite per liter of seawater\nno3u_m - Micromoles nitrate per liter of seawater\nnh3u_m - Micromoles ammonia per liter of seawater\nr_temp - Reported (Potential) temperature in degrees (°C)\nr_depth - Reported Depth (from pressure) in meters (m)\nr_sal - Reported Salinity (from Specific Volume anomoly, (M^3/Kg)\nr_dynht - Reported Dynamic Height in units of dynamic meters (work per unit mass)\nr_nuts - Reported ammonium concentration\nr_oxy_micromol_kg - Reported Oxygen micro-moles/kilogram\npo4u_m - Micro-moles Phosphate per liter of seawater\nsi_o3u_m- Micro-moles Silicate per liter of seawater\nta1_x - Total Alkalinity micro-moles per kilogram solution\nsalinity1 - Salinity\ntemperature_deg_c Temperature in Celsius (°C)\ndic - Dissolved inorganic carbon (Outcome)\n\n\n\n\nV-fold cross-validation (also known as k-fold cross-validation) randomly splits the data into the number of groups you desire making them roughly in equal size. In this case we will split into 10 groups.\n\n\nCode\nset.seed(1)\n\n#Split the data\ndata_split &lt;- initial_split(train, strata = \"dic\")\n\ndata_cv &lt;- train |&gt; \n  vfold_cv(v = 10)\n\n\n\n\n\nFor the pre-processing step we will create a recipe to prepare and normalize our data so we can proceed with the model. In this case we are interested in predicting the outcome variable dic which is the Inorganic Carbon in micro-moles\n\n\nCode\ndic_recipe &lt;- recipe(dic ~ .,\n                     data = train) |&gt;\n  step_normalize() |&gt; \n  prep() |&gt; \n  bake(new_data = train)"
  },
  {
    "objectID": "posts/machine-learning/index.html#select-our-model",
    "href": "posts/machine-learning/index.html#select-our-model",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "We will go with the XGBoost model. For short is Extreme Gradient Boosting. This model combines the predictions of multiple decision tree models in a systematic way to improve the accuracy of the final prediction. XGBoost works by iteratively adding decision trees to the model, where each subsequent tree tries to correct the errors of the previous tree. This model is highly customizable and allows for the tuning of many different parameters, making it a popular choice among data scientists and machine learning practitioner thanks to its speed and accuracy.\nFor this lab purpose we will first create a model tuning the learn_rate. The learn rate controls the step size for each iteration and its crucial to find the optimal learning rate at the beginning to ensure that the model converges efficiently and effectively. Secondly we will create a second model grid, tuning the number of trees, tree_depth, min_n, loss_reduction. Tuning this features such as the number of trees, tree depth, min_n, and loss reduction, we can ensure that our model is robust and able to capture complex relationships within the data. These features help prevent over-fitting and under-fitting by controlling the complexity of the model. Thirdly we will move with the stochastic parameters such as sample_size, mtry, and stop_iter, which they are essential for controlling the randomness in the training process preventing over-fitting by introducing randomness in the model’s selection of features and observations during each iteration. This ensures that the model does not memorize the training data and can generalize well to unseen data.\n\n\nFollowing the XGBoost tuning strategy, first conduct tuning on just the learn_rate parameter:\n\n\nCode\nfirst_model &lt;-parsnip::boost_tree( # Set the classification for the dic variable\n  trees = 1000,\n  learn_rate = tune(),\n) |&gt; \n  set_engine(\"xgboost\") |&gt; # Set the model that you want to use.\n  set_mode(\"regression\") # Set the mode depending on your outcome variable\n\n\nSet up a grid to tune our first model to set the object where the model runs all possible combinations of the specified hyper-parameter values. Then this grid is used to train and evaluate the model using each combination of hyper-parameters to determine which one results in the best performance on the validation set.\n\n\nCode\nset.seed(1)\nfirst_grid &lt;- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n\nCreate a workflow for this process: workflow() is a function in the Tidy Models package in R that enables users to define and execute a pipeline of data pre-processing, modeling, and post-processing steps.\n\n\nCode\nfirst_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(first_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nCreate a tuning process: Here we will tune the first grid with tune_grid. (tune_grid() runs a set of performance metrics (e.g. accuracy or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data.\n\n\nCode\ndoParallel::registerDoParallel() # to build trees in parallel\n\nfirst_tuned &lt;- tune_grid(\n  object = first_workflow,\n  resamples = data_cv,\n  grid      = first_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\n\n\nCode\n#first_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfirst_model_best &lt;- first_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\n\nkable(first_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model Result of the First tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model Result of the First tune\n\n\nlearn_rate\n.config\n\n\n\n\n0.021\nPreprocessor1_Model03"
  },
  {
    "objectID": "posts/machine-learning/index.html#tune-second-parameters",
    "href": "posts/machine-learning/index.html#tune-second-parameters",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "The tune() function is used to perform hyper-parameter tuning for machine learning models, where the goal is to find the optimal values for one or more hyper-parameters that maximize the performance of the model on a given data set.\nThis three parameters control the complexity of the trees being built and the stopping criteria for each tree:\n\ntree_depth: This parameter specifies the maximum depth of each tree in the boosting process. Increasing the tree depth can lead to more complex models that may capture more intricate relationships in the data, but it can also increase the risk of over-fitting.\nmin_n: This parameter specifies the minimum number of observations required in each terminal node (leaf) of a tree. Setting a higher value for this parameter can prevent the model from fitting to noise or outlines, but it may result in less flexible models.\nloss_reduction: This parameter determines the minimum amount of loss reduction required to further split a node in the tree. A higher value for this parameter can result in less complex models and faster convergence, but it may result in a model that is under-fitting the data.\ntrees: This parameter control the number of trees used in the boosting process. Increasing the number of trees typically improves the performance of the model on the training set, but too many trees can lead to over-fitting.\n\n\n\nCode\nsecond_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = tune(),\n  learn_rate = first_model_best$learn_rate,\n  min_n = tune(), \n  tree_depth = tune(), \n  loss_reduction = tune() \n  ) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate the workflow\n\n\nCode\nsecond_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(second_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space\n\n\nCode\nsecond_grid &lt;- dials::grid_max_entropy(\n  min_n(),\n  tree_depth(),\n  loss_reduction(),\n  trees(), \n  size = 100) #GRID specs from the book\n\n\nUse the tune_grid to feed all alternatives and create the tuning process.\n\n\nCode\ndoParallel::registerDoParallel() # this function helps to build trees in parallel\nseconed_tuned &lt;- tune_grid(\n  object = second_workflow,\n  resamples = data_cv,\n  grid      = second_grid,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nCode\n#seconed_tuned %&gt;% tune::show_best(metric = \"rmse\")\n\nsecond_model_best &lt;- seconed_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(second_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the second tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the second tune\n\n\ntrees\nmin_n\ntree_depth\nloss_reduction\n.config\n\n\n\n\n868\n3\n7\n5.601\nPreprocessor1_Model006\n\n\n\n\n\n\n\nIn this case the show_best() displays the optimal hyperparameters and the corresponding performance metric(s) from the tuning process."
  },
  {
    "objectID": "posts/machine-learning/index.html#tune-stochastic-parameters",
    "href": "posts/machine-learning/index.html#tune-stochastic-parameters",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "Now, for our third model we will randomly sample observations and predictors (features) at each iteration, which will help us to reduce overfitting and improve the generalization performance of the model. This process is called Stochastic Gradient Boosting (SGB) and we will tune tune() the hyperparameters of sample_size, mtry and stop_iter:\n\nsample_size: This argument specifies the fraction (or absolute number) of observations to be sampled at each iteration. The default value is 1, which corresponds to sampling all observations. Setting sample_size to a value less than 1 results in stochastic gradient boosting, where each iteration uses a randomly selected subset of the data.\nmtry: This argument specifies the number of randomly selected predictors (features) to be used at each split in the tree. The default value is NULL, which corresponds to using all predictors. Setting mtry to a value less than the total number of predictors results in stochastic feature selection, where each split uses a randomly selected subset of the predictors.\nstop_iter: This argument in boost_tree() specifies the stopping criterion for the boosting process. This hyperparameter determines when the boosting process should be terminated based on a certain criterion, such as the improvement in the loss function or the number of consecutive iterations with no improvement.\n\n\n\nCode\nthird_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees,\n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth,\n  min_n = second_model_best$min_n,\n  loss_reduction = second_model_best$loss_reduction,\n  sample_size = tune(),\n  mtry = tune(),\n  stop_iter = tune()\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nSet up the new parameters with new range of sample proportions to improve our model\n\n\nCode\nthird_params &lt;- dials::parameters(\n  stop_iter(c(5, 50)),\n  sample_size = sample_prop(c(0.4, 0.9)),\n  finalize(mtry(), train))\n\n\nCreate a workflow\n\n\nCode\nthird_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(third_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nSet up a tuning grid\n\n\nCode\nthird_grid &lt;- dials::grid_max_entropy(third_params, size = 100) \n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nCode\ndoParallel::registerDoParallel() # to build trees in parallel\n\nthird_tuned &lt;- tune_grid(\n  object = third_workflow,\n  resamples = data_cv,\n  grid      = third_grid ,\n  metrics   = metric_set(rmse),\n  control   = control_grid(verbose = TRUE))\n\n\nShow the performance of the best models\n\n\nCode\n#third_tuned %&gt;% tune::show_best(metric = \"rmse\")\nthird_model_best &lt;- third_tuned %&gt;% tune::select_best(metric = \"rmse\")\nkable(third_model_best,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Model for the Third Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Model for the Third Tune\n\n\nmtry\nsample_size\nstop_iter\n.config\n\n\n\n\n6\n0.679\n11\nPreprocessor1_Model093\n\n\n\n\n\n\n\nAs we can see in the results above for the Stochastic Parameteres, which are this randomized selections to reduce overfitting, our new optimal parameter are:\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 6\n  trees = second_model_best$trees\n  min_n = second_model_best$min_n\n  tree_depth = second_model_best$tree_depth\n  learn_rate = first_model_best$learn_rate\n  loss_reduction = second_model_best$loss_reduction\n  sample_size = 0.678898499789648\n  stop_iter = 11\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/machine-learning/index.html#finalize-workflow-and-make-final-prediction",
    "href": "posts/machine-learning/index.html#finalize-workflow-and-make-final-prediction",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "Assemble your final workflow will all of your optimized parameters and do a final fit.\n\n\n\nCode\nfinal_model &lt;-parsnip::boost_tree(\n  mode = \"regression\",\n  trees = second_model_best$trees, \n  learn_rate = first_model_best$learn_rate,\n  tree_depth = second_model_best$tree_depth, \n  min_n = second_model_best$min_n, \n  loss_reduction = second_model_best$loss_reduction, \n  sample_size = third_model_best$sample_size,\n  mtry = third_model_best$mtry,\n  stop_iter = third_model_best$stop_iter\n) |&gt; \n  set_engine(\"xgboost\")\n\n\nCreate your Workflow\n\n\nCode\nfinal_workflow &lt;- workflows::workflow() %&gt;%\n  add_model(final_model) %&gt;% \n  add_formula(dic ~ .)\n\n\nUse the tune_grid to feed all alternatives and create the tuning process\n\n\nCode\n# Run your final model\nfinal_tuned &lt;- tune_grid(\n  object = final_workflow,\n  resamples = data_cv,\n  metrics   = metric_set(rmse))\n\n\nTake a look at the results\n\n\nCode\n#final_tuned %&gt;% tune::show_best(metric = \"rmse\")\nfinal_metrics &lt;- final_tuned %&gt;% tune::select_best(metric = \"rmse\")\n\nkable(final_metrics,\n      digits = round(3),\n      align = \"c\",\n      caption = \"Best Final Tune\") |&gt; \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nBest Final Tune\n\n\n.config\n\n\n\n\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\n\nCode\nfinal_boost &lt;- finalize_model(final_model,\n                             select_best(final_tuned))\nfinal_boost_fit &lt;- last_fit(final_boost, dic ~ ., data_split)"
  },
  {
    "objectID": "posts/machine-learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make",
    "href": "posts/machine-learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "Show all metrics\n\n\nCode\nfinal_metrics &lt;- final_boost_fit %&gt;% \n  collect_metrics(summarise = TRUE) %&gt;%\n  mutate(param_tuned = \"final\")"
  },
  {
    "objectID": "posts/machine-learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make-1",
    "href": "posts/machine-learning/index.html#how-well-did-your-model-perform-what-types-of-errors-did-it-make-1",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "RMSE: On average, the predicted DIC values from your model are off by about 5.610567 units of DIC. This means that if the actual DIC value is 100, your model might predict a value between 105.610567and 94.389433, with an average error of 5.610567 units. It’s important to note that this is an average error across all predictions, and there may be individual predictions that are much more or less accurate than this.\nR²: Your model explains 99.7510634% of the variance in DIC. This means that the model fits the data very well and can be used to make accurate predictions. A high R² value indicates that there is a strong linear relationship between the predictors and the outcome, and that the model is able to capture this relationship well."
  },
  {
    "objectID": "posts/machine-learning/index.html#now-lets-add-the-predicted-data-to-our-testing-data",
    "href": "posts/machine-learning/index.html#now-lets-add-the-predicted-data-to-our-testing-data",
    "title": "Inorganic Carbon in Machine Leaning",
    "section": "",
    "text": "Code\ntest_id &lt;- read_csv(\"/Users/javipatron/Documents/MEDS/Personal/personal_webpage/javipatron.github.io/posts/machine-learning/data/test.csv\") |&gt; \n  select(id)\n\n\nRows: 485 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): id, Lat_Dec, Lon_Dec, NO2uM, NO3uM, NH3uM, R_TEMP, R_Depth, R_Sal,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nresult &lt;- final_workflow |&gt; \n  fit(data = train) |&gt; \n  predict(new_data = test)\n\npredict_test &lt;- cbind(test, result) |&gt; \n  cbind(test_id) |&gt; \n  rename(DIC = .pred) |&gt; \n  select(c(id, DIC)) \n\n#This is the line where you can add your final results\n#write_csv(predict_test, \"final_results.csv\")\n\n\nCreate a graph and prints the result of RSME error (The lower the better), and the rsq value. This can be seen in the table above.\n\n\nCode\nggplot(final_metrics, aes(x = .metric, y = .estimate, fill = .metric)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"Metric\", y = \"Value\", title = \"Model Performance\")"
  },
  {
    "objectID": "posts/global-fishing-watch/index.html",
    "href": "posts/global-fishing-watch/index.html",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "Javier Patrón jpatron@ucsb.edu\nJessica French jfrench@bren.ucsb.edu\nPol Carbó Mestre pcarbomestre@ucsb.edu\n\n\n\n\n\nPurpose(#purpose)\nDataset Description(#overview)\nData I/O(#io)\nMetadata Display and Basic Visualization(#display)\nUse Case Examples(#usecases)\nCreate Binder Environment(#binder)\nReferences\n\n\n\n\nThe purpose of this notebook is to explore Global Fishing Watch’s dataset showing daily fishing effort as inferred fishing hours daily. This notebook will show how to read in the dataset, visualize the data using google earth engine, and give an overview of how the data can be used to explore differences in fishing effort within and outside Peru’s EEZ and how fishing effort is impacted by El Niño Southern Oscillation (ENSO) events.\n\n\n\nThe Global Fishing Watch (GFW) provides an open platform to access Automatic Identification System (AIS) data from commercial fishing activities. The AIS is a tracking system that uses transceivers on ships to broadcast vessel information such as unique identification, position, course, and speed. AIS is integrated into all classes of vessels as a collision avoidance tool. However, the GFW collects and processes raw AIS data related to fishing activities to improve records and assign additional information, such as the distance from shore, depth, etc. Then, with the use of machine learning models, they characterize vessels and fishing activities, which constitute some of the products available in their API.\n\n\n\n\n\nOne of the most interesting products that the GFW API offers is estimates of fishing effort derived from AIS data. GFW uses machine learning models to classify fishing vessels and predict when they are fishing. First, they identify fishing vessels in the AIS system. Then the vessel is characterized using the best available information and their fleet registry data combined with a characterization model trained on 45,441 marine vessels (both fishing and non-fishing) matched to official fleet registries. Then, GFW estimates the vessel’s fishing time and location based on its movement patterns. To do so, a fishing detection model was trained on AIS data from 503 ships and identified fishing activity with &gt;90% accuracy. The model predicts a score for every AIS position in their database to distinguish fishing positions from non-fishing positions (i.e., navigation time). When the model identifies fishing activity, the time associated with that AIS position is registered as apparent fishing activity. More details about the model can be found on the following GitHub repository (link).\n{fig-align= “center” width= “40%”}\n\n\n\n\n\nOnce the fishing vessels are identified and their AIS positions have been assigned as fishing positions, the apparent fishing effort can be calculated for any area by summarizing the fishing hours for all fishing vessels in that area. The resulting maps of apparent fishing effort are created by rasterizing all AIS positions into a grid and calculating the total fishing activity in each grid cell. For the present project we will access this type processed data.\nPre-processed AIS data can be accessed from their R package “gfwr” or downloaded from their website as .cvs files. For this project, we will use some of their existing products related to fishing effort. The data can be accessed from Google Big Query in a less processed format and through Google Earth Engine (GEE) for two data subproducts daily fishing hours and daily vessel hours. For accessibility reasons, we will focus on the GEE data related to fishing hours.\nEach image in the collection contains daily rasters of fishing effort measured in hours of inferred fishing activity per square kilometer. Data is available for a given flag state and day, over a 5 years period (2012-2017), where each band represent a fishing gear type. The following figure summarizes the image resolution and the available bands.\nThe data used belongs to the first global assessment of commercial fishing activity, published in Science by GFW (2018).\n\n\n\nLoad necessary packages.\n\n\nCode\n#1\n# Import packages\nimport ee\nimport geemap.foliumap as geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nfrom itertools import chain\nimport os\nimport rasterio\n\n\nAuthenticate and initialize google earth engine.\n\n\nCode\n#2\n# Authenticate google earth engine\n#ee.Authenticate()\n# Initialize google earth engine \nee.Initialize()\n#Read in the data from google earth engine and filter metadata to include all countries."
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#table-of-contents",
    "href": "posts/global-fishing-watch/index.html#table-of-contents",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "Purpose(#purpose)\nDataset Description(#overview)\nData I/O(#io)\nMetadata Display and Basic Visualization(#display)\nUse Case Examples(#usecases)\nCreate Binder Environment(#binder)\nReferences"
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#notebook-purpose",
    "href": "posts/global-fishing-watch/index.html#notebook-purpose",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "The purpose of this notebook is to explore Global Fishing Watch’s dataset showing daily fishing effort as inferred fishing hours daily. This notebook will show how to read in the dataset, visualize the data using google earth engine, and give an overview of how the data can be used to explore differences in fishing effort within and outside Peru’s EEZ and how fishing effort is impacted by El Niño Southern Oscillation (ENSO) events."
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#dataset-description",
    "href": "posts/global-fishing-watch/index.html#dataset-description",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "The Global Fishing Watch (GFW) provides an open platform to access Automatic Identification System (AIS) data from commercial fishing activities. The AIS is a tracking system that uses transceivers on ships to broadcast vessel information such as unique identification, position, course, and speed. AIS is integrated into all classes of vessels as a collision avoidance tool. However, the GFW collects and processes raw AIS data related to fishing activities to improve records and assign additional information, such as the distance from shore, depth, etc. Then, with the use of machine learning models, they characterize vessels and fishing activities, which constitute some of the products available in their API.\n\n\n\n\n\nOne of the most interesting products that the GFW API offers is estimates of fishing effort derived from AIS data. GFW uses machine learning models to classify fishing vessels and predict when they are fishing. First, they identify fishing vessels in the AIS system. Then the vessel is characterized using the best available information and their fleet registry data combined with a characterization model trained on 45,441 marine vessels (both fishing and non-fishing) matched to official fleet registries. Then, GFW estimates the vessel’s fishing time and location based on its movement patterns. To do so, a fishing detection model was trained on AIS data from 503 ships and identified fishing activity with &gt;90% accuracy. The model predicts a score for every AIS position in their database to distinguish fishing positions from non-fishing positions (i.e., navigation time). When the model identifies fishing activity, the time associated with that AIS position is registered as apparent fishing activity. More details about the model can be found on the following GitHub repository (link).\n{fig-align= “center” width= “40%”}\n\n\n\n\n\nOnce the fishing vessels are identified and their AIS positions have been assigned as fishing positions, the apparent fishing effort can be calculated for any area by summarizing the fishing hours for all fishing vessels in that area. The resulting maps of apparent fishing effort are created by rasterizing all AIS positions into a grid and calculating the total fishing activity in each grid cell. For the present project we will access this type processed data.\nPre-processed AIS data can be accessed from their R package “gfwr” or downloaded from their website as .cvs files. For this project, we will use some of their existing products related to fishing effort. The data can be accessed from Google Big Query in a less processed format and through Google Earth Engine (GEE) for two data subproducts daily fishing hours and daily vessel hours. For accessibility reasons, we will focus on the GEE data related to fishing hours.\nEach image in the collection contains daily rasters of fishing effort measured in hours of inferred fishing activity per square kilometer. Data is available for a given flag state and day, over a 5 years period (2012-2017), where each band represent a fishing gear type. The following figure summarizes the image resolution and the available bands.\nThe data used belongs to the first global assessment of commercial fishing activity, published in Science by GFW (2018)."
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#dataset-inputoutput",
    "href": "posts/global-fishing-watch/index.html#dataset-inputoutput",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "Load necessary packages.\n\n\nCode\n#1\n# Import packages\nimport ee\nimport geemap.foliumap as geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nfrom itertools import chain\nimport os\nimport rasterio\n\n\nAuthenticate and initialize google earth engine.\n\n\nCode\n#2\n# Authenticate google earth engine\n#ee.Authenticate()\n# Initialize google earth engine \nee.Initialize()\n#Read in the data from google earth engine and filter metadata to include all countries."
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#metadata-display",
    "href": "posts/global-fishing-watch/index.html#metadata-display",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Metadata Display",
    "text": "Metadata Display\nWe can look at the metadata and bands in a couple different ways. The code below is a little overwhelming and difficult to search through.\n\n\nCode\n#7\n# Extract the first image so we can look at info about the data in general. \nimage_test = dataset.first()\ninfo = image_test.getInfo()\nprint(info)\n\n\n{'type': 'Image', 'bands': [{'id': 'drifting_longlines', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'fixed_gear', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'other_fishing', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'purse_seines', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'squid_jigger', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'trawlers', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}], 'version': 1518594814620050.0, 'id': 'GFW/GFF/V1/fishing_hours/ARG-20120101', 'properties': {'system:time_start': 1325376000000, 'country': 'ARG', 'system:footprint': {'type': 'LinearRing', 'coordinates': [[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]]}, 'system:time_end': 1325462400000, 'system:asset_size': 731, 'system:index': 'ARG-20120101'}}\n\n\nThis code allows you to see only the band names and numbers in a more readable format.\n\n\nCode\n#8\ndef get_image_ids(gee_snipet):\n  x = ee.ImageCollection(gee_snipet)\n  first_image = x.first()\n  bands_list = first_image.bandNames()\n  lista = bands_list.getInfo()\n  for i, val in enumerate(lista):\n    print(i,val)\n    \n#Example\nprint(get_image_ids('GFW/GFF/V1/fishing_hours'))\n\n\n0 drifting_longlines\n1 fixed_gear\n2 other_fishing\n3 purse_seines\n4 squid_jigger\n5 trawlers\nNone\n\n\nTo read on the metadata we will first create an object image from the collection and index their properties. To built this first image we will create and use simple methods over the original data set of GEE.\n\n\nCode\n#9\n# Creating an image with some filters on time and space\nfishing_image = dataset \\\n    .filterBounds(aoi_2) \\\n    .first()\n\n# See image properties with their names and values\nfishing_props = geemap.image_props(fishing_image)\nfishing_props.getInfo()\n\n# Index by country\ncountry = fishing_props.get('country')\nprint('Country:', country.getInfo())\n\n#Represent the image properties with propertyNames()\nproperties = fishing_image.propertyNames()\nprint('Metadata properties:' , properties.getInfo())\n\n\nCountry: ARG\nMetadata properties: ['system:time_start', 'country', 'system:footprint', 'system:time_end', 'system:version', 'system:id', 'system:asset_size', 'system:index', 'system:bands', 'system:band_names']"
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#metadata-csv-description",
    "href": "posts/global-fishing-watch/index.html#metadata-csv-description",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Metadata CSV Description:",
    "text": "Metadata CSV Description:\nThe GFW website allows us to access individual data sets of AIS-based fishing effort and vessel presence. These data sets are provided in daily .csv file format and include the same data that is available through the GFW’s Google Earth Engine (GEE) API. By examining these files, we can gain insight into the GEE API metadata, such as the number of fishing hours per cell and day, the fishing state flag, and the type of gear used. This information can help us understand how the data is structured on the GEE fishing hours data set.\n\n\nCode\n#10\n#Reading the CSV file with the attributes of the metadata\nraw_fishing_vessels = pd.read_csv('2016-01-01.csv')\nraw_fishing_vessels.head()\n\n\n\n\n\n\n\n\n\ndate\ncell_ll_lat\ncell_ll_lon\nflag\ngeartype\nhours\nfishing_hours\nmmsi_present\n\n\n\n\n0\n2016-01-01\n-4.00\n-144.82\nUSA\nfishing\n0.2005\n0.0\n1\n\n\n1\n2016-01-01\n-4.25\n-144.92\nUSA\nfishing\n0.0750\n0.0\n1\n\n\n2\n2016-01-01\n5.25\n-138.54\nUSA\nfishing\n1.4925\n0.0\n1\n\n\n3\n2016-01-01\n-5.75\n-145.45\nUSA\nfishing\n0.0500\n0.0\n1\n\n\n4\n2016-01-01\n-6.50\n-164.41\nUSA\nfishing\n0.0333\n0.0\n1\n\n\n\n\n\n\n\nFrom that file, we can group, count, and sort the data to see which countries and gear types are most highly represented for that day.\n\n\nCode\n#11\n# Check for even representation of vessels\nprint(raw_fishing_vessels['flag'].value_counts().sort_values(ascending=False).head())\nprint(raw_fishing_vessels['geartype'].value_counts().sort_values(ascending=False).head())\n\n\nCHN    39022\nTWN     6297\nRUS     6164\nJPN     6130\nUSA     5213\nName: flag, dtype: int64\ntrawlers              38067\ndrifting_longlines    21721\nfishing               13557\nset_longlines          6244\ntuna_purse_seines      3758\nName: geartype, dtype: int64"
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#peruvian-fisheries-use-case-example",
    "href": "posts/global-fishing-watch/index.html#peruvian-fisheries-use-case-example",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Peruvian Fisheries Use Case Example",
    "text": "Peruvian Fisheries Use Case Example\nPeru is the second largest producer of wild caught sea food in the world landing 5,658,917 tonnes in 2020 (FAO, 2022). The marine fishing sector is responsible for 232,000 jobs and $3.2 billion in GDP as of 2005 (Christensen et al., 2014). Peruvian fishers were some of the first people to recognize El Niño events through their impact on fisheries (El Niño | National Geographic Society). El Niño Southern Oscillation causes warmer ocean temperatures and a reduction in upwelling, reducing the number of anchoveta and fisheries production.\nOur case study focuses on using Global Fishing Watch (GFW) data on fishing hours to explore fishing activities inside and outside of Peru’s economic exclusive zone (EEZ). The Peruvian fisheries serve as a valuable example of how international law, including the 1982 United Nations Convention on the Law of the Sea, enables developing countries to retain control over their marine resources and maintain food sovereignty. This is especially critical as foreign fleets from across the globe often take advantage of the rich fishing resources and high productivity of Peruvian waters, which extend beyond the boundaries of the EEZ. By studying the data on fishing hours, we can gain a better understanding of the overall health and sustainability of the Peruvian fisheries. This information can provide valuable insights into the importance of the EEZ for Peru and how it helps to protect the country’s marine resources from being exploited by foreign fleets.\nIn addition to providing insights into the importance of the EEZ, our analysis will also investigate whether trends in fishing efforts can be used as a proxy to evaluate the impacts of El Niño events on the Peruvian fisheries. Although this constitutes a secondary aspect of our study, we will comment on the fishing fluctuations and whether they may be influenced by the El Niño Southern Oscillation (ENSO). El Niño events can be identified using the Ocean Niño Index (ONI), which is a 3-month running mean of ERSST.v5 sea surface temperature anomalies in the Niño 3.4 region (between 5 degrees North and South of the equator and between 120 and 170 degrees West). For this study, we will focus on the 2015 El Niño event and compare it to the data from the previous year. More information is available here\n\nVisualizing Peruvian fisheries\nThis Visualization map allows us to understand the significance of Exclusive Economic Zones, particularly in the case of Peru where the fishing industry contributes significantly to the country’s GDP. By using this map, we can compare the total fishing hours within and outside of Peru’s EEZ without considering the flag or gear type. This information is crucial for understanding the impact Peru’s EEZ for managing and regulating this sector.\n\n\nCode\n# File path to save the images to your google drive\n#output_dir = '/Users/javipatron/Documents/MEDS/Personal/personal_webpage/images-gee'\n\n# Function to save each image in the collection\n#def save_image(image):\n    # Get the image ID\n    #image_id = image.get('system:index').getInfo()\n\n    # Define the output path\n    #output_path = os.path.join(output_dir, image_id + '.tif')\n\n    # Export the image as a GeoTIFF\n    #task = ee.batch.Export.image.toDrive(image=image,\n                                         #description=image_id,\n                                         #folder='dummy_folder',  # Doesn't matter for local export\n                                         #fileNamePrefix=image_id,\n                                         #region=aoi_1,\n                                         #scale=1000)\n    #task.start()\n\n# Iterate over the image collection and save each image\n#collection_list = fishing_effort_ImgCollection.toList(fishing_effort_ImgCollection.size())\n#for i in range(collection_list.length().getInfo()):\n    #image = ee.Image(collection_list.get(i))\n    #save_image(image)\n\n\n\n\nCode\n#12\n# Global fishing effort:\n## Aggregate 2016 collection to single image of global fishing effort\neffort_all = fishing_effort_ImgCollection.sum()\n## Sum bands to get total effort across gear types\neffort_all = effort_all.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_all = effort_all.mask(effort_all.gt(0))\n\n# Fishing effort in Peru's EEZ\neffort_EEZ = fishing_effort_ImgCollection.filterBounds(aoi_1).map(lambda image: image.clip(aoi_1))\n## Aggregate 2016 collection to single image of global fishing effort\neffort_EZZ = effort_EEZ.sum()\n## Sum bands to get total effort across gear types\neffort_EZZ = effort_EZZ.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_EZZ = effort_EZZ.mask(effort_EZZ.gt(0))\n\n# Visualization parameters\ntrawlersVis = {\n  'palette': ['0C276C', '3B9088', 'EEFF00', 'ffffff']\n}\n\n# Comparing the two maps\nleft_layer = geemap.ee_tile_layer(effort_EZZ, trawlersVis, name = \"All 2016\", opacity = 0.9)\nright_layer = geemap.ee_tile_layer(effort_all, trawlersVis, name = \"Effort EZZ\", opacity = 0.9)\nmap_comparison = geemap.Map(center = [-15, -83],zoom = 5)\nmap_comparison.centerObject(aoi_1, 5)\nmap_comparison.setOptions(\"HYBRID\")\nmap_comparison.addLayer(aoi_1, {\"color\": \"white\", \"width\": 1.5}, \"EEZ of Perú\");\nmap_comparison.split_map(left_layer, right_layer)\nmap_comparison\n\n\n\n\n\n\n\n\nGenerating a Gif of Monthly Fishing Effort off of Peru’s Coast.\nAnother aspect worth exploring is the evolution of the fishing effort across the years. The following code allows us to create an animated image of fishing activity in front of Peru. To do this, we will first aggregate the daily data into monthly fishing hour estimates. Then, we will sum up all band values to obtain information about total fishing. With the resulting images, we will create a .gif to visualize the temporal and spatial evolution of the fisheries.\n\n\nCode\n#13\n# monthly sum of fishing effort\ndef monthly_Sum (collection, years, months):\n  effort_sum = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('system:time_start',{'month': month, 'year': year})\n      Monthly_sum = Monthly_sum.mask(Monthly_sum.gt(0))\n      Monthly_sum = SRTM.blend(Monthly_sum)\n      effort_sum.append (Monthly_sum)\n  return ee.ImageCollection.fromImages(effort_sum)\n\n# list of images\ndef monthly_images (collection, years, months):\n  effort_images = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('system:time_start',{'month': month, 'year': year})\n      effort_images.append (Monthly_sum)\n  return effort_images\n\n\n\n\nCode\n#14\n# create list years and months for use in the function\nyears = range(2012, 2017)\nmonths = range(1, 13)\n\n\n\n\nCode\n#15\n# Create an image collection of fishing effort where each image is a sum of the fishing effort for that month. \neffort_collection = monthly_Sum(fishing_effort_ImgCollection, years, months)\n# Creates a list of the images from the image collection. \neffort_list = monthly_images(fishing_effort_ImgCollection, years, months)\nlen(effort_list)\n\n# Creates an empty list to be populated with month and year of images. \ndate_list = []\n\n# Populates the date_list with the month and year of each image. This will be used to annotate the gif with a time stamp. \nfor i in effort_list:\n  date = i.get('system:time_start').getInfo()\n  date_list.append(date)\n    \nlen(date_list)\n\n\n60\n\n\n\n\nCode\n#16\nsaved_gif = os.path.join(os.path.expanduser('~'), \"\".join([os.getcwd(),\"/img/monthly_fishing.gif\"]))\ngeemap.download_ee_video(effort_collection, videoArgs, saved_gif)\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/bdfb4cbb8e73c164946fbcc9f4f8bab8-75030eaf746e2164a0f2b0c9e1b6817a:getPixels\nPlease wait ...\nThe GIF image has been saved to: /Users/javipatron/Documents/MEDS/Personal/personal_webpage/javipatron.github.io/posts/global-fishing-watch/img/monthly_fishing.gif\n\n\n\n\nCode\n#17\nout_gif = os.path.join(os.path.expanduser('~'), \"\".join([os.getcwd(),\"/img/monthly_fishing.gif\"]))\n\n\n\n\nCode\n#18\ngeemap.add_text_to_gif(\n    saved_gif,\n    out_gif,\n    xy=('3%', '5%'),\n    text_sequence=date_list,\n    font_size=20,\n    font_color='#ffffff',\n    duration=600\n)\n\n\n\n\nCode\n#19\ngeemap.show_image(out_gif)\n\n\n\n\n\nIn the previous .gif, we can observe a seasonal pattern where fishing grounds migrate from the north to the south along the coast and the EEZ borders, with months experiencing minimal activity. However, one limitation of the image the difficulty to discern trend changes across years, making it challenging to determine if ENSO events may be diminishing fishing activity.\n\n\nQuantifying fishing hours inside and outside the EEZ\nIn order to further analyze the observed fishing differences between the Exclusive Economic Zone (EEZ) and the region immediately next to it, we will extract temporal series of total fishing hours inside and outside the EEZ. This will provide us with a quantifiable measure to compare the fishing efforts in these two areas. Additionally, the temporal series may enable us to identify any patterns or trends in fishing effort that may be influenced by the 2015 El Niño event.\nSince the GEE dataset exceeds the allowed extraction size, we have created a function that calls for the data of interest in order to work with it in pandas. The following function allow us to agregate all daily fishing hours values per month. With it we get first a dataset with monthly data instead of daily. Once we have temporarilly aggregated the data, we apply a reducer to sum up all fishing gears in one band and get a unique value representing the total fishing hours.\n\n\nCode\n#20\n# Loop ranges already defined for the gif. \n# Function to extract data of interest:\n## .sum() Aggregates collections to single monthly image of global fishing effort\n## .reduce(ee.Reducer.sum()) Sum bands to get total effort across all gear types\n\ndef monthly_Sum (collection, years, months):\n  effort_sum = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('year', year).set('system:time_start', ee.Date.fromYMD(year, 1, 1))\\\n                              .set('month', month).set('system:time_start', ee.Date.fromYMD(year, month, 1).millis())                                                     \n      effort_sum.append (Monthly_sum)\n  return ee.ImageCollection.fromImages(effort_sum)\n\n\n\n\nCode\n#21\nbyMonth = monthly_Sum(fishing_effort_ImgCollection,years,months)\ntype(byMonth)\n\n\nee.imagecollection.ImageCollection\n\n\nOur updated image collection now includes monthly fishing data for all gears combined. In order to effectively analyze the time series of this data, we will need to design a new function that allows us to summarize the information within our designated areas of interest (AOIs). This function will take the AOI as a parameter, enabling us to conduct time series analysis on each AOI individually. The upcoming sections will focus on conducting time series analysis for each AOI.\n\nFishing inside the EEZ\nThe following time series correspond to the fishing data contained inside the EEZ. previously saved as aoi_1.\n\n\nCode\n#22\n# Function to summarize fishing monthly data:\n## Extracting all image values in the collection by the AOI relative to the EEZ\ndef aoi_sum(img):\n    sum = img.reduceRegion(reducer=ee.Reducer.sum(), geometry=aoi_1, # EEZ area of interest\n                           scale=1113.2) # Resolution of the GFW product\n    return img.set('time', img.date().millis()).set('sum',sum)\n\naoi_reduced_imgs = byMonth.map(aoi_sum)\nnested_list = aoi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['time','sum']).values().get(0)\n\n\n\n\nCode\n#23\n# Converting GEE data to Pandas and rearranging it for its representation\ndf = pd.DataFrame(nested_list.getInfo(), columns=['time','sum'])\ndf['sum'] = df['sum'].apply(lambda x: x.get('sum'))\ndf.rename(columns = {'sum':'total_fishing_hours'}, inplace = True)\ndf[\"id\"] = df.index \nfirst_column = df.pop('id')\ndf.insert(0, 'id', first_column)\n\n# Setting time format for representation purposes\ndf['datetime'] = pd.to_datetime(df['time'], unit='ms')\ndf['datetime'] = pd.to_datetime(df['datetime'],format=\"%Y%m%d\")\n#df.head()\n\n\n\n\nCode\n#24\nplt.figure(figsize=(10, 6), dpi=300)   # create a new figure, set size and resolution (dpi)\nplt.fill_between(df['datetime'],df['total_fishing_hours'])   # add data to the plot\nplt.title(\"Fishing hours inside Peru's EEZ\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\n\n\nText(0, 0.5, 'Total hours')\n\n\n\n\n\n\n\nFishing outside the EEZ\nTo characterize the fishing activity outside of the EEZ, we need to define a new area of interest. In this case, we will focus on a square region with the same latitude boundaries as the EEZ but extending towards the west in longitude (aoi_2). The first step is to quantify the number of fishing hours over that region. Then we will subtract the EEZ fishing hours previously calculated from it to graphically compare effort from inside and outside.\nThe map below shows the new area of interest and the data across a the 5-year time period that will be included in the time series analysis.\n\n\nCode\n#25\n# Fishing effort in Peru's EEZ\neffort_AOI2 = fishing_effort_ImgCollection.filterBounds(aoi_2).map(lambda image: image.clip(aoi_2))\n## Aggregate 2016 collection to single image of global fishing effort\neffort_AOI2 = effort_AOI2.sum()\n## Sum bands to get total effort across gear types\neffort_AOI2 = effort_AOI2.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_AOI2 = effort_AOI2.mask(effort_AOI2.gt(0))\n\n# Add the total fishing effort layer\nMap = geemap.Map(zoom = 2)\nMap.centerObject(aoi_2, 5)\nMap.setOptions('HYBRID');\nMap.addLayer(aoi_2, {'color': 'white','width': 1.5}, \"Sampling area\");\nMap.addLayer(effort_AOI2,trawlersVis);\nMap\n\n\n\n\n\n\n\n\nCode\n#26\n# Function to summarize fishing monthly data inside and outside the EEZ\ndef aoi_sum(img):\n    sum = img.reduceRegion(reducer=ee.Reducer.sum(), geometry=aoi_2, \n                           scale=1113.2)\n    return img.set('time', img.date().millis()).set('sum',sum)\n\naoi_reduced_imgs = byMonth.map(aoi_sum)\nnested_list = aoi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['time','sum']).values().get(0)\n\n# Converting GEE data to Pandas and rearranging it for its representation\ndf2 = pd.DataFrame(nested_list.getInfo(), columns=['time','sum'])\ndf2['sum'] = df2['sum'].apply(lambda x: x.get('sum'))\ndf2.rename(columns = {'sum':'total_fishing_hours'}, inplace = True)\ndf2[\"id\"] = df2.index \nfirst_column = df2.pop('id')\ndf2.insert(0, 'id', first_column)\ndf2['datetime'] = pd.to_datetime(df2['time'], unit='ms')\ndf2['datetime'] = pd.to_datetime(df2['datetime'],format=\"%Y%m%d\")\n\n# Ploting time series\nplt.figure(figsize=(10, 6), dpi=300)   \nplt.fill_between(df2['datetime'],df2['total_fishing_hours'])\nplt.title(\"Fishing hours inside the second AOI\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\n\n\nText(0, 0.5, 'Total hours')\n\n\n\n\n\nTo compare fishing efforts within and outside, we can subtract the total fishing hours from the previous data frames. This will give us the actual fishing effort outside the EEZ, which we can then represent alongside fishing within the EEZ in an area chart for easy visualization and comparison.\n\n\nCode\n#27\n# Combining both df\ndf[\"total_fishing_hours_outside\"] = abs(df[\"total_fishing_hours\"] - df2[\"total_fishing_hours\"])\ndf.rename(columns = {'total_fishing_hours':'total_fishing_hours_inside'}, inplace = True) \ntotal_fishing_df = df[['id', 'time', 'datetime','total_fishing_hours_inside','total_fishing_hours_outside']]\ntotal_fishing_df.head()\n\n\n\n\n\n\n\n\n\nid\ntime\ndatetime\ntotal_fishing_hours_inside\ntotal_fishing_hours_outside\n\n\n\n\n0\n0\n1325376000000\n2012-01-01\n0.000000\n160.185273\n\n\n1\n1\n1328054400000\n2012-02-01\n29.597059\n442.994487\n\n\n2\n2\n1330560000000\n2012-03-01\n0.683282\n512.164755\n\n\n3\n3\n1333238400000\n2012-04-01\n7.479013\n2290.269927\n\n\n4\n4\n1335830400000\n2012-05-01\n11.036600\n3353.737642\n\n\n\n\n\n\n\n\n\nCode\n#28\nplt.figure(figsize=(10, 6), dpi=300)   \nplt.stackplot(total_fishing_df['datetime'],\n              [total_fishing_df['total_fishing_hours_inside'], total_fishing_df['total_fishing_hours_outside']],\n              labels=['Inside the EEZ', 'Outside the EEZ'],\n              alpha=0.8)\nplt.title(\"Fishing hours in front of the coast of Perú\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\nplt.legend(loc=2, fontsize='large')\nplt.show()\n\n\n\n\n\nThe orange region of the graph indicates the amount of fishing occurring in international waters, while the blue area represents the fishing that takes place within Peru’s exclusive economic zone (EEZ). It is likely that this fishing is carried out by Peru’s own fleet or by other countries with which Peru has bilateral fishing agreements. From the data, it is clear that the majority of fishing effort occurs outside of Peru’s EEZ, with some instances exceeding 30,000 hours. This highlights the importance of the EEZ, as it protects Peru’s resources from being over exploited by foreign fleets. Without the EEZ, foreign fleets would be able to legally access and deplete Peru’s waters.\nOn the other hand, there is a distinct pattern of fishing activity that tends to peak during the summer and fall months, and decrease during the winter. This trend is consistent both inside and outside the Exclusive Economic Zone (EEZ). Additionally, it appears that there has been an increase in fishing hours when comparing the first two years of data with the last three years. However, this could be attributed to better availability of AIS data over the years rather than any actual changes in fishing patterns. Furthermore, there is no clear evidence of an influence of ENSO events on fishing activity. In fact, values for 2014 and 2015 are relatively similar, indicating that fishing hours may not be the most effective way to measure the impact of atmospheric and oceanic oscillations on fisheries.\n\n\n\nImproving our analysis\nOverall, the notebook has successfully introduced the GFW datasets and its access through GEE. However, continuing the analysis could provide further insights into the study case.\nWe could improve our analysis in several ways. For instance, we could investigate the contribution of different fishing flags to the total fishing effort both inside and outside the EEZ. This would allow us to identify which international fleets are fishing in Peru’s waters and evaluate the countries that have fishing agreements with Peru. We could also compare total navigation hours (a data set also available from GEE) with fishing hours to establish how busy the waters are within our areas of interest.\nFurthermore, we could delve deeper into exploring the effects of ENSO. Initially, we could have included a time series of the ENSO indicator from the 3-month mean of ERSST.v5 sea surface temperature anomalies in the Niño 3.4 region, which we did not incorporate as we did not detect any influence. In this regard, we could have explored additional GFW data sets (accessible through Google BigQuery) representing fishing activities using indicators influenced by El Niño."
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#binder-environment",
    "href": "posts/global-fishing-watch/index.html#binder-environment",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Binder Environment",
    "text": "Binder Environment\nThe content of the present notebook can be accessed using Binder’s services for building and sharing reproducible and interactive computational environments from online repositories.\nClick this link to launch the notebook in your browser:\n\n\n\nBinder"
  },
  {
    "objectID": "posts/global-fishing-watch/index.html#references",
    "href": "posts/global-fishing-watch/index.html#references",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "References",
    "text": "References\n\nChristensen, V., de la Puente, S., Sueiro, J. C., Steenbeek, J., & Majluf, P. (2014). Valuing seafood: The Peruvian fisheries sector. Marine Policy, 44, 302–311. https://doi.org/10.1016/j.marpol.2013.09.022\nEl Niño | National Geographic Society. (n.d.). Retrieved November 29, 2022, from https://education.nationalgeographic.org/resource/el-nino\nFAO. Fishery and Aquaculture Statistics. Global capture production 1950-2020 (FishStatJ). 2022. In: FAO Fisheries and Aquaculture Division [online]. Rome. Updated 2022.\nGFW (global fishing watch) daily fishing hours | Earth Engine Data catalog | google developers (no date) Google. Google. Available at: https://developers.google.com/earth-engine/datasets/catalog/GFW_GFF_V1_fishing_hours (Accessed: November 30, 2022).\nGlobal Fishing Watch Application Programming Interfaces (API) Documentation (https://globalfishingwatch.org/our-apis/documentation#introduction)\nKroodsma, David A., Juan Mayorga, Timothy Hochberg, Nathan A. Miller, Kristina Boerder, Francesco Ferretti, Alex Wilson et al. “Tracking the global footprint of fisheries.” Science 359, no. 6378 (2018): 904-908. DOI:10.1126/science.aao5646."
  },
  {
    "objectID": "posts/texas-outage/index.html",
    "href": "posts/texas-outage/index.html",
    "title": "Texas Outage",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nThe tasks in this post are: - Estimating the number of homes in Houston that lost power as a result of the first two storms\n- Investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau.\n\n\n\nLoad vector/raster data\n\nSimple raster operations\n\nSimple vector operations\n\nSpatial joins"
  },
  {
    "objectID": "posts/texas-outage/index.html#overview",
    "href": "posts/texas-outage/index.html#overview",
    "title": "Texas Outage",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nThe tasks in this post are: - Estimating the number of homes in Houston that lost power as a result of the first two storms\n- Investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau.\n\n\n\nLoad vector/raster data\n\nSimple raster operations\n\nSimple vector operations\n\nSpatial joins"
  },
  {
    "objectID": "posts/texas-outage/index.html#data",
    "href": "posts/texas-outage/index.html#data",
    "title": "Texas Outage",
    "section": "Data",
    "text": "Data\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nRoads\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shape file of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouses\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "posts/texas-outage/index.html#assignment",
    "href": "posts/texas-outage/index.html#assignment",
    "title": "Texas Outage",
    "section": "Assignment",
    "text": "Assignment\n\nFind locations of blackouts\nFor improved computational efficiency and easier interoperability with sf, we will use the stars package for raster handling.\n\n\n\nCode\n# Setting my filepaths\nrootdir &lt;- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndatatif &lt;- file.path(rootdir,\"data\",\"VNP46A1\")\ndata &lt;- file.path(rootdir,\"data\")\n\n#Creating the names for each file\nnightlight1 &lt;- 'VNP46A1.A2021038.h08v05.001.2021039064328.tif' \nnightlight2 &lt;- 'VNP46A1.A2021038.h08v06.001.2021039064329.tif' \nnightlight3 &lt;- 'VNP46A1.A2021047.h08v05.001.2021048091106.tif'\nnightlight4 &lt;- 'VNP46A1.A2021047.h08v06.001.2021048091105.tif'\n  \n#Downloading the raster data to a star object\none &lt;- read_stars(file.path(datatif, nightlight1))\ntwo &lt;- read_stars(file.path(datatif, nightlight2))\nthree &lt;- read_stars(file.path(datatif,nightlight3))\nfour &lt;- read_stars(file.path(datatif,nightlight4))\n\n\n\n\nCode\n#Combine tiles to have the full size\nlights_off &lt;- st_mosaic(one,two)\nlights_on &lt;- st_mosaic(three,four)\n\nplot(lights_off, main= \"Satellite Image of Houston Feb 7th\")\n\n\ndownsample set to 6\n\n\n\n\n\nCode\nplot(lights_on, main= \"Satellite Image of Houston Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nCreate a blackout mask\n\nFind the change in night lights intensity (presumably) caused by the storm\nResponse: The change on the mean from the second date (All lights) goes from 13.86 down 12.13939, on the first date (Outrage)\nReclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nAssign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\n\n\n\nCode\n#Create a new raster that has the difference of the values from the Feb 7th (Lights Off) raster, and the 16th (Lights On) raster. This will just have the difference of the attribute value on each pixel\n\nraster_diff &lt;-  lights_off - lights_on\n\nplot(raster_diff, main= \"Difference in light intensity from Feb 7th and Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nVectorize the mask\n\nUsing st_as_sf() to vectorize the blackout mask\nFixing any invalid geometries by using st_make_valid\n\n\n\nCode\n# Converts the non-spatial star object (.tif) file to an sf object. An sf object will have an organizing structure that will have the geom column and the layer as attribues\nblackout &lt;- st_as_sf(raster_diff)\nsummary(blackout)\n\n\n VNP46A1.A2021038.h08v05.001.2021039064328.tif          geometry    \n Min.   :  200.0                               POLYGON      :24950  \n 1st Qu.:  262.0                               epsg:4326    :    0  \n Median :  359.0                               +proj=long...:    0  \n Mean   :  543.8                                                    \n 3rd Qu.:  563.0                                                    \n Max.   :65512.0                                                    \n\n\n\n\nCrop the vectorized map to our region of interest.\n\nDefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nTurn these coordinates into a polygon using st_polygon\nConvert the polygon into a simple feature collection using st_sfc() and assign a CRS\n\nhint: because we are using this polygon to crop the night lights data it needs the same CRS\n\nCrop (spatially subset) the blackout mask to our region of interest \nRe-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nCode\n#Create vectors for the desired CRS\nlon = c(-96.5, -96.5, -94.5, -94.5,-96.5)\nlat = c(29, 30.5, 30.5, 29, 29)\n\n\n#Create an array or matrix with those vectors \ncoordinates_array &lt;-cbind(lon,lat)\n\n#Creating a polygon with the st_polygon function but the coordinates_array has to be in the form of a list so the st_polygon can read it.\nhouston_polygon &lt;- st_polygon(list(coordinates_array))\n\n# Create a simple feature geometry list column, and add coordinate reference system so you can \"speak\" the same language than your recent blackout object\nhouston_geom &lt;- st_sfc(st_polygon(list(coordinates_array)), crs = 4326)\n\n# Indexing or Cropping the blackout sf object with just the houston geometery polygon. \n\nhouston_blackout_subset &lt;- blackout[houston_geom,]\n\n# Re-project the cropped blackout dataset with a new CRS (EPSG:3083) (NAD83 / Texas Centric Albers Equal Area)\nhouston_projection &lt;- st_transform(houston_blackout_subset,\"EPSG:3083\")\n\n\n\n\nCode\nplot(houston_blackout_subset, main = \"Blackout pixels in Houston\")\n\n\n\n\n\n\n\nExclude highways from blackout mask\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\n\nDefine SQL query\nLoad just highway data from geopackage using st_read\nReproject data to EPSG:3083\nIdentify areas within 200m of all highways using st_buffer\nFind areas that experienced blackouts that are further than 200m from a highway\n\n\n\nCode\n#Reading the data with format .gpkg\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(file.path(data, \"gis_osm_roads_free_1.gpkg\"), query = query)\n\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway''\nfrom data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n\nCode\n#Transform that new sf object with the CRS we have been using, so we stay in the same space.\nhighways_3083 &lt;- st_transform(highways, \"EPSG:3083\")\n\n\n\n\nCode\n#Create a buffer that contains all the roads, including the 200 meters across the roads.\n# BUT, to make a buffer object we need to first create a union of all those rows (highways), so we can use the st_buffer function...\nhighways_union &lt;- st_union(highways_3083)\nhighways_buffer &lt;- st_buffer(x = highways_union,\n          dist = 200)\n\nplot(highways_buffer, main = \"Highways in Houston\")\n\n\n\n\n\nCode\n# Use the buffer to subtract those entire pixels or \"rows\" from the houston projection sf object.\nhouston_out &lt;- houston_projection[highways_buffer, op = st_disjoint]\nplot(houston_out,\n     main= \"Houston Without the Roadlights\")\n\n\n\n\n\n\n\nFind homes impacted by blackouts\n\nLoad buildings dataset using st_read and the following SQL query to select only residential buildings\nhint: reproject data to EPSG:3083\n\n\n\nCode\n#Read the data\nquery2 &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings &lt;- st_read(file.path(data, \"gis_osm_buildings_a_free_1.gpkg\"), query = query2)\n\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')'\nfrom data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\n\n\nFind homes in blackout areas\n\nFilter to homes within blackout areas.\nTip: You can use [ ] option to subtract those geometries from the building sf object that do not correspond to the Houston area. Or you can use the st_filter.\nCount number of impacted homes.\nTip: The total number of impacted houses when using st_filter or [ ] is 139,148. This could vary depending on how the st_filter considers the limits and borders of the geometries. The default method when using st_filter is st_intersects.\n\n\n\nCode\n#Count number of houses\ndim(homes_blackout_join)[1]\n\n\n[1] 139148\n\n\n\n\nInvestigate socioeconomic factors\nLoad ACS data\n\nUse st_read() to load the geodatabase layers\nGeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\nIncome data is stored in the X19_INCOME layer\nSelect the median income field B19013e1\nhint: reproject data to EPSG:3083\n\n\n\n\nCode\n#Read the data and understand the data layers\nst_layers(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n\nDriver: OpenFileGDB \nAvailable layers:\n                            layer_name geometry_type features fields crs_name\n1                      X01_AGE_AND_SEX            NA     5265    719     &lt;NA&gt;\n2                             X02_RACE            NA     5265    433     &lt;NA&gt;\n3        X03_HISPANIC_OR_LATINO_ORIGIN            NA     5265    111     &lt;NA&gt;\n4                         X04_ANCESTRY            NA     5265    665     &lt;NA&gt;\n5         X05_FOREIGN_BORN_CITIZENSHIP            NA     5265   1765     &lt;NA&gt;\n6                   X06_PLACE_OF_BIRTH            NA     5265   1221     &lt;NA&gt;\n7                        X07_MIGRATION            NA     5265   1793     &lt;NA&gt;\n8                        X08_COMMUTING            NA     5265   2541     &lt;NA&gt;\n9  X09_CHILDREN_HOUSEHOLD_RELATIONSHIP            NA     5265    263     &lt;NA&gt;\n10      X10_GRANDPARENTS_GRANDCHILDREN            NA     5265    373     &lt;NA&gt;\n11    X11_HOUSEHOLD_FAMILY_SUBFAMILIES            NA     5265    781     &lt;NA&gt;\n12      X12_MARITAL_STATUS_AND_HISTORY            NA     5265    759     &lt;NA&gt;\n13                       X13_FERTILITY            NA     5265    399     &lt;NA&gt;\n14               X14_SCHOOL_ENROLLMENT            NA     5265    779     &lt;NA&gt;\n15          X15_EDUCATIONAL_ATTAINMENT            NA     5265    715     &lt;NA&gt;\n16         X16_LANGUAGE_SPOKEN_AT_HOME            NA     5265    871     &lt;NA&gt;\n17                         X17_POVERTY            NA     5265   3941     &lt;NA&gt;\n18                      X18_DISABILITY            NA     5265    893     &lt;NA&gt;\n19                          X19_INCOME            NA     5265   3045     &lt;NA&gt;\n20                        X20_EARNINGS            NA     5265   2185     &lt;NA&gt;\n21                  X21_VETERAN_STATUS            NA     5265    565     &lt;NA&gt;\n22                     X22_FOOD_STAMPS            NA     5265    243     &lt;NA&gt;\n23               X23_EMPLOYMENT_STATUS            NA     5265   1625     &lt;NA&gt;\n24         X25_HOUSING_CHARACTERISTICS            NA     5265   4415     &lt;NA&gt;\n25                X27_HEALTH_INSURANCE            NA     5265   1593     &lt;NA&gt;\n26       X28_COMPUTER_AND_INTERNET_USE            NA     5265    385     &lt;NA&gt;\n27           X29_VOTING_AGE_POPULATION            NA     5265     35     &lt;NA&gt;\n28                      X99_IMPUTATION            NA     5265    783     &lt;NA&gt;\n29             X24_INDUSTRY_OCCUPATION            NA     5265   2107     &lt;NA&gt;\n30                  X26_GROUP_QUARTERS            NA     5265      3     &lt;NA&gt;\n31                 TRACT_METADATA_2019            NA    35976      2     &lt;NA&gt;\n32         ACS_2019_5YR_TRACT_48_TEXAS Multi Polygon     5265     15    NAD83\n\n\n\n\nDetermine which census tracts experienced blackouts.\n\nJoin the income data to the census tract geometries\nhint: make sure to join by geometry ID\nSpatially join census tract data with buildings determined to be impacted by blackouts\nFind which census tracts had blackouts\n\n\n\nCode\n#Create a big sf that has the income information and its geometry\nincome_geom &lt;- left_join(texas_geom, texas_income, by = \"GEOID_Data\")\n\n#Create an new sf that adds the income information to just the blackout sf object that we previously had. In this step I learned that if you use st_join will give you a different number and result than st_filter or [ ]\n\n#blackout_income_join &lt;- st_join(homes_blackout_join, income_geom)\nblackout_join &lt;- income_geom[homes_blackout_join,]\n\n#Print those Census Tracts a had blackouts. Im printing the number and the name\nlength(unique(blackout_join$NAMELSAD))\n\n\n[1] 711\n\n\nCode\n#unique(blackout_join$NAMELSAD)\n\n\n\n\nCompare incomes of impacted tracts to unimpacted tracts.\n\nCreate a map of median income by census tract, designating which tracts had blackouts\nPlot the distribution of income in impacted and unimpacted tracts\nWrite approx. 100 words summarizing your results and discussing any limitations to this study\n\n\n\nCode\n#Creating a list of tracts and counties that were affected by the blackout\nblackout_tracts &lt;- unique(blackout_join$TRACTCE)\nblackout_counties &lt;- unique(blackout_join$COUNTYFP)\n\n#Creating a Data Frames that includes only the rows that have the county affected by using the geom from blackout_tracts. One with the Counties and the other one with the Tracts.\ntracts_affected &lt;- income_geom |&gt; \n  filter(TRACTCE %in% blackout_tracts)\n\ncounties_affected &lt;- income_geom |&gt; \n  filter(COUNTYFP %in% blackout_counties)\n\n#Create a map were the base is the counties of Houston, then fill the color with the income_median column we created with \"B19013e1\", and then highlight the counties that were had building affected from our dataset\nmap &lt;- tm_shape(counties_affected) +\n  tm_fill(col = \"income_median\", palette = \"BrBG\") +\n  tm_borders() +\n  tm_shape(blackout_join) +\n  tm_fill(col = \"pink\", alpha= 0.3) +\n  tm_layout(legend.outside = T,\n            main.title = \"Median Income by Census Tract\",\n            frame = T,\n            title = \"*Affected areas in pink*\") +\n  tm_compass(type = \"arrow\", \n             position = c(\"left\", \"top\")) +\n  tm_scale_bar()\n\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nCode\nmap\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\n\n\n\nCode\n#Finding the difference in income between the affected tracts and unaffected.\n\n#First we need create two sf data frames to categorize if they were impacted or not by the blackout\n\nnot_impacted &lt;- anti_join(tracts_affected , as.data.frame(blackout_join)) |&gt; \n  mutate(impacted = \"no\")\n\n\nJoining with `by = join_by(STATEFP, COUNTYFP, TRACTCE, GEOID, NAME, NAMELSAD,\nMTFCC, FUNCSTAT, ALAND, AWATER, INTPTLAT, INTPTLON, Shape_Length, Shape_Area,\nGEOID_Data, income_median, Shape)`\n\n\nCode\nimpacted &lt;- blackout_join |&gt; \n  mutate(impacted = \"yes\")\n\n#Second, we need to create a new data frame were the \"key\" is the same column name, so we will have which tracts were impacted and which ones were not.\ncombination &lt;- rbind(not_impacted, impacted) |&gt; \n  dplyr::select(income_median, impacted)\n\nsummary(combination)\n\n\n income_median      impacted                   Shape    \n Min.   : 13886   Length:741         MULTIPOLYGON :741  \n 1st Qu.: 43749   Class :character   epsg:3083    :  0  \n Median : 60414   Mode  :character   +proj=aea ...:  0  \n Mean   : 71330                                         \n 3rd Qu.: 89796                                         \n Max.   :250001                                         \n NA's   :25                                             \n\n\nCode\n#Third, create a histogram and a box plot of that new column and analyse the income median \n\nggplot(combination, aes(x = income_median, fill= impacted)) +\n  geom_histogram()  +\n    labs(title = \"Distribution of Income\",\n       x = \"Income Median\",\n       y = \"Count\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 25 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nCode\nggplot(combination, aes(y = income_median, fill = impacted)) +\n  geom_boxplot() +\n  labs(title = \"Blackout Impact by Income\",\n       x = \"Impacted\",\n       y = \"Income Median\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nCode\n# Statistical comparison\nsummary(impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  13886   43749   60642   71462   90013  250001       3 \n\n\nCode\nsummary(not_impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25755   42793   53442   59586   63670  134500      22 \n\n\nCode\nt.test(impacted$income_median, not_impacted$income_median)\n\n\n\n    Welch Two Sample t-test\n\ndata:  impacted$income_median and not_impacted$income_median\nt = 0.9696, df = 7.2121, p-value = 0.3636\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -16915.65  40668.75\nsample estimates:\nmean of x mean of y \n 71462.30  59585.75"
  },
  {
    "objectID": "posts/texas-outage/index.html#summary",
    "href": "posts/texas-outage/index.html#summary",
    "title": "Texas Outage",
    "section": "Summary",
    "text": "Summary\nThe results show a high number people affected by this winter storm. According to the data, almost 140,000 houses/ building were impacted within the designated area of Houston showing an almost equal impact between high income census tracts and low income census tracts. Looking in detail at the plots, the distribution plot (Histogram), is a right skew, showing a higher density around the low income median, nevertheless in the summary results you can see that the median income for the not impacted census tracts is lower that the impacted census tract, showing that the effect of the storm affected almost equally everyone in the zone, regardless of the income. It is important to consider that we are not weighting each tract by impacted houses. Which could affect final results, and conclusions."
  },
  {
    "objectID": "posts/texas-outage/index.html#footnotes",
    "href": "posts/texas-outage/index.html#footnotes",
    "title": "Texas Outage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "posts/stats-final/index.html",
    "href": "posts/stats-final/index.html",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "",
    "text": "Blue carbon is attracting massive attention from governments, investors, and companies needing carbon accounting practices. Mangrove forests can sequester almost ten times more than boreal, temperate, or tropical forests, thanks to the mangrove biological characteristics. That is one of the main reasons they received the nickname of “The Superheroes of Climate Change.”\nMoreover, carbon stocks are the total amount of organic carbon stored within a system and are comprised of carbon pools, which fall into different bins: living aboveground living (live plants, including epiphytes); dead aboveground biomass (e.g., fallen branches); living belowground biomass (e.g., roots); and belowground carbon (sediment organic matter).\nWith that in mind, it is critical to understand the main factors to calculate the total mangrove carbon stocks’ in restoration practices. Additionally, to analyze and calculate an appropriate amount of tree samples and plot sizes to estimate the amount of carbon storage in the area.\n\n\n\nThe more you know: Mangroves (2020)\n\n\n\n\nBlue carbon accounting is a new strategy with few practical examples and evidence worldwide. Some methodologies align with the Verra non-profit organization VCS (Verified Carbon Standard) that helps us describe and outline the correct procedures to quantify the net greenhouse gas emission reductions and removals resulting from project activities implemented to restore tidal wetlands. For this case, the provided data uses equations and formulas designed on the VCS0032 and VCS0033 methodologies, which are aligned with the VCS."
  },
  {
    "objectID": "posts/stats-final/index.html#introduction",
    "href": "posts/stats-final/index.html#introduction",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "",
    "text": "Blue carbon is attracting massive attention from governments, investors, and companies needing carbon accounting practices. Mangrove forests can sequester almost ten times more than boreal, temperate, or tropical forests, thanks to the mangrove biological characteristics. That is one of the main reasons they received the nickname of “The Superheroes of Climate Change.”\nMoreover, carbon stocks are the total amount of organic carbon stored within a system and are comprised of carbon pools, which fall into different bins: living aboveground living (live plants, including epiphytes); dead aboveground biomass (e.g., fallen branches); living belowground biomass (e.g., roots); and belowground carbon (sediment organic matter).\nWith that in mind, it is critical to understand the main factors to calculate the total mangrove carbon stocks’ in restoration practices. Additionally, to analyze and calculate an appropriate amount of tree samples and plot sizes to estimate the amount of carbon storage in the area.\n\n\n\nThe more you know: Mangroves (2020)\n\n\n\n\nBlue carbon accounting is a new strategy with few practical examples and evidence worldwide. Some methodologies align with the Verra non-profit organization VCS (Verified Carbon Standard) that helps us describe and outline the correct procedures to quantify the net greenhouse gas emission reductions and removals resulting from project activities implemented to restore tidal wetlands. For this case, the provided data uses equations and formulas designed on the VCS0032 and VCS0033 methodologies, which are aligned with the VCS."
  },
  {
    "objectID": "posts/stats-final/index.html#data",
    "href": "posts/stats-final/index.html#data",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Data",
    "text": "Data\nThe data we will analyze to answer our questions are actual field measurements from mangrove trees over four replantation sites (2010, 2016, 2017, 2020). This data was provided directly to me by Silvestrum Climate Associates ®. This data is confidential, and there is limited access to the site.\n\nLimitations, spatial and temporal features.\nThe dataset contains a record for each tree height and canopy. With those measurements, Silvestrum has calculated the total tree carbon in (kg) and the total tree carbon per hectare in (Mg/ha). In this analysis, we will assume that the correct protocols and equations were followed for this purpose, and this analysis is dependent from those results.\nRegarding the dataset, the above and below ground carbon storage was assessed using a nested sub-sampling protocol along the 25m2, 77m2, and 154m2 transect following the Kauffman and Donato procedure. The equations for above and below ground biomass are defined by Comley and McGuiness.\nFurthermore, seeking for data limitations this mathematical calculations made by Silvestrum is a significant one, as this observations do not have the actual carbon samples to be analyzed. Another limitation is the possible man errors from the field sampling and the natural growth and variability from the mangroves.\nTo get you familiarized, the table below shows the first 5 rows of the data we will be studying. It has a total of 10,826 rows and 11 columns.\n\n\n\nTable 1. Full Mangrove Data Set (head)\n\n\nplantation_year\nplot\nheight_cm\ncanopy_width_1_cm\ncanopy_width_2_cm\ncrown_area_m2\ncd_chatting_m\nchatting_agb_kg\nchatting_agb_mg_ha\ncomley_mc_guinness_planted_bgb_kg\ncomley_mc_guinness_planted_bgb_mg_ha\ntotal_tree_kg\ntotal_tree_mg_ha\nchatting_agc_kg_c\nchatting_agc_mg_c_ha\ncomley_mc_guinness_planted_bgc_kg_c\ncomley_mc_guinness_planted_bgc_mg_c_ha\ntotal_tree_kg_c\ntotal_tree_mg_c_ha\nplot_size_m2\n\n\n\n\n2017\n8\n112\n59\n57\n0.264\n0.58\n0.494\n0.032\n0.456\n0.030\n0.950\n0.062\n0.237\n0.015\n0.178\n0.012\n0.415\n0.027\n153.938\n\n\n2017\n2_new\n280\n90\n101\n0.716\n0.96\n1.436\n0.571\n1.326\n0.527\n2.762\n1.099\n0.689\n0.274\n0.517\n0.206\n1.206\n0.480\n25.133\n\n\n2017\n2_new\n275\n114\n119\n1.066\n1.17\n2.198\n0.874\n2.028\n0.807\n4.226\n1.681\n1.055\n0.420\n0.791\n0.315\n1.846\n0.734\n25.133\n\n\n2017\n2_new\n274\n95\n102\n0.762\n0.99\n1.534\n0.611\n1.416\n0.564\n2.951\n1.174\n0.737\n0.293\n0.552\n0.220\n1.289\n0.513\n25.133\n\n\n2017\n2_new\n264\n133\n104\n1.103\n1.19\n2.279\n0.907\n2.104\n0.837\n4.383\n1.744\n1.094\n0.435\n0.820\n0.326\n1.914\n0.762\n25.133\n\n\n2017\n2_new\n264\n133\n104\n1.103\n1.19\n2.279\n0.907\n2.104\n0.837\n4.383\n1.744\n1.094\n0.435\n0.820\n0.326\n1.914\n0.762\n25.133\n\n\n\n\n\n\n\n\n\n\nTable 2. Total mangrove Dataset summary\n\n\nplantation_year\nPlot Count\nSample Count\nTotal Carbon (mg/ha)\nTotal Carbon Variance (mg/ha)\n\n\n\n\n2010\n7\n746\n45.725\n0.1072349\n\n\n2017\n27\n4126\n242.993\n0.1302739\n\n\n2019\n8\n998\n22.994\n0.0185163\n\n\n2020\n46\n4956\n87.824\n0.0159968\n\n\n\n\n\n\n\nAs we can see in the table above, the 2017 plantation year has an exciting number of samples, different sample plot sizes, and high variation of total carbon in Mg per hectare. We will focus on this year to answer our questions.\nThe table #3 contains only information from the 2017 plantation year. We will use this filter to analyse the different questions, and interpret our results.\n\n\nCode\nmangrove_2017 &lt;- mangrove_df |&gt; \n  filter(plantation_year == 2017) |&gt; \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha)\nmangrove_2017 &lt;- mangrove_2017[!(mangrove_2017$total_tree_mg_c_ha== 1.224),]\n\n#Statistical Table with the selected variables for question #1\nmangrove_plots_2017 &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nmangrove_plots_2017 %&gt;% \n  kbl(align=rep('c', 5),\n      caption =\"Table 3. Plot summary of the 2017 plantation year\",\n      position = \"left\") %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 3. Plot summary of the 2017 plantation year\n\n\nplot_size_m2\ntot_samples\ntot_plot_count\nmean_tree_c_ha\nsd_tree_c_ha\n\n\n\n\n25m2\n323\n4\n0.3708173\n0.3035103\n\n\n77m2\n336\n5\n0.0624196\n0.0651300\n\n\n154m2\n3466\n18\n0.0291466\n0.0297558\n\n\n\n\n\n\n\nCode\n#Statistical table of each unique plot to analyse variability in the question #2\nplots_2017 &lt;- mangrove_2017 |&gt; \n  group_by(plot, plot_size_m2) |&gt; \n  summarise(sample_count = n(),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha))\n\n\nNow let us take a closer look at the effect of sample plot size and the total tree carbon (Mg/ha).\n\n\nCode\n#Boxplot\nboxplot_2017 &lt;- mangrove_2017 |&gt;\n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_boxplot() +\n  labs(title = \"Tree Carbon vs Plot Type\",\n       subititle = \"Plantation Year - 2017\",\n       x = \"Plot Size (m2)\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") \n\nggplotly(boxplot_2017)"
  },
  {
    "objectID": "posts/stats-final/index.html#question-1",
    "href": "posts/stats-final/index.html#question-1",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 1",
    "text": "Question 1\nThe graph above shows a visual difference in carbon means between the 25m2 sample plot and the other two sample plots. So let us analyze the relationship between the 77m2 and the 154m2, with a hypothesis test and see if the means in the total carbon per tree (Mg/ha) are different.\nAnalysis: Hypothesis Testing between 77m2 vs 154m2\nThe Null Hypothesis - The Total Carbon per Tree (Mg/ha) mean in the Sample Size of 77m2 is no different from the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} = 0\\]\nAlternative Hypothesis- The Total Carbon per Tree (Mg/ha) in the sample size of 77m2 is different from the one the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} \\neq 0\\]\nCalculate the Point Estimate and the Standard Error\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\\]\n\n\nCode\n#Calculate Point Estimate\npoint_est_2 = (mangrove_plots_2017$mean_tree_c_ha[2] - mangrove_plots_2017$mean_tree_c_ha[3])\n\n#Define the Standard Error\nn_77m = mangrove_plots_2017$tot_samples[2]\ns_77m =  mangrove_plots_2017$sd_tree_c_ha[2]\nn_154m = mangrove_plots_2017$tot_samples[3]\ns_154m =  mangrove_plots_2017$sd_tree_c_ha[3]\n\nprint(paste(\"Point Estimate:\", round(point_est_2,5)))\n\n\n[1] \"Point Estimate: 0.03327\"\n\n\nCode\nSE_2 = as.numeric(sqrt(s_77m^2/n_77m + s_154m^2/n_154m))\nprint(paste(\"Standard Error:\", round(SE_2,5)))\n\n\n[1] \"Standard Error: 0.00359\"\n\n\nCalculate the Z-Score \\[z_{score}=\\frac{\\text { point estimate }-\\text { null value }}{S E}\\]\n\n\nCode\nz_score_2 &lt;- (point_est_2 - 0) / SE_2\nz_score_2\n\n\n[1] 9.271106\n\n\nThe Z-Score will tell us that the the observed difference between a sample plot of 77m2 and 154m2 is 9.3 standard deviations above our null hypothesis of “zero difference” of our dependent variable in Tree Carbon (Mg/ha).\nCalculate the p-value and run a t - test\n\n\nCode\noption2_ttest &lt;- t.test(mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969], mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938])\noption2_ttest\n\n\n\n    Welch Two Sample t-test\n\ndata:  mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969] and mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938]\nt = 9.2711, df = 348.68, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.02621446 0.04033169\nsample estimates:\n mean of x  mean of y \n0.06241964 0.02914657 \n\n\n\nResults:\nWith this results we can reject the null as the p-value is telling us there it exists a statistical significant difference between the means of carbon storage between the sample plot size of 77m2 to the sample size of 154m2.\nWe are 95% confident that the true value of the difference in tree carbon across the two plot sizes lies between 0.0262 - 0.0403 Mg per hectare.\nThis graph summarizes all the important take away.\n\n\nCode\ncrit_val_2 = qnorm(0.025, lower.tail = F)\nci_lower_2 = round(point_est_2 - crit_val_2*SE_2, 3)\nci_upper_2 = round(point_est_2 + crit_val_2*SE_2, 2)\n\nmangrove_2017 |&gt;\n  filter(plot_size_m2 %in% c(76.969, 153.938)) |&gt; \n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_point(alpha = 0.5) +\n  stat_summary(fun= \"mean\", aes(shape= \"mean\"),color = \"darkblue\", geom = \"pointrange\",size = 1.5) +\n  labs(title = \"Tree mg C vs Plot Type\",\n       x = \"Plot Size\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_hline(aes(yintercept = 0.026, linetype = \"Lower CI\"), color = \"gray50\", size = .5) +\n  geom_hline(aes(yintercept = 0.04, linetype = \"Upper CI\"), color = \"gray10\", size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Confidence Intervals\", values = c(2, 2), \n                        guide = guide_legend(override.aes = list(color = c(\"gray50\", \"gray10\"))))\n\n\n\n\n\n\n\nImportant take away from Question 1\nAs we can see in the graph, both means are close to each other, but they are statistically significantly different. With this data analysis and results, we can help the actual client better understand their plot sampling sizes, concluding that smaller plot sizes are getting a larger carbon estimate."
  },
  {
    "objectID": "posts/stats-final/index.html#question-2",
    "href": "posts/stats-final/index.html#question-2",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 2:",
    "text": "Question 2:\nLet analyze why the smaller plots are getting a larger estimate of total carbon per hectare.\nFor this purpose, we will create a new table and graph to easily visualize what the density per plot type (Number of trees per m2) looks like:\n\n\nCode\n#Changing the column to numeric\nmangrove_df$plot_size_m2_num &lt;- as.numeric(mangrove_df$plot_size_m2)\n\ndensity_df &lt;- read_csv(\"/Users/javipatron/Documents/MEDS/Courses/eds222/homework/eds222-finalproject/data/clean_monitoring_data_javier.csv\") |&gt; \n  clean_names() |&gt; \n  filter(plantation_year == 2017) |&gt; \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha) |&gt; \n  group_by(plot, plot_size_m2) |&gt; \n  summarise(sample_count = n(),\n            mean_hight = mean(height_cm),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(density = sample_count/plot_size_m2)\n\nhead(density_df) %&gt;% \n  kbl(align=rep('c', 5),\n      caption =\"Table 4. Density Table per Plot\",\n      position = \"left\") %&gt;%\n  kable_paper(\"hover\", full_width = F)\n\n\n\nTable 4. Density Table per Plot\n\n\nplot\nplot_size_m2\nsample_count\nmean_hight\nmean_tree_c\nmean_carbon_mg_ha\nsd_tree_c\nsd_tree_c_ha\ndensity\n\n\n\n\n12\n153.938\n298\n96.80872\n0.3211477\n0.0208591\n0.2032983\n0.0132004\n1.935844\n\n\n13\n153.938\n183\n84.13115\n0.3955847\n0.0257268\n0.2680318\n0.0174041\n1.188790\n\n\n14\n153.938\n160\n108.31875\n0.5805625\n0.0377312\n0.3058017\n0.0198459\n1.039379\n\n\n16\n153.938\n288\n107.60069\n0.3394236\n0.0220590\n0.3061588\n0.0198987\n1.870883\n\n\n17\n153.938\n223\n105.67265\n0.3120404\n0.0202825\n0.2867950\n0.0186237\n1.448635\n\n\n18\n76.969\n91\n97.43956\n0.3579780\n0.0465385\n0.4200426\n0.0545699\n1.182294\n\n\n\n\n\n\n\nCode\ndensity_graph &lt;- ggplot(density_df, aes(x= as.factor(plot_size_m2), y= density, fill = as.factor(plot_size_m2))) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Density of trees per plot size\",\n       x= \"Plot Size (m2)\",\n       y = \"Density (Tree/m2)\",\n       col = \"Plot Size (m2)\") +\n  scale_fill_discrete(name = \"Plot Type\")\n\nggplotly(density_graph)\n\n\n\n\n\n\nAs we can see in this graph, the plot sizes significantly differ in density (Number of trees per m2). Plots of 25 m2 have a density of 3.2 trees per m2, and plots of 77 m2 have a density of 0.873 trees per m2. To avoid possible bias, this is a point to highlight for future projects.\nNow, to visually show the effect of plot sizes on the total carbon per tree, let us take a deeper look at these two histograms.\nThis Histogram shows how much variance we have between our plots.\n\n\nCode\nggplot(plots_2017, aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density)), color = \"gray20\", fill = \"darkolivegreen\", alpha = 0.7) +\n  geom_density(col = \"gray30\", alpha = 0.15, fill = \"green\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"All Plots\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Denisity Plots\")\n\n\n\n\n\nWe have a skew right histogram with a long tail. Now let us analyze this same plot, but separating per plot type, to see who is responsible for those outliers.\n\n\nCode\nhistogram_plot &lt;- plots_2017 |&gt; \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |&gt; \nggplot(aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Per Plot\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Density Plots\")\n\nggplotly(histogram_plot)\n\n\n\n\n\n\n\n\nCode\n#Total Tree Carbon (mg/ha)\n# Just the 25m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 25.133) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"red\", color = \"gray30\", alpha = 0.6) +\n  geom_density(col = \"gray30\", alpha = 0.3, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"25m2\")\n\n\n\n\n\n\n\nCode\n#Total Tree Carbon (mg/ha)\n# Just the 77m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 76.969) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightgreen\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"77m2\")\n\n\n\n\n\n\n\nCode\n#Total Tree Carbon (mg/ha)\n# Just the 154m2\nmangrove_2017 |&gt; \n  filter(plot_size_m2 == 153.938) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightblue\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"154m2\")\n\n\n\n\n\n\nImportant take away from Question 2:\nThe 25m2 plots have the highest carbon per hectare, a higher density of trees per m2, and a higher variance in their calculations. Overall, the plot of 25m2 has a more considerable amount of carbon stock per hectare, however, with high variability, which is not very confident for the methodologies and the Verified Carbon Standard protocols.\nAn important take away to ask and further analyse with Silvestrum, is why the 25m2 samples are having the highest trees records and the highest tree density in comparison to the 77m2 and 154m2."
  },
  {
    "objectID": "posts/stats-final/index.html#question-3",
    "href": "posts/stats-final/index.html#question-3",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 3:",
    "text": "Question 3:\nAs we can see a significant effect of sample plot size in the calculated Total Carbon per Tree, it would be helpful to put our shoes in the field, and estimate an appropriate sample size to determine the amount of carbon storage in this same area.\nFirst, does a larger number of samples decrease the variance in our calculations?\n\n\nCode\nvariance_graph &lt;- ggplot(plots_2017, aes(x= sample_count, y = (sd_tree_c_ha^2))) +\n  geom_point(aes(color = plot_size_m2)) +\n  geom_smooth(method = lm,\n              color = \"cyan4\",\n              se = F) +\n  labs(title = \"Tree Carbon variance per Sample plot\",\n       x = \"Number of samples per Plot\",\n       y = \"Variance of Carbon in each plot\")\n\nggplotly(variance_graph)\n\n\n\n\n\n\nThis graph shows that a larger number of samples decrease the variance in our calculations. But is there a way we can define our “sweet spot” of sample sizes? In the case of the 154m2 the 3,466 did lower the variance, but it sounds like a lot of work!\nFurthermore, to correctly create a statistical approach, define an “ideal” sample count, and help guide sample sizes in future projects. We will use this data, and the pwr()` package to estimate the number of samples needed to obtain a high power of confidentiality. In other words, depending on the means overlaps for each plot size, we will need more or less samples to get the 95% confidence that we are looking for.\nFirst, we need to understand and visualize the overlaps that we currently have between our sample plots.\n\n\nCode\n#Total Tree Carbon (mg/ha)\nhistogram_all &lt;- mangrove_2017 |&gt; \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |&gt; \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Histogram of Carbon per Tree (Mg/ha)\",\n       subtitle = \"2017\")\n\nggplotly(histogram_all)\n\n\n\n\n\n\nThe histogram above shows the density between all tree measurements per plot size. The important part is to see the overlap between the sample types to calculate the power.\nAnother way to visualize this distribution of a single continuous variable of total tree carbon (Mg/ha) is by dividing the plot sizes into bins and using the frequency polygons geom_freqpoly()` function to display the counts with lines. Frequency polygons are more suitable when you want to compare the distribution across the levels of a categorical variable.\n\n\nCode\n#Plotting just the lines\nggplot(mangrove_2017, aes(x=total_tree_mg_c_ha, color = plot_size_m2)) +\n  geom_freqpoly(aes(y = stat(density))) +\n  labs(title = \"Frequency Polygon\",\n    subtitle = \"Invisible Histogram to see overlaps between Sample Size\",\n       x = \"Total Carbon per Tree (Mg/ha)\",\n       y = \"Density\")\n\n\n\n\n\nThanks to the Frequency polygon graph, we can see clearly the overlap between 77m2 and 154m2 plot types, which are way more correlated than the overlap with 25m2.\nThirdly, we will use the pwr.t.test function to estimate the number of counts we need from each sample plot to have a power of 90%.\n\n\nCode\n# Create the objects with the sd of each group\nsd_25m = mangrove_plots_2017[[1,5]]\nsd_77m = mangrove_plots_2017[[2,5]]\nsd_154m =  mangrove_plots_2017[[3,5]]\nmean_25m = mangrove_plots_2017[[1,4]]\nmean_77m = mangrove_plots_2017[[2,4]]\nmean_154m = mangrove_plots_2017[[3,4]]\n\n\n\nA) Power between 25m2 and 154m2.\n\n\nCode\nmean_difference_a = mean_25m - mean_154m\nd25_154 = as.numeric(sqrt(sd_25m^2/2 + sd_154m^2/2))\n\neffect_size_a = mean_difference_a / d25_154\n\npower_test_1 &lt;- pwr.t.test(d = round(effect_size_a,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_1)\n\n\n\n     Two-sample t test power calculation \n\n              n = 11.46789\n              d = 1.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBecause the means between 25m2 and 154m2 are so different, we will only need twelve samples to have a power of 95%.\n\n\nB) Power between 77m2 and 154m2\n\n\nCode\nmean_difference_b = mean_77m - mean_154m\nd77_154 = as.numeric(sqrt(sd_77m^2/2 + sd_154m^2/2))\neffect_size_b = mean_difference_b / d77_154\n\n\npower_test_2 &lt;- pwr.t.test(d = round(effect_size_b,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_2)\n\n\n\n     Two-sample t test power calculation \n\n              n = 60.64108\n              d = 0.66\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\nplot(power_test_2)\n\n\n\n\n\nIn this case, as the means between 77m2 and 154m2 are closer together, we can estimate that with Sixty samples, we could have a power of 95%."
  },
  {
    "objectID": "posts/stats-final/index.html#conclusion",
    "href": "posts/stats-final/index.html#conclusion",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Conclusion",
    "text": "Conclusion\nFurther research needs to be conducted with other mangrove reforestation sites to increase our confidentiality regarding the correct selection of plot sizes and tree measurements.\nIn order to increase our probabilities and reduce possible bias for future projects, its important to follow the VCS methodologies regarding uncertainty. A random stratified sampling could be a better approach than measuring every single tree in the reforestation. Nevertheless, this study supports the efforts put in place with the 77m2 and 154m2 sample plots as they show a general low variance in their calculations, and a significant lower amount of total measurement specially for the 77m2 plot, saving valuable time in the field. Assuming that all calculation are right in the initial provided data, this study successfully supports the initial question of what is an “ideal” number of samples to reduce extra effort, and still be certain of your estimates (see results of Question 3).  \nOverall, Mangrove forests are an incredible biological ecosystem; this type of small analysis and efforts definitely help the nature-based solutions get closer to unimaginable changes in future projects. As this analysis has proved, there are some questions being answered, but there are still some other important opportunities to be discovered regarding the structure of sampling according to the methodologies, which they seem to be the key factor for certain results.\nPS. Thanks Silvestrum Climate Associates for sharing this collection of data samples data from one of your real reforestation sites!"
  },
  {
    "objectID": "posts/stats-final/index.html#references",
    "href": "posts/stats-final/index.html#references",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "References",
    "text": "References\n\nVerraadmin (2020) First Blue Carbon Conservation Methodology expected to scale up finance for Coastal Restoration & Conservation Activities, Verra. Available at: https://verra.org/first-blue-carbon-conservation-methodology-expected-to-scale-up-finance-for-coastal-restoration-conservation-activities (Accessed: December 8, 2022). \nVM0033 methodology for tidal wetland and Seagrass Restoration, v2.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0033-methodology-for-tidal-wetland-and-seagrass-restoration-v2-0/ (Accessed: December 8, 2022). \nVM0032 methodology for the adoption of sustainable grasslands through adjustment of fire and grazing, v1.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0032-methodology-for-the-adoption-of-sustainable-grasslands-through-adjustment-of-fire-and-grazing-v1-0/ (Accessed: December 8, 2022). \nMcGuinness, K. (2005) Above- and below-ground biomass, and allometry, of four common northern ..., Research Gate. Available at: https://www.researchgate.net/publication/248899546_Above-_and_below-ground_biomass_and_allometry_of_four_common_northern_Australian_mangroves (Accessed: December 9, 2022).\nKauffman, and Donato, (2012) Center for International Forestry Research - CIFOR, Cifor. Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nCenter for International Forestry Research - CIFOR (no date). Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nThe more you know: Mangroves (2020) Ripley’s Aquarium of Canada. Available at: https://www.ripleyaquariums.com/canada/the-more-you-know-mangroves/ (Accessed: December 8, 2022).\n\n\nSupporting Figures\n\nTest the power results with random samples\nHere, we have created a power table with the means and SD of your random table and run a power analysis with the selected random samples to test my results\nTest your results 25m2 vs 154m2 by changing the number of samples!\n\n\nCode\nsamples = 12\nrandom_table_test &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  slice_sample(n = samples)\n\npower_table_test &lt;- random_table_test |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_25m = power_table_test[[1,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_25m = power_table_test[[1,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_a = (random_mean_25m - random_mean_154m) / (as.numeric(sqrt(random_sd_25m^2/2 + random_sd_154m^2/2)))\n\n\nrandom_power_test &lt;- pwr.t.test(d = random_effect_size_a, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\nprint(effect_size_a)\n\n\n[1] 1.584427\n\n\nCode\nprint(random_effect_size_a)\n\n\n[1] 1.781295\n\n\nCode\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = mean_difference_a, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = (random_mean_25m - random_mean_154m), \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_a,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_25m - random_mean_154m),2), \"(Mg/ha)\", \"\\nPower:\", round(random_power_test$power,2)))\n\n\n\n\n\nTest your results 77m2 vs 154m2 with 50 samples\n\n\nCode\nsamples = 60\nrandom_table_test &lt;- mangrove_2017 |&gt; \n  group_by(plot_size_m2) |&gt; \n  slice_sample(n = samples)\n\npower_table_test &lt;- random_table_test |&gt; \n  group_by(plot_size_m2) |&gt; \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |&gt; \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_77m = power_table_test[[2,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_77m = power_table_test[[2,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_b = (random_mean_77m - random_mean_154m) / (as.numeric(sqrt(random_sd_77m^2/2 + random_sd_154m^2/2)))\n\nrandom_power_test_b &lt;- pwr.t.test(d = random_effect_size_b, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\n\nprint(effect_size_b)\n\n\n[1] 0.6571471\n\n\nCode\nprint(random_effect_size_b)\n\n\n[1] 0.7562093\n\n\nCode\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = effect_size_b, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = random_effect_size_b, \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means Difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_b,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_77m - random_mean_154m),2), \"(Mg/ha)\",\"\\nPower:\", round(random_power_test_b$power,2)))\n\n\n\n\n\n\n\nC) Power of just 154m2\n\n\nCode\nd_154 = as.numeric(sqrt(sd_154m^2/2))\neffect_size_c = mean_154m / d77_154\n\n\npower_test_c &lt;- pwr.t.test(d = round(effect_size_c,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_c)\n\n\n\n     Two-sample t test power calculation \n\n              n = 78.23115\n              d = 0.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\nplot(power_test_c)\n\n\n\n\n\n\n\nD) Power of just 77m2\n\n\nCode\nd_77 = as.numeric(sqrt(sd_77m^2/2))\neffect_size_d = mean_77m / d_77\n\n\npower_test_d &lt;- pwr.t.test(d = round(effect_size_d,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_d)\n\n\n\n     Two-sample t test power calculation \n\n              n = 15.08404\n              d = 1.36\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nE) Power of just 25m2\n\n\nCode\nd_25 = as.numeric(sqrt(sd_25m^2/2))\neffect_size_e = mean_25m / d_25\n\n\npower_test_e &lt;- pwr.t.test(d = round(effect_size_e,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_e)\n\n\n\n     Two-sample t test power calculation \n\n              n = 9.759221\n              d = 1.73\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "posts/env-modeling/index.html",
    "href": "posts/env-modeling/index.html",
    "title": "Sensitivity Analysis of Forest Carbon Sequestration",
    "section": "",
    "text": "Carbon sequestration in forests is the process by which trees and vegetation absorb carbon dioxide (CO2) from the atmosphere and store it in the form of carbon (C) within their biomass (AGB) and soils (BGB). Forests play a vital role in mitigating climate change as they act as natural carbon sinks, helping to reduce the concentration of CO2, a greenhouse gas, in the atmosphere.\nCarbon cycle in forests"
  },
  {
    "objectID": "posts/env-modeling/index.html#the-model-ode",
    "href": "posts/env-modeling/index.html#the-model-ode",
    "title": "Sensitivity Analysis of Forest Carbon Sequestration",
    "section": "The Model (ODE)",
    "text": "The Model (ODE)\nConsider the following models of forest growth\n\nForests where C is below a threshold canopy closure.\n\n\\[{\\frac{C}{dt}} = r \\cdot C\\]\nwhere:\n\n\\({C}\\) = forest size measured in units of carbon (C))\n\\({dt}\\) = time increment (change in time.)\n\\({r}\\) = early exponential growth rate of the forest\n\n\nFor forests where C is at or above the threshold canopy closure\n\n\\[{\\frac{dC}{dt}} = g \\cdot (1 - {\\frac{C}{K}})\\]\nwhere:\n\n\\({\\frac{dC}{dt}}\\) = rate of change of \\({C}\\) (Forest size) with respect to time\n\\({g}\\) = linear growth rate of the forest\n\\({K}\\) = carrying capacity in units of carbon of the forest\n\nYou could think of the canopy closure threshold as the size of the forest at which growth rates change from exponential to linear You can think of , as early exponential growth rate and as the linear growth rate once canopy closure has been reached. Now lets start with the assignment.\nRead in the libraries\n\n1. Implement this model in R (as a differential equation). This functions are stored as R scripts in a folder within the working directory.\n\n\nCode\n#' Forest growth condition\n#' @param time - Time since start\n#' @param C - Forest size measured in units of carbon\n#' @param r - Exponential growth rate before before canopy closure\n#' @param g - Linear growth rate after canopy closure (kg/year)\n#' @param K - Carrying capacity in units of carbon of the forest\n#' @param parms - as list with two values, r, g, K\n#' @return derivative of forest size with time\n\n\nequation_forestgrowth &lt;- function(time, C, parms) {\n\n  if(C &lt;= parms$thres) {\n    growth = parms$r * C\n  } else {\n  growth = parms$g * (1 - C/parms$K)\n  }\n  return(list(growth))\n}\n\n\n\n\n2. Run the model for 300 years (using the ODE solver) starting with an initial forest size of 10 kg/C, and using the following parameters: In this case the parameters will be constant.\n\nthresh = canopy closure threshold of 50 kg C\nK = 250 kg C (carrying capacity): On average, it is estimated to range from 200 to 400 kg C per square meter (kg C/m²) of forest area.\nr = 0.01 (exponential growth rate before before canopy closure)\ng = 2 kg/year (linear growth rate after canopy closure)\n\n\n\nCode\n# Set the carbon sequestration starting point\nC &lt;- 10\nK = 250 \nr = 0.01 \ng &lt;- 2\nthresh &lt;- 50\n\n#Now lets use our equation with this fixed values\nyears = seq(from=1, to=300)\n\nparms = list(r = r, \n             K = K,\n             g = g,\n             thresh = thresh)\n\nresult_fixed = ode(y = C,\n             times = years,\n             func = equation_forestgrowth,\n             parms = parms)\n\nhead(result_fixed,10)\n\n\n      time        1\n [1,]    1 10.00000\n [2,]    2 10.10050\n [3,]    3 10.20202\n [4,]    4 10.30455\n [5,]    5 10.40811\n [6,]    6 10.51271\n [7,]    7 10.61837\n [8,]    8 10.72508\n [9,]    9 10.83287\n[10,]   10 10.94175\n\n\n\n\nGraph the results. Here you are graphing the trajectory with the parameters as given (e.g no uncertainty)\n\n\nCode\ncolnames(result_fixed) = c(\"year\",\"C\")\n\n# turn it into a data frame\nresult_fixed = as.data.frame(result_fixed)\n\nggplot(result_fixed, aes(year, C)) +\n  geom_point(color = ifelse(result_fixed$C &lt; 50, \"peru\", \"darkgreen\"), \n             size = 6, \n             alpha = 0.1) +\n  labs(title = \"Tracking the Forest Size Over Time\",\n       subtitle = \"Constant Values (No sensitivity Analysis)\",\n       y = \"Forest Size (C)\",\n       x = \"Time (300 years)\") +\n  theme_minimal()\n\n\n\n\n\nIn the graph above, we can clearly see the threshold being crossed at a Forest Size of 50. To enhance the visual representation, I have incorporated a color change after the year 162, as its when it surpasses the carrying capacity. It is important to note that this graph utilizes fixed parameter values, and no sensitivity analysis has been conducted yet.\n\n\n3. Run a sobol global (vary all parameters at the same time) sensitivity analysis that explores how the estimated maximum forest size (e.g maximum of 300 years, varies with these parameters:\n-   pre canopy closure growth rate ( )\n-   post-canopy closure growth rate ( )\n-   canopy closure threshold and carrying capacity( )\n-   Assume that parameters are all normally distributed with means as given above and standard deviation of 10% of mean value.\n\n\nCode\nset.seed(39)\n\n# Set the list of possible parameters\nnumber_parameters &lt;- 2000\n\nK &lt;- rnorm(mean = 250, sd = 25, n = number_parameters) # Set a variance of SD 1\nr &lt;- rnorm(mean = 0.01, sd = 0.001, n = number_parameters)\ng &lt;- rnorm(mean = 2, sd = 0.2, n = number_parameters)  # Linear growth rate after canopy closure (kg/year)\nthresh &lt;- rnorm(mean = 50, sd = 5, n = number_parameters)\n\n\npossibility1_df &lt;- cbind.data.frame(r=r, K=K, g=g, thresh = thresh)\n\n\nK &lt;- rnorm(mean = 250, sd = 25, n = number_parameters) # Set a variance of SD 1\nr &lt;- rnorm(mean = 0.01, sd = 0.001, n = number_parameters)\ng &lt;- rnorm(mean = 2, sd = 0.2, n = number_parameters)  # Linear growth rate after canopy closure (kg/year)\nthresh &lt;- rnorm(mean = 50, sd = 5, n = number_parameters)\n\npossibility2_df &lt;- cbind.data.frame(r=r, K=K, g=g, thresh = thresh)\n\n# fix any negative values and they are not meaningful\nX1 &lt;- possibility1_df %&gt;% map_df(pmax, 0.0)\nX2 &lt;- possibility2_df %&gt;% map_df(pmax, 0.0)\n\n# create our sobol object and get sets of parameters for running the model\nsens_C &lt;- sobolSalt(model = NULL, X1, X2, nboot = 300)\n\n\n# our parameter sets are\nhead(sens_C$X)\n\n\n            [,1]     [,2]     [,3]     [,4]\n[1,] 0.007689411 245.3609 2.266990 51.90316\n[2,] 0.009590433 219.2689 2.106078 52.95753\n[3,] 0.008733306 239.3199 2.223310 57.08114\n[4,] 0.010061178 235.1005 2.398984 65.32527\n[5,] 0.009540462 261.6831 1.812181 52.32526\n[6,] 0.009764386 260.5410 2.401418 59.82489\n\n\nCode\n# lets add names \ncolnames(sens_C$X) = c(\"r\",\"K\", \"g\", \"thresh\")\n\n\n#This code is to do an example of the randomize numbers in the effect of the graph above.\n\nparms_2 &lt;- list(r = as.data.frame(sens_C$X)$r[1],\n             K = as.data.frame(sens_C$X)$K[1],\n             g = as.data.frame(sens_C$X)$g[1],\n             thresh = as.data.frame(sens_C$X)$thresh[1])\n\nresult_2 &lt;- ode(y = C,\n             times = years,\n             func = equation_forestgrowth,\n             parms = parms_2)\n\ncolnames(result_2) = c(\"year\",\"C\")\n\nhead(result_2)\n\n\n     year        C\n[1,]    1 10.00000\n[2,]    2 10.07719\n[3,]    3 10.15498\n[4,]    4 10.23336\n[5,]    5 10.31236\n[6,]    6 10.39196\n\n\nCode\n# turn it into a data frame\nresult_2 &lt;- as.data.frame(result_2)\n\nggplot(result_2, aes(year, C)) +\n  geom_point(color = ifelse(result_2$C &lt; 50, \"peru\", \"skyblue4\"),\n             size = 6,\n             alpha = 0.1) +\n  labs(title = \"Tracking the Forest Size Over Time\",\n       subtitle = \"Std.Dev Values (Sensitivity Analysis)\",\n       y = \"Forest Size (C)\",\n       x = \"Time (300 years)\") +\n  theme_minimal()\n\n\n\n\n\nIn the graph above shows that the carrying capacity is reached later in time compared to the previous example. This change is reflected by a color variation in the graph. Specifically, in this case (which is random), the carrying capacity is reached after the 211 year of forest growth. In contrast, in the previous graph with fixed values, the carrying capacity was reached at the 162nd year. It is important to note that this graph (blue) utilizes the first values in the our sensitivity analysis table created above.\nNow, lets create two additional functions that will help us\n\nA function that computes the metrics we want\nA function that runs our ODE solver and computes the metrics (I call it a wrapper function as it is really just a workflow/wrapper to call ode solver and then compute metrics)\n\n\n\nCode\n# turn computing our metrics into a function\ncompute_metrics = function(result) {\n  max_forest_size = max(result$C)\n  \n\nreturn(list(max_forest_size = max_forest_size))\n}\n\ncompute_metrics(result_fixed)\n\n\n$max_forest_size\n[1] 183.7213\n\n\nNow we need to apply the ode and this function for all of our parameters\n\n\nCode\n# Define a wrapper function \n# Lets make the threshold 90% of carrying capacity. \n#This function will run the ODE for each parameter\n\ncarbon_wrapper &lt;- function(K,r, g, thresh, C, years, func) {\n  \n  parms = list(K=K, r=r, g=g, thresh=thresh)\n  result = ode(y = C,\n               times = years,\n               func = func,\n               parms = parms)\n    \n    colnames(result) = c(\"Year\", \"C\")\n    \n  # get metrics\n  metrics = compute_metrics(as.data.frame(result))\n  return(metrics)\n\n}\n\n\n\n\nCode\n# now use pmap as we did before\nallresults &lt;- as.data.frame(sens_C$X) %&gt;% pmap(carbon_wrapper, \n                                               C = C, \n                                               years = years, \n                                               func = equation_forestgrowth)\n\n# extract out results from pmap into a data frame\nallres &lt;- allresults %&gt;% map_dfr(`[`, c(\"max_forest_size\"))\n\n\n# create boxplots\ntmp = allres %&gt;% pivot_longer(cols = everything(), \n                              names_to = \"metric\", \n                              values_to = \"value\")\n\n\n\n\n4. Graph the results of the sensitivity analysis as a box plot of maximum forest size and record the two Sobol indices (S and T).\n\n\nCode\n# Create the plot\nggplot(tmp, aes(metric, value, fill = metric)) +\n  geom_boxplot(fill = \"darkgreen\",\n               color = \"black\", \n               alpha = 0.6, \n               outlier.color = \"black\", \n               outlier.shape = 16) +\n  labs(title = \"Distribution of Metrics\",\n       y = \"Value\",\n       x = \"Metric\") +\n  theme_minimal()\n\n\n\n\n\nThe graph illustrates the distribution of different forest growth models. On average, the forest size is projected to reach 181.3 C, with a range between 95.2 and 237.4 C. This box plot represents the results of the sensitivity analysis conducted on 2000 random scenarios, providing valuable insights into the factors influencing forest growth.\n\n\nCompute the sobol indicies for each metric\n\n\nCode\n# sobol can only handle one output at a time - so we will need to do them separately\nsen_C = sensitivity::tell(sens_C, allres$max_forest_size)\n\n# example of the \nsen_C$S[1:3,]\n\n\n    original         bias std. error min. c.i. max. c.i.\nX1 0.3849438 -0.002171223 0.02226505 0.3442150 0.4305949\nX2 0.3303375 -0.001464472 0.02173332 0.2906076 0.3752807\nX3 0.1997678 -0.002116465 0.02199632 0.1609880 0.2466009\n\n\nCode\nsen_C$T[1:3,]\n\n\n    original         bias  std. error min. c.i. max. c.i.\nX1 0.3672519 0.0010813218 0.016604094 0.3273641 0.3992072\nX2 0.3515119 0.0003248276 0.014113185 0.3231483 0.3754006\nX3 0.2075388 0.0002857185 0.008787598 0.1879067 0.2237080\n\n\nIn summary, the numbers in the output represent the sensitivity indices,\n\noriginal: This column represents the original sensitivity indices calculated using the Sobol method. It quantifies the main effect of each input variable on the output variable of interest. A higher value indicates a stronger influence of that particular input variable on the output.\nbias: The bias column represents the discrepancy between the original sensitivity indices and the estimated values. It measures the deviation or error in the sensitivity index estimation. Positive values indicate an overestimation, while negative values indicate an underestimation.\nstd. error: This column indicates the standard error associated with each sensitivity index estimation. It provides a measure of uncertainty or variability in the sensitivity index calculation. Smaller standard error values indicate more precise estimates.\nmin. c.i. & max. c.i.: The confidence interval are a range of values that shows how certain we are about the estimated sensitivity index. A narrower interval means we have a more precise estimate, while a wider interval indicates more uncertainty and less precision in determining the true sensitivity index.\n\nInterpretation in words: In the example of X1, the original sensitivity index for X1 is 0.352 indicating that X1 has a significant main effect on the output variable. The bias for X1 is -0.0004489. This value represents the difference between the original sensitivity index and the estimated value. In this case, the estimated value is slightly lower than the original sensitivity index. The standard error associated with the sensitivity index estimation for X1 is 0.02246737. This value represents the uncertainty or variability in the estimation. A smaller standard error suggests a more precise estimate. The minimum confidence interval for the sensitivity index of X1 is 0.3074961. It provides a lower bound estimate for the sensitivity index. The maximum confidence interval for the sensitivity index of X1 is 0.3912342. It provides an upper bound estimate for the sensitivity index.\n\n\n5. In 2-3 sentences, discuss what the results of your simulation might mean. (For example think about how what parameters climate change might influence).\nClimate change has a significant impact on forests, as they are sensitive to weather conditions and changes in water availability due to higher temperatures. This can result in slower forest growth before reaching full canopy coverage, as water scarcity and decreased soil nutrients impede plant growth. Sensitivity analysis is a valuable method for understanding the initial equations of forest growth, and by creating a randomized sensitivity table, we can observe the potential effects on final forest growth. On average, for this post we can see that the maximum forest size in terms of carbon (C) units is approximately 180, showing a significant increase after reaching canopy closure, but with a flattened growth rate over time as we can see in the first plots."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "EDUCATION",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "resume.html#resume",
    "href": "resume.html#resume",
    "title": "EDUCATION",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "SHARK.html",
    "href": "SHARK.html",
    "title": "Shark_tab",
    "section": "",
    "text": "Extra wizard…\nTo be continued"
  },
  {
    "objectID": "posts/text-analysis/index.html",
    "href": "posts/text-analysis/index.html",
    "title": "Sentiment Analysis of Blue Carbon Discourse",
    "section": "",
    "text": "Overview\nSentiment analysis is a tool for assessing the mood of a piece of text. For example, we can use sentiment analysis to understand public perceptions of topics in environmental policy like energy, climate, and conservation. In this post we will access the Nexis Uni Database through the UCSB library https://www.library.ucsb.edu/research/db/211. Then we will choose a key search term or terms to define a set of articles.\nIn this lab, I will be exploring the words ‘blue cabron’ and ‘mangrove’. I am particularly interested in investigating the relationship between positivism and negativism as they relate to political and government initiatives in the environment.\nHere is a link that takes you to the related articles from the University of California data base.\nUse your search term along with appropriate filters to obtain and download a batch of at least 100 full text search results (.docx)\n\nPart 1: Load the data\nRead your Nexis article document into RStudio.\nNow we will use the full text of the articles for the analysis. First clean any artifacts of the data collection process (hint: this type of thing should be removed: “Apr 04, 2022 (Biofuels Digest: http://www.biofuelsdigest.com/ Delivered by Newstex”))\nIt appears that the composition of the text is not being impacted by any artifacts in this particular section, so im going to leave the tibbles as they come.\n\n\nPart 2: Explore and Clean\nLets explore your data a bit and replicate the analyses above presented in class (Lab2)\n\n\nCode\n# The @ is for indexing within tibbles\nmeta_df &lt;- text_dat@meta\narticles_df &lt;- text_dat@articles\nparagraphs_df &lt;- text_dat@paragraphs\n\ndat2 &lt;- tibble(Date = meta_df$Date, \n              Headline = meta_df$Headline, \n              id = text_dat@articles$ID, \n              text = text_dat@articles$Article)\n\n\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\n\n\nPart 3: Score\nNext, we will calculate the average sentiment score of words in the filtered articles using the stop_words and bing_sentiment datasets.\n\n\nCode\nmean(sentiment_words$sent_num)\n\n\n[1] 0.422933\n\n\n\n\nPart 4: Statistic Analysis\nNow lets calculate mean sentiment (by word polarity) across articles\n\n\nCode\nsentiment_article &lt;- sentiment_words |&gt; \n  group_by(Headline) |&gt; \n  count(id, sentiment) |&gt; \n  pivot_wider(names_from = sentiment, \n              values_from = n,\n              values_fill = 0) |&gt; \n  mutate(polarity = positive - negative)\n\nmean(sentiment_article$polarity)\n\n\n[1] 9.635659\n\n\nUpon this analysis, it is evident that the average sentiment score per article (9.63) is considerably higher than that of words (0.43).\n\n\nPart 5: Examination\nNow, let’s examine the sentiment distribution of articles by plotting a graph, in order to visualize the proportion of positive and negative articles.\n\n\nCode\nggplot(sentiment_article, aes(x = id)) +\n  geom_col(aes(y = positive, fill = \"Positive\"), alpha = 0.8) +\n  geom_col(aes(y = negative, fill = \"Negative\"), alpha = 0.8) +\n  labs(title = \"Sentiment Analysis\",\n       subtitle = \"Positive vs. negative articles\",\n       caption = \"Articles related words: Blue Carbon/ Mangrove\", \n       y = \"Sentiment Score\",\n       x = \"ID of the Article\",\n       fill = \"Sentiment\") +\n  scale_fill_manual(values = c(\"darkorange\", \"darkblue\"), \n                    labels = c(\"Negative\", \"Positive\"), \n                    name = \"Sentiment\") \n\n\n\n\n\n\n\nPart 6: Plots on emotions\nNow, lets look at the nrc contribution to emotion by word plots. Identify and reclassify or remove at least one term that gives misleading results in your context.\n\n\nCode\n# Create a new data frame with the sentiments per word.\nword_sentiment &lt;- get_sentiments(\"nrc\") |&gt; \n  filter(word != \"blue\") #Remove the word blue because is misleading\n\n\n\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(group_by(anti_join(text_words, stop_words, by = \"word\"), : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 8 of `x` matches multiple rows in `y`.\nℹ Row 9845 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nPart 7: Plots over time\nPlot the amount of nrc emotion words as a percentage of all the emotion words used each day (aggregate text from articles published on the same day).\nHow does the distribution of emotion words change over time? Can you think of any reason this would be the case? Plot all the emotion word from the article. What percentage is for the emotion and how it changes over time.\nFun Fact: The NRC lexicon was created by selecting 10,000 words from a thesaurus and asking a set of five questions about each word to reveal associated emotions and polarity. Over 50,000 questions were asked to over 2,000 people on Amazon’s Mechanical Turk website, resulting in a comprehensive word-emotion lexicon for over 10,000 words. The Turkers were paid 4 cents for each set of properly answered questions.\n\n\nCode\nwords_time &lt;- text_words |&gt;\n  select(id , date = Date, word) |&gt; \n  mutate(date = as.Date(date, format = \"%B %d, %Y\")) |&gt; \n  anti_join(stop_words, by= \"word\") |&gt; \n  inner_join(bing_sent) |&gt; \n  rename(bing = sentiment) |&gt; \n  mutate(bing_num = case_when(bing == \"negative\" ~ -1,\n                              bing == \"positive\" ~ 1)) |&gt; \n  inner_join(word_sentiment) |&gt; \n  rename(nrc = sentiment)\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(mutate(rename(inner_join(anti_join(mutate(select(text_words, : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 7 of `x` matches multiple rows in `y`.\nℹ Row 9845 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\nCode\ncolors &lt;- c(\"positive\" = \"green\", \n            \"anticipation\" = \"cyan2\", \n            \"joy\" = \"pink\", \n            \"trust\" = \"coral\",\n            \"negative\" = \"red\", \n            \"sadness\" = \"gray30\", \n            \"anger\" = \"darkorchid3\", \n            \"disgust\" = \"black\",\n            \"fear\" = \"darkred\", \n            \"surprise\" = \"gold\")\n\n\n\n\nCode\nwords_time %&gt;%\n  filter(date &gt;= as.Date(\"2021-1-1\")) |&gt; \n  group_by(date) %&gt;%\n  summarise(bing_num = sum(bing_num)) %&gt;%\n  ggplot(aes(x = date, y = bing_num)) +\n  geom_line(color = \"#6C8EBF\", size = 1.2) +\n  geom_hline(yintercept = 0, color = \"gray10\", size = 0.5, alpha = 0.6 ) +\n  geom_smooth(method = lm, \n              se = FALSE, \n              color = \"#E47E72\") +\n  labs(x = \"Date\", y = \"Sentiment score\", \n       title = \"Sentiment Analysis over Time\", \n       subtitle = \"Bing lexicon classification\", \n       caption = \"Source: library.ucsb \") +\n  theme_classic() +\n  theme(plot.title = element_text(face = \"bold\", size = 18),  \n        axis.text = element_text(size = 12), \n        axis.title = element_text(size = 14),\n        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  scale_x_date(date_labels = \"%b %Y\", \n               breaks = seq(as.Date(\"2021-01-01\"), \n                            as.Date(\"2023-04-16\"), by = \"1 month\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis plot above shows the overall sentiment per day, based on sentiment analysis using the Bing lexicon classification method. The sentiment score is aggregated for each day by summing the scores of individual words analyzed that day, and the resulting plot shows the trend in sentiment over time. The x-axis shows the date range from January 1st, 2021 to April 16th, 2023, and the y-axis shows the sentiment score. The plot suggests that there have been fluctuations in sentiment over time, with both positive and negative trends observed during the period analyzed.\n\n\nCode\nwords_time |&gt; \n  filter(nrc %in% c(\"anticipation\", \"joy\", \"trust\", \"sadness\", \"anger\", \"disgust\" ,\"fear\", \"surprise\")) |&gt; \n  group_by(date, nrc) |&gt; \n  summarise(word_appearance = n()) |&gt; \n  ungroup() |&gt; \n  group_by(date) |&gt; \n  mutate(tot_appearance = sum(word_appearance)) |&gt; \n  mutate(proportion = word_appearance/tot_appearance) |&gt; \n  ggplot(aes(x=date, y= proportion, color = nrc)) +\n  geom_line(alpha = 0.2) +\n  geom_smooth(method = \"loess\",\n              se = F,\n              span = 0.5,\n              alpha = 0.4) +\n  labs(x = \"Date\", \n       y = \"Sentiment Proportion\", \n       color = \"NRC Sentiment\", \n       title = \"Sentiment Proportion over Time\", \n       subtitle = \"NRC word classification\", \n       caption = \"Source: library.ucsb \") \n\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe plot above displays the distribution of NRC sentiments used over time, enabling an examination of any potential correlations. To enhance clarity, I have removed the postive, and negative sentiments, and used the geom_smooth() function to create trend lines that are easier to interpret. This is particularly useful due to the recent high influx of articles related to blue carbon, which can make the line graphs difficult to decipher. Nevertheless, I have made these lines transparent (alpha = 0.2) in the background to ensure that the trends remain visible. Upon analyzing the plot, two significant trends become apparent: surprise is consistently low, while trust and sadness are consistently high.\nBelow, you can see the total distribution of sentiments across our database.\n\n\nCode\n# Compute the proportion of each category\nnrc_proportions &lt;- words_time %&gt;%\n  count(nrc) %&gt;%\n  mutate(proportion = n / sum(n))\n\nnrc_proportions %&gt;%\n  mutate(proportion = sprintf(\"%.2f%%\", proportion * 100)) %&gt;% # Multiply by 100 and format as percentage\n  kable(format = \"html\", align = \"c\") %&gt;%\n  kable_styling(full_width = F, \n                bootstrap_options = \"striped\", \n                position = \"center\", \n                font_size = 16)\n\n\n\n\n\nnrc\nn\nproportion\n\n\n\n\nanger\n153\n5.28%\n\n\nanticipation\n235\n8.11%\n\n\ndisgust\n104\n3.59%\n\n\nfear\n188\n6.49%\n\n\njoy\n264\n9.11%\n\n\nnegative\n514\n17.74%\n\n\npositive\n780\n26.92%\n\n\nsadness\n241\n8.32%\n\n\nsurprise\n69\n2.38%\n\n\ntrust\n349\n12.05%\n\n\n\n\n\n\n\n\n\nCode\nwords_time %&gt;%\n  group_by(date, nrc) %&gt;%\n  summarise(appearance = n()) %&gt;%\n  group_by(nrc) %&gt;%\n  mutate(cum_appearance = cumsum(appearance)) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(cum_appearance)) |&gt; \n  ggplot(aes(x = date, y = cum_appearance, color = nrc)) +\n  geom_line(alpha = 0.8) +\n   scale_color_manual(values = colors) +\n  labs(x = \"Date\", \n       y = \"Cumulative Appearances\", \n       color = \"NRC Sentiment\", \n       title = \"Sentiment Cumulative Analysis over Time\", \n       subtitle = \"NRC classification\", \n       caption = \"Source: library.ucsb \")\n\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\nIn the above graph, I have calculated a cumulative score for each sentiment to gain insight into the overall trends of words and vibes used in the articles. This plot also highlights the high influx of articles in recent years. As anticipated, the positive and negative sentiments have the highest scores, but the positive line surpasses the negative line by a significant margin. Additionally, there are noteworthy correlations between certain words and the sentiments of trust, joy, and sadness, which have seen increased usage in recent times. Conversely, surprise and disgust are used less frequently, representing the less common sentiments.\n\n\n\n\n\nCitationBibTeX citation:@online{patrón2023,\n  author = {Patrón, Javier},\n  title = {Sentiment {Analysis} of {Blue} {Carbon} {Discourse}},\n  date = {2023-04-18},\n  url = {https://github.com/javipatron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrón, Javier. 2023. “Sentiment Analysis of Blue Carbon\nDiscourse.” April 18, 2023. https://github.com/javipatron."
  },
  {
    "objectID": "posts/eez-spatial/index.html",
    "href": "posts/eez-spatial/index.html",
    "title": "Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .2\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nSea surface temperature: 11-30 °C\nDepth: 0-70 meters below sea level\n\n\n\n\nCombining vector/raster data\nResampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "posts/eez-spatial/index.html#overview",
    "href": "posts/eez-spatial/index.html#overview",
    "title": "Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .2\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nSea surface temperature: 11-30 °C\nDepth: 0-70 meters below sea level\n\n\n\n\nCombining vector/raster data\nResampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "posts/eez-spatial/index.html#data",
    "href": "posts/eez-spatial/index.html#data",
    "title": "Marine Aquaculture",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\nPrepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\n\nLoad necessary packages and set path\n\nI recommend using the here package\n\nRead in the shape file for the West Coast EEZ (wc_regions_clean.shp)\nRead in SST rasters\n\naverage_annual_sst_2008.tif\naverage_annual_sst_2009.tif\naverage_annual_sst_2010.tif\naverage_annual_sst_2011.tif\naverage_annual_sst_2012.tif\n\nCombine SST rasters into a raster stack\nRead in bathymetry raster (depth.tif)\nCheck that data are in the same coordinate reference system\n\nReproject any data not in the same projection\n\n\n\n\n\nCode\n# Setting my filepaths\nrootdir &lt;- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndata &lt;- file.path(rootdir,\"data\",\"assignment4\")\n\n# Creating the names for each file\nsst_2008 &lt;- 'average_annual_sst_2008.tif' \nsst_2009 &lt;- \"average_annual_sst_2009.tif\"\nsst_2010 &lt;- \"average_annual_sst_2010.tif\"\nsst_2011 &lt;- \"average_annual_sst_2011.tif\"\nsst_2012 &lt;- \"average_annual_sst_2012.tif\"\ndepth &lt;- \"depth.tif\"\n  \n# Downloading the raster data to a star object\nsst_2008 &lt;- rast(file.path(data, sst_2008))\nsst_2009 &lt;- rast(file.path(data, sst_2009))\nsst_2010 &lt;- rast(file.path(data, sst_2010))\nsst_2011 &lt;- rast(file.path(data, sst_2011))\nsst_2012 &lt;- rast(file.path(data, sst_2012))\nwc_regions &lt;- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `/Users/javipatron/Documents/MEDS/Courses/eds223/data/assignment4/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nCode\ndepth &lt;- rast(file.path(data,depth))\n\n# Stack all the raster\nall_sst &lt;- c(sst_2008, sst_2009, sst_2010, sst_2011, sst_2012)\n\n\n# Check coordinate reference system\n#st_crs(wc_regions)\n#st_crs(depth)\n#st_crs(all_sst)\n\n\n# Set the new CRS\nall_sst &lt;- project(all_sst, \"EPSG:4326\")\nall_sst\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 5  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nnames       : averag~t_2008, averag~t_2009, averag~t_2010, averag~t_2011, averag~t_2012 \nmin values  :      278.8167,      278.0800,      279.9200,      278.8600,      278.1920 \nmax values  :      301.4321,      301.4475,      300.9486,      307.2733,      309.8502 \n\n\nPrint the West Cost Polygon Vector data to see how it looks like:\n\n\nCode\ntm_shape(wc_regions) +\n  tm_polygons(col=\"rgn\",\n              palette= \"RdYlBu\",\n              legend.reverse = T,\n              title = \"EEZ West Coast Regions\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass()\n\n\nSome legend labels were too wide. These labels have been resized to 0.44, 0.45, 0.49. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\nCode\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing"
  },
  {
    "objectID": "posts/eez-spatial/index.html#process-data",
    "href": "posts/eez-spatial/index.html#process-data",
    "title": "Marine Aquaculture",
    "section": "Process Data",
    "text": "Process Data\nNext, we need process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\n\nFind the mean SST from 2008-2012\nConvert SST data from Kelvin to Celsius\nCrop depth raster to match the extent of the SST raster\n\nnote: the resolutions of the SST and depth data do not match\n\nResample the depth data to match the resolution of the SST data using the nearest neighbor approach.\nCheck that the depth and SST match in resolution, extent, and coordinate reference system. Can the rasters be stacked?\n\n\n\nCode\n#Finding the mean \nmean_sst &lt;- terra::app(all_sst, mean)\n\n#Converting to Celsius\nmean_sst_c &lt;- (mean_sst - 273.15)\n\n#Cropping the Depth to just the area of SST\ncrop_depth &lt;- crop(depth, mean_sst)\n\nclass(crop_depth)\n\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nCode\n# Re-sample the depth with the needed resolution\nnew_depth &lt;- terra::resample(crop_depth, mean_sst_c, method = \"near\")\n\n# Stack both rasters and see if they match\nstack_depth_sst &lt;- c(mean_sst_c, new_depth)\n\n\n\nFind suitable locations\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\n\nReclassify SST and depth data into locations that are suitable for Lump sucker fish\n\nhint: set suitable values to 1 and unsuitable values to NA\n\nFind locations that satisfy both SST and depth conditions\n\nhint: create an overlay using the lapp() function multiplying cell values\n\n\n\n\n\nCode\n#Create the matrix for Temperature between 11°C and 30°C\ntemp_vector &lt;- c(-Inf, 11, NA, \n                   11, 30, 1,\n                   30, Inf, NA)\n\ntemp_oysters_matrix &lt;- matrix(temp_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of °C\ntemp_oysters &lt;- classify(mean_sst_c, temp_oysters_matrix)\n\n\n#Create the matrix for Depth between 0 & -70\ndepth_vector &lt;- c(-Inf, -70, NA, \n                   -70, 0, 1,\n                   0, Inf, NA)\n\ndepth_oysters_matrix &lt;- matrix(depth_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of depth\ndepth_oysters &lt;- classify(new_depth, depth_oysters_matrix, include.lowest = T)\n\n# Combine the two raster\nmatrixes &lt;- c(depth_oysters, temp_oysters)\n\n# Rename the matrixes attributes\nnames(matrixes) &lt;- c(\"temp_matx\",\"depth_matx\")\n\n#Combine both matrixes\ncombined_matrix_stack &lt;- c(matrixes, stack_depth_sst)\n\n\n\n\nCode\n#Find the locations where the oysters have a 1 in the pixel.\ncheck_condition &lt;- function(x,y){\n  return(x * y)\n   }\n\ntemp_conditions &lt;- lapp(combined_matrix_stack[[c(1,3)]], fun = check_condition)\ndepth_conditions &lt;- lapp(combined_matrix_stack[[c(2,4)]], fun = check_condition)\n\ntm_shape(temp_conditions) +\n  tm_raster(title = \"Sea Temp °C\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\nCode\ntm_shape(depth_conditions) +\n  tm_raster(title = \"Depth (Meters)\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\nNow lets create the mask of both conditions\n\n\nCode\nsuitable_conditions &lt;- lapp(matrixes[[c(1,2)]], fun = check_condition)\nprint(suitable_conditions)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : lyr1 \nmin value   :    1 \nmax value   :    1"
  },
  {
    "objectID": "posts/eez-spatial/index.html#determine-the-most-suitable-eez",
    "href": "posts/eez-spatial/index.html#determine-the-most-suitable-eez",
    "title": "Marine Aquaculture",
    "section": "Determine the most suitable EEZ",
    "text": "Determine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nSelect suitable cells within West Coast EEZs\nFind area of grid cells\nFind the total suitable area within each EEZ\n\nhint: it might be helpful to rasterize the EEZ data\n\nFind the percentage of each zone that is suitable\n\nhint it might be helpful to join the suitable area by region onto the EEZ vector data\n\n\n\n\nCode\ncell_ezz &lt;- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n\nrast_ezz &lt;- rasterize(wc_regions, suitable_conditions, field= 'rgn')\nmask_ezz &lt;-  mask(rast_ezz, suitable_conditions)\nsuitable_area &lt;- zonal(cell_ezz, mask_ezz, sum )\n\njoined_area &lt;-  left_join(wc_regions, suitable_area, by = 'rgn') |&gt; \n  mutate(area_suitkm2 = area,\n         percentage = (area_suitkm2 / area_km2) * 100,\n         .before = geometry)"
  },
  {
    "objectID": "posts/eez-spatial/index.html#visualize-results",
    "href": "posts/eez-spatial/index.html#visualize-results",
    "title": "Marine Aquaculture",
    "section": "Visualize results",
    "text": "Visualize results\nNow that we have results, we need to present them!\nCreate the following maps:\n\nTotal suitable area by region\nPercent suitable area by region\n\n\n\nCode\ntm_shape(joined_area) +\n  tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Total Suitable area per EEZ for: Oysters\"),\n            frame = T)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(joined_area) +\n  tm_polygons(col = \"percentage\", palette = \"RdYlBu\", legend.reverse = T,\n              title = \"Percent (%)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Suitable Area per EEZ for: Oysters \"),\n            frame = T)"
  },
  {
    "objectID": "posts/eez-spatial/index.html#conclusion",
    "href": "posts/eez-spatial/index.html#conclusion",
    "title": "Marine Aquaculture",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see in the maps above the most suitable areas for the oysters are the northerner regions with a 3 to 3.5 % of the entire EEZ region. As the assignments states the sea surface temperature that the oysters like are between the 11-30 °C, and for the depth is between 0-70 meters below sea level, which are pretty specific. Thus, this results can give us hints on how the Economic Exclusive Zones help those species, and we can compare to other EEZ or other species of the US West Coast."
  },
  {
    "objectID": "posts/eez-spatial/index.html#broaden-your-workflow",
    "href": "posts/eez-spatial/index.html#broaden-your-workflow",
    "title": "Marine Aquaculture",
    "section": "Broaden your workflow!",
    "text": "Broaden your workflow!\nNow that you’ve worked through the solution for one group of species, let’s update your workflow to work for other species. Please create a function that would allow you to reproduce your results for other species. Your function should be able to do the following:\n\n\nAccept temperature and depth ranges and species name as inputs\n\nCreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\nRun your function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\n\n\nCode\nfind_my_happy_place &lt;- function(species = \"NAME\", min_temp = 5, max_temp = 30, min_depth = 0, max_depth = -5468) {\n  temp_vector &lt;- c(-Inf, min_temp, NA, min_temp, max_temp, 1,max_temp, Inf, NA)\n  temp_matrix &lt;- matrix(temp_vector, ncol= 3, byrow = T)\n  temp_condition &lt;- classify(mean_sst_c, temp_matrix)\n  depth_vector &lt;- c(-Inf, max_depth, NA, max_depth, min_depth, 1, min_depth, Inf, NA)\n  depth_matrix &lt;- matrix(depth_vector, ncol= 3, byrow = T)\n  depth_condition &lt;- classify(new_depth, depth_matrix, include.lowest = T)\n  mix_rasters &lt;- c(depth_condition, temp_condition)\n  suitable_conditions &lt;- lapp(mix_rasters[[c(1,2)]], fun = check_condition)\n  cell_ezz &lt;- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n  rast_ezz &lt;- rasterize(wc_regions, suitable_conditions, field= 'rgn')\n  mask_ezz &lt;-  mask(rast_ezz, suitable_conditions)\n  suitable_area &lt;- zonal(cell_ezz, mask_ezz, sum )\n  joined_area &lt;-  left_join(wc_regions, suitable_area, by = 'rgn') |&gt;\n    mutate(happy_area_km2 = area,\n           \"happy_(%)\" = (happy_area_km2 / area_km2) * 100,\n           .before = geometry) |&gt; \n    arrange(desc(happy_area_km2))\n  map &lt;- tmap_arrange(tm_shape(joined_area) +\n                        tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Total Suitable area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T),\n                        tm_shape(joined_area) +\n                        tm_polygons(col = \"happy_(%)\", palette = \"RdYlBu\", legend.reverse = T,\n                                    title = \"Percent (%)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Suitable Area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T))\n  print(joined_area[c(\"rgn\", \"rgn_key\", \"area_km2\", \"happy_area_km2\", \"happy_(%)\",\"geometry\")])\n  print(paste(\"*Conclusion:* For the species\", species, \"the most suitable region is\", joined_area$rgn[1], \"with\", round(joined_area$happy_area_km2[1],2), \"km2 of 'happy' area.\"))\n  \n  map\n}\n\n\n\nNow Test your function\nREMEMBER:\n1. The species name has to be in quotes. The Default value is “NAME”.\n2. The depth has to include the negative sign for the maximum depth.\nDefault; Min: 0, Max: -5468.\n3. The Temperature is in °C .\nDefault; Min: 5, Max: 30).\n\n\nCode\nfind_my_happy_place()\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2 happy_(%)\n1 Southern California    CA-S 206860.78      205730.95  99.45382\n2  Central California    CA-C 202738.33      201478.95  99.37882\n3              Oregon      OR 179994.06      178650.46  99.25353\n4 Northern California    CA-N 164378.81      162975.61  99.14636\n5          Washington      WA  66898.31       64695.95  96.70790\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-123.4318 4...\n4 MULTIPOLYGON (((-124.2102 4...\n5 MULTIPOLYGON (((-122.7675 4...\n[1] \"*Conclusion:* For the species NAME the most suitable region is Southern California with 205730.95 km2 of 'happy' area.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Try your species!! \nfind_my_happy_place(\"Turtle\", 13, 28, 0, -290)\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2  happy_(%)\n1 Southern California    CA-S 206860.78     10890.3024 5.26455645\n2  Central California    CA-C 202738.33       360.9648 0.17804468\n3          Washington      WA  66898.31        29.1610 0.04359004\n4              Oregon      OR 179994.06             NA         NA\n5 Northern California    CA-N 164378.81             NA         NA\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-122.7675 4...\n4 MULTIPOLYGON (((-123.4318 4...\n5 MULTIPOLYGON (((-124.2102 4...\n[1] \"*Conclusion:* For the species Turtle the most suitable region is Southern California with 10890.3 km2 of 'happy' area.\""
  },
  {
    "objectID": "posts/eez-spatial/index.html#footnotes",
    "href": "posts/eez-spatial/index.html#footnotes",
    "title": "Marine Aquaculture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).↩︎\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).↩︎\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "posts/land-change/index.html",
    "href": "posts/land-change/index.html",
    "title": "Land Cover Analysis",
    "section": "",
    "text": "Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into landcover classes enables us to understand the distribution and change in landcover types over large areas. There are many approaches for performing landcover classification – supervised approaches use training data labeled by the user, whereas unsupervised approaches use algorithms to create groups which are identified by the user afterward.\n\ncredit: this lab is based on a materials developed by Chris Kibler."
  },
  {
    "objectID": "posts/land-change/index.html#summary",
    "href": "posts/land-change/index.html#summary",
    "title": "Land Cover Analysis",
    "section": "Summary",
    "text": "Summary\n\nload and process Landsat scene\n\ncrop and mask Landsat data to study area\n\nextract spectral data at training sites\n\ntrain and apply decision tree classifier\n\nplot results"
  },
  {
    "objectID": "posts/land-change/index.html#data",
    "href": "posts/land-change/index.html#data",
    "title": "Land Cover Analysis",
    "section": "Data",
    "text": "Data\nLandsat 5 Thematic Mapper\n\n\nLandsat 5\n1 scene from September 25, 2007\n\nbands: 1, 2, 3, 4, 5, 7\nCollection 2 surface reflectance product\n\n\nStudy area and training data\n\npolygon representing southern Santa Barbara county\npolygons representing training sites\n\n\ntype: character string with land cover type"
  },
  {
    "objectID": "posts/land-change/index.html#process-data",
    "href": "posts/land-change/index.html#process-data",
    "title": "Land Cover Analysis",
    "section": "Process data",
    "text": "Process data\n\nLoad packages and set working directory\nWe’ll be working with vector and raster data, so will need both sf and terra. To train our classification algorithm and plot the results, we’ll use the rpart and rpart.plot packages. Set your working directory to the folder that holds the data for this lab.\n\nNote: my filepaths may look different than yours!\n\n\nCode\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)\n\nrm(list = ls())\n\n\n\n\nLoad Landsat data\nLet’s create a raster stack based on the 6 bands we will be working with. Each file name ends with the band number (e.g. B1.tif). Notice that we are missing a file for band 6. Band 6 corresponds to thermal data, which we will not be working with for this lab. To create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the rast function. We’ll then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n\nCode\n# list files for each band, including the full file path\nfilelist &lt;- list.files(\"/Users/javipatron/Documents/MEDS/Courses/eds223/eds223-week9/data/landsat-data\", full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat_20070925 &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat_20070925) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\nLoad study area\nWe want to contstrain our analysis to the southern portion of the county where we have training data, so we’ll read in a file that defines the area we would like to study.\n\n\nCode\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(\"/Users/javipatron/Documents/MEDS/Courses/eds223/eds223-week9/data/SB_county_south.shp\")\n\n\nReading layer `SB_county_south' from data source \n  `/Users/javipatron/Documents/MEDS/Courses/eds223/eds223-week9/data/SB_county_south.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -120.2327 ymin: 34.33603 xmax: -119.5757 ymax: 34.53716\nGeodetic CRS:  NAD83\n\n\nCode\n# project to match the Landsat data\nSB_county_south &lt;- st_transform(SB_county_south, crs = crs(landsat_20070925))\n\n\n\n\nCrop and mask Landsat data to study area\nNow, we can crop and mask the Landsat data to our study area. This reduces the amount of data we’ll be working with and therefore saves computational time. We can also remove any objects we’re no longer working with to save space.\n\n\nCode\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat_20070925, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat_20070925, SB_county_south, landsat_cropped)\n\n\n\n\nConvert Landsat values to reflectance\nNow we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\n\nIn this case, we are working with Landsat Collection 2. The valid range of pixel values for this collection 7,273-43,636, with a multiplicative scale factor of 0.0000275 and an additive scale factor of -0.2. So we reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%.\n\n\nCode\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\nCode\n# check values are 0 - 100\nsummary(landsat)\n\n\nWarning: [summary] used a sample\n\n\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809"
  },
  {
    "objectID": "posts/land-change/index.html#classify-image",
    "href": "posts/land-change/index.html#classify-image",
    "title": "Land Cover Analysis",
    "section": "Classify image",
    "text": "Classify image\n\nExtract reflectance values for training data\nWe will load the shapefile identifying different locations within our study area as containing one of our 4 land cover types. We can then extract the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\nCode\n# read in and transform training data\ntraining_data &lt;- st_read(\"/Users/javipatron/Documents/MEDS/Courses/eds223/eds223-week9/data/trainingdata.shp\") %&gt;%\n  st_transform(., crs = crs(landsat))\n\n\nReading layer `trainingdata' from data source \n  `/Users/javipatron/Documents/MEDS/Courses/eds223/eds223-week9/data/trainingdata.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 40 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 215539.2 ymin: 3808948 xmax: 259927.3 ymax: 3823134\nProjected CRS: WGS 84 / UTM zone 11N\n\n\nCode\n# extract reflectance values at training sites\ntraining_data_values &lt;- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n  mutate(type = as.factor(type)) # convert landcover type to factor\n\n\n\n\nTrain decision tree classifier\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are). The rpart function implements the CART algorithm. The rpart function needs to know the model formula and training data you would like to use. Because we are performing a classification, we set method = \"class\". We also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\nTo understand how our decision tree will classify pixels, we can plot the results. The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\nCode\n# establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data,\n                          method = \"class\",\n                          na.action = na.omit)\n\n\n# Plot decision tree with colored last nodes\nprp(SB_decision_tree, extra = 2, under = TRUE, tweak = 1, branch = 1, shadow.col = \"gray\", round = 1, cex = 1, split.cex = 1.3, col = \"gray20\", box.palette = list(\"#8DB580\", \"#F2DDA4\", \"#7E8987\", \"#6A8EAE\"))\n\n\n\n\n\n\n\nApply decision tree\nNow that we have created our decision tree, we can apply it to our entire image. The terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n\nCode\n# classify image based on decision tree\nSB_classification &lt;- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n\n\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\"           \n\n\n\n\nPlot results\nNow we can plot the results and check out our land cover map!\n\n\nCode\nlibrary(tmap)\n\n# Plot results with improved aesthetics\ntm_shape(SB_classification) +\n  tm_raster(style = \"cat\",\n            palette = c(\"#8DB580\", \"#F2DDA4\", \"#7E8987\", \"#6A8EAE\"),\n            labels = c(\"Green Vegetation\",\n                       \"Soil/Dead Grass\",\n                       \"Urban\",\n                       \"Water\"),\n            title = \"Land Cover\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\")) +\n  tm_compass(type = \"arrow\",\n             size = 2,\n             position = c(\"right\", \"top\"),\n             color.light = \"black\",\n             color.dark = \"black\") +\n  tm_scale_bar(color.light = \"gray90\",\n               color.dark = \"gray30\",\n               size = 0.9, \n               position = c(\"right\", \"bottom\"))\n\n\nWarning: The argument size of tm_scale_bar is deprecated. It has been renamed\nto text.size\n\n\nstars object downsampled to 1611 by 620 cells. See tm_shape manual (argument raster.downsample)"
  },
  {
    "objectID": "posts/blog-example/index.html",
    "href": "posts/blog-example/index.html",
    "title": "My Blog Post Title",
    "section": "",
    "text": "Hi this my first class blog post, and this is my first graph in R.\nSHARK\n\n\n\n\n\nThis page is intended as a wake-up call, illustrating the progression before and after my master’s degree. The graph you see here took a lot of time, lessons learned, and effort from my past self in August 2022. It was during this time that I started diving deeper into the field of data science.\nThis post is a way for me to express my creativity and share my belief that practicing, repeating, and staying disciplined can lead to incredible achievements. If you’re reading this now, it’s because you have a special dedication and commitment to paying attention. I want to sincerely congratulate you for genuinely engaging with my webpage. Your careful attention to detail is greatly appreciated.\nThe world needs more people like you who care about the environment. So, thank you.\nHere is some more information about webpages and posts(Csik 2022)\n\n\n\n\nReferences\n\nCsik, Samantha. 2022. “Adding a Blog to Your Existing Quarto Website.” October 24, 2022. https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/.\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Patrón, Javier},\n  title = {My {Blog} {Post} {Title}},\n  date = {2022-08-28},\n  url = {https://javipatron.github.io/posts/blog_example_1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrón, Javier. 2022. “My Blog Post Title.” August 28,\n2022. https://javipatron.github.io/posts/blog_example_1/."
  },
  {
    "objectID": "posts/ethics-post/index.html",
    "href": "posts/ethics-post/index.html",
    "title": "Project Chaac Dives into the Pool of Ethical Sense",
    "section": "",
    "text": "Data is a new language in which we read, write, and transform the figures our eyes cannot naturally see, hidden in our surrounding reality. However, thanks to the technological revolution, those “natural numbers” can now be deciphered and manipulated by people who are fortunate and deft enough to speak and understand such language. \nRephrasing a definition of data coined by Stefania Milan “Data is seen as an avenue to revert or challenge our dominant understandings of the world, (re)creating conditions of possibility for counter-imaginaries and social justice claims to emerge.” With that in mind, In this blog post, I will discuss how data experts would be able to generate more consciousness on ethics and bias while developing a more sensitive touch in their daily work as environmental data scientists. To help express these thoughts on paper, I will use an actual environmental project held in the Yucatán Peninsula of Mexico and discuss how responsible and equitable data scientists should behave when exercising the power of data to design a world case example out of this project. \nMy musings expressed in this post are meant to awaken present and future “data evangelists”, motivating them to nourish a better sense of community within their field, and understanding the substantial change that both environment and society will experience through Project Chaac. This reflection will be based on the skills and foundations I have acquired throughout the course of Ethics and Bias in Environmental Data Science at the Bren School of Environmental Science & Management. \nAt its core, Project Chaac seeks to protect, conserve, and restore a territory of 42,676 hectares of degraded mangroves in the southern Mexican state of Yucatán. The project involves a significant amount of team management that must be aligned with overall environmental impact. Worldwide, most of the lands and territories targeted for greenhouse gas (GHG) mitigation action overlap with areas customarily held by indigenous groups, local communities, and Afro-descendant peoples. \nMoreover, most of the lands and forests targeted for nature-based greenhouse gas removal and offset, like Project Chaac, are located within areas where the State’s customary rights of communities have yet to be recognized (Reference two). Therefore, I will briefly describe some difficulties and potential roadblocks involving ethics and bias that the project might face in Mexico, since its complexity resides in the entangled path of multiple factors and circumstances at constant play. Metaphorically speaking, this project is a living, growing vine, in which the roots are essential; biodiversity stands for the open minds involved; key nutrients represent human values, and the environment as a whole is the high frequency vibe of all the passionate individuals that will provide a firm and rich soil to plant the seed of change. Finally, we will analyze how this tiny seed can become a fully grown, jade-green turquoise plant, as astonishing as the Strongylodon Macrobotrys.\n\n\n\nImage 1: How does a seed germinate?\n\n\nAs with all plants, a careful assessment of the seed germination and the creation of concise strategies will have an exponential impact on the growth and direction of the project. Here are the three axes on which I foresee true potential for building solid bridges between ethics and bias resides:\n\nData Justice.- Satellite images are used to select the pieces of land with the highest potential for mangrove restoration in the area. Therefore an equitable allocation of carbon credits rights, land use, and business strategy must be well cemented among all players involved: Federal and local authorities, as well as the private sector, along with the rural communities called ejidos, as well as the territories inhabited by Afro-descendants. (Reference four)\nProject Development.- The actual value of the land comes when you dip your hands in the dirt while sweating big fat droplets on the rich soil, learning and adapting to your surroundings. In this case, the project implies complicated engineering analysis to build an efficient network of hydrological channels in the near future, in order to embrace a healthy and protected ecosystem in the restored mangrove lands.\n\n\n\n\nImage 2: Hydrological construction and rehabilitation of canals. Photo: Primary Production Laboratory (CINVESTAV- IPN)\n\n\n\nLegal Feasibility.- Carbon accounting in developing countries is facing difficult but also exciting times. Agreements regarding carbon rights will encounter critical roadblocks, and hence need to be exceptionally well-planned. Additionally, with climate change hot on our heels and the Paris Agreement goals swiftly approaching, countries and corporations are increasing their participation to meet their emission reduction targets and net-zero commitments 5. Keeping that in mind, embracing and understanding Mexican laws will become a crucial element for the success of the project.\n\nNow, I would like to define what Blue Carbon is and why it is so important. A newly developed concept, it refers to the carbon stocks sequestered in any coastal ecosystem, like mangroves, sea grasses, and salt marshes. Mangroves are considered one of the most productive and biologically complex ecosystems on earth; that is why they are also regarded as Nature’s “superheroes” fighting against climate change. \nHowever, over the last few years, mangroves have been deforested at an alarming rate, due to increased coastal development and other anthropogenic factors. Thus, conservation and restoration of these coastal ecosystems are essential both to sustain the natural environment and to assure their decisive impact on our world. As with coral reefs, mangrove forests allow the development of true environmental health by fostering fisheries and healthy coastal ecosystems, as well as by providing significant protection from natural disasters.\nNext, I will introduce the topic of Data Justice and why it is relevant to this project and blog post. Data Justice happens whenever data relies on insufficient recognition to understand real community needs. It is an approach that addresses new ways of data collection and dissemination of crucial facts which have been invisibilized in the past, thus harming marginalized communities (Reference six). Therefore, it plays an essential role in carbon-credit adjudications. In environmental science, carbon stocks represent the total amount of organic carbon stored within an environmental system, so the correct manipulation of data is key to quantifying such processes. Hence, data scientists play a vital role in elegant calculations in the pursuit of data justice.\nRegarding carbon rights in Mexico, I am presenting two tables to describe some general carbon trade legal agreements. Carbon rights are used to describe a number of different tradeable GHG prerogatives. In other words, it is the given right to benefit from sequestered carbon and/or reduced greenhouse gas emissions.” (Reference two)\n\n\nCode\n# Add the tables in a nice format\n\n\n\nTable 1. Legal frameworks to support carbon-linked transactions.\n\n\n\n\n\n\n\n\n\n\nCountry\nLaws for securing community tenure to forests?\nEstablished a national legal framework for carbon trade?\nDefined carbon rights?\nAre carbon rights linked to tenure?\nDo communities have carbon rights?\n\n\n\n\nMexico\nYes\nPartial\nInconclusive or undefined\nInconclusive or undefined\nInconclusive or undefined\n\n\n\n\nTable 2. Legal recognition of community land.\n\n\n\n\n\n\n\n\nCountry\nCountry area where rights of Indigenous  peoples, local communities, and Afro-descendants are legally recognized\nCountry area where rights of Indigenous peoples, local communities, and Afro-descendants are not legally recognized\nTotal percent of land held by Indigenous peoples, local communities, and Afro-descendants\n\n\n\n\nMexico\n52 %\n0.50%\n52.5 %\n\n\n\nAs the tables clearly depict, Mexico has an inconclusive law on carbon rights. However, almost half of its territory is legally recognized when referring to its indigenous peoples, local communities, and Afro-descendants, which is today a leading example among developing countries.\nSo now we have an overview of the project’s objectives. However, how and where does data manipulation impact its success? \nCurrently, the CINVESTAV (The Center for Research and Advanced Studies of the National Polytechnic Institute), a non-governmental Mexican research institution, has settled and defined the actual conditions of the mangroves by using satellite imagery. After collecting images from the Sentinel-2A 7 from March 2021 to June 2022, and by using a powerful environmental data science tool called NDVI (Normalized Difference Vegetation Index), they were able to quantify and define the areas with higher and lower vegetation density health. In other words, they designated the patches of land in specific areas of interest, where vegetation reflects green back to space (meaning healthy mangroves), comparing such areas against those showing degraded or deforested vegetation, (which reflected longer color wavelengths).\n\n\n\n\nImage 3: NDVI Formula\n\n\nBy using the NDVI tool, a language that only a few people know about (remember “potential bias”?), the CINVESTAV team defined those sections of land with a higher probability of mangrove reforestation along with the potential for the development of a potential carbon accounting business. As you can imagine, such results designate an important plant tutor, which sets the standard for launching critical collaborative agreements with land owners, state governments, private companies, and ejidos.\nNowadays, technology has become a potent influential tool across the globe, but it can also be scary and threatening to those who do not understand it. Consequently, I invite all involved parties, especially data scientists, to think outside the box from the very beginning and bring the terminology of Data Justice into play. \nIn contrast, if a project lacks a strategy of environmental data justice or fails to recognize an equitable carbon right among indigenous people and/or local communities, it will surely generate a negative domino effect with huge implications, leaving long-lasting scars for all future environmental nature-based initiatives.\nThroughout the course of Ethics and Bias, I learned the importance of becoming data-sensitive. I am one of those scientists who are fortunate enough to speak the language of data, having developed a sense for reading and hearing his surroundings in order to find a deeper connection with their heart and soul, ensuring that the local wisdom moves from one generation to the next, honoring its ancient knowledge as an invaluable heritage. I wish to become a key player in communicating the importance of this work, quickly generating new allies within the communities. Undoubtedly, data sensitivity is the vital foundation for building a solid ethical environment.\nUnfortunately, only some countries explicitly recognize community carbon rights, and even fewer have tested established rules’ operational and political feasibility. However, Project Chaac has all the tools to become a world case example of a well-organized project, although a topnotch level of data sensitivity, knowledge on carbon rights, data justice, and firm management must be assured to bring all the pieces together.\nOverall, the success of this mangrove restoration project depends heavily on a vigorous commitment between all players involved: Collaborators, government agents, lawyers, field engineers, data scientists, and ejido leaders, among others. These 40,000 hectares of land are heavily threatened by tourism and pressure from the governmental instances. Yet, I am certain that Project Chaac represents a golden opportunity to show the world how a nature-based solution with a well-cemented ethical culture is definitely possible. All of the aforementioned factors will empower our incipient vine to evolve into an emerald wonder that will make the world’s eyes widen in awe, aiming to the skies.\n\n\n\nImage 4: The beautiful blue jade vine (Strongylodon Macrobotrys)\n\n\nReferences:\n1. Stefania, S. (2019) Full article: Exploring Data Justice: Conceptions, applications and ..., Exploring Data Justice: Available at: https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1606268 (Accessed: December 8, 2022).\n2. Initiative, A.R.and R. (2020) Rights-based conservation: The path to preserving Earth’s biological and cultural diversity?, Rights + Resources. Available at: https://rightsandresources.org/publication/rights-based-conservation/ (Accessed: December 7, 2022).\n3. Strongylodon macrobotrys (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Strongylodon_macrobotrys (Accessed: December 7, 2022).\n4. Ejido (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Ejido (Accessed: December 7, 2022).\n5. Hood, C. (2020) Completing the Paris ‘rulebook’: Key article 6 issues, Center for Climate and Energy Solutions. Available at: https://www.c2es.org/document/completing-the-paris-rulebook-key-article-6-issues/ (Accessed: December 7, 2022).\n6. Taylor, L. (2017). What is Data Justice? The case for connecting digital rights and freedoms globally. Big Data & Society.https://doi.org/10.1177/2053951717736335.\n7. Home (no date) Sentinel. Available at: https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a (Accessed: December 7, 2022).\n\n\n\n\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Patrón, Javier},\n  title = {Project {Chaac} {Dives} into the {Pool} of {Ethical} {Sense}},\n  date = {2022-12-19},\n  url = {https://github.com/javipatron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrón, Javier. 2022. “Project Chaac Dives into the Pool of\nEthical Sense.” December 19, 2022. https://github.com/javipatron."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Javier Patrón",
    "section": "",
    "text": "Engineer &\n\n\nEnvironmental Data Scientist"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blogs",
    "section": "",
    "text": "Sensitivity Analysis of Forest Carbon Sequestration\n\n\n\nStatistics\n\n\nForest\n\n\nR\n\n\nMEDS\n\n\n\nBy conducting a sensitivity analysis, the goal is to identify the most significant factors, quantify their impact, and gain precise insights into the dynamics of forest…\n\n\n\nJavier Patrón\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis of Blue Carbon Discourse\n\n\n\nMangroves\n\n\nMachine Learning\n\n\nR\n\n\nMEDS\n\n\n\nAn investigation of the articles from the New York Times and the libraries of the University of California to examine the emotional tone associated with blue carbon trends…\n\n\n\nJavier Patrón\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInorganic Carbon in Machine Leaning\n\n\n\nOcean\n\n\nMachine Leaning\n\n\nR\n\n\nMEDS\n\n\n\nUsing data from the California Cooperative Oceanic Fisheries Investigations program (CalCOFI) we will train machine learning models for predicting dissolved inorganic carbon…\n\n\n\nJavier Patrón\n\n\nApr 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLand Cover Analysis\n\n\n\nSpatial\n\n\nOcean\n\n\nR\n\n\nMEDS\n\n\n\nBy utilizing remote sensing analysis of land cover, we gain insights into the effects of climate change, natural disasters, deforestation, and urbanization, enabling us to…\n\n\n\nJavier Patrón\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Economic Exclusive Zone in Perú\n\n\n\nPython\n\n\nSpatial\n\n\nOcean\n\n\nMEDS\n\n\n\nAssessing Trends in Fishing Effort Inside and Outside Peru’s EEZ Using AIS Data from Global Fishing Watch.\n\n\n\nJavier Patrón\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Chaac Dives into the Pool of Ethical Sense\n\n\n\nMangroves\n\n\nMEDS\n\n\n\nAn Ethics and Bias thought\n\n\n\nJavier Patrón\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Aquaculture\n\n\n\nSpatial\n\n\nOcean\n\n\nR\n\n\nMEDS\n\n\n\nDetermining which Exclusive Economic Zones (EEZ) on the US West Coast are best suited to developing marine aquaculture for ocean species like Oysters.\n\n\n\nJavier Patrón\n\n\nDec 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAn Statistical Mangrove Analysis Post\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\nMangroves\n\n\n\nA blue carbon analysis based on the skills learned from the course of Statistics for Environmental Data Science at the Bren School of Environmental Science & Management\n\n\n\nJavier Patrón\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas Outage\n\n\n\nMEDS\n\n\nSpatial\n\n\nR\n\n\n\nAnalyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area\n\n\n\nJavier Patrón\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Blog Post Title\n\n\n\nMEDS\n\n\nR\n\n\nOcean\n\n\n\nWhat my first post looks like\n\n\n\nJavier Patrón\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]