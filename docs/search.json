[
  {
    "objectID": "SHARK.html",
    "href": "SHARK.html",
    "title": "Shark_tab",
    "section": "",
    "text": "Extra wizard…\nTo be continued"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Javier Patrón",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "posts/stats_final/index.html#introduction",
    "href": "posts/stats_final/index.html#introduction",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Introduction",
    "text": "Introduction\nBlue carbon is attracting massive attention from governments, investors, and companies needing carbon accounting practices. Mangrove forests can sequester almost ten times more than boreal, temperate, or tropical forests, thanks to the mangrove biological characteristics. That is one of the main reasons they received the nickname of “The Superheroes of Climate Change.”\nMoreover, carbon stocks are the total amount of organic carbon stored within a system and are comprised of carbon pools, which fall into different bins: living aboveground living (live plants, including epiphytes); dead aboveground biomass (e.g., fallen branches); living belowground biomass (e.g., roots); and belowground carbon (sediment organic matter).\nWith that in mind, it is critical to understand the main factors to calculate the total mangrove carbon stocks’ in restoration practices. Additionally, to analyze and calculate an appropriate amount of tree samples and plot sizes to estimate the amount of carbon storage in the area.\n\n\n\nThe more you know: Mangroves (2020)\n\n\n\nIs there existing evidence for this question? If so, why is it inconclusive?\nBlue carbon accounting is a new strategy with few practical examples and evidence worldwide. Some methodologies align with the Verra non-profit organization VCS (Verified Carbon Standard) that helps us describe and outline the correct procedures to quantify the net greenhouse gas emission reductions and removals resulting from project activities implemented to restore tidal wetlands. For this case, the provided data uses equations and formulas designed on the VCS0032 and VCS0033 methodologies, which are aligned with the VCS."
  },
  {
    "objectID": "posts/stats_final/index.html#data",
    "href": "posts/stats_final/index.html#data",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Data",
    "text": "Data\nThe data we will analyze to answer our questions are actual field measurements from mangrove trees over four replantation sites (2010, 2016, 2017, 2020). This data was provided directly to me by Silvestrum Climate Associates ®. This data is confidential, and there is limited access to the site.\n\n\n\n\n\n\n\nLimitations, spatial and temporal features.\nThe dataset contains a record for each tree height and canopy. With those measurements, Silvestrum has calculated the total tree carbon in (kg) and the total tree carbon per hectare in (Mg/ha). In this analysis, we will assume that the correct protocols and equations were followed for this purpose, and this analysis is dependent from those results.\nRegarding the dataset, the above and below ground carbon storage was assessed using a nested sub-sampling protocol along the 25m2, 77m2, and 154m2 transect following the Kauffman and Donato procedure. The equations for above and below ground biomass are defined by Comley and McGuiness.\nFurthermore, seeking for data limitations this mathematical calculations made by Silvestrum is a significant one, as this observations do not have the actual carbon samples to be analyzed. Another limitation is the possible man errors from the field sampling and the natural growth and variability from the mangroves.\nTo get you familiarized, the table below shows the first 5 rows of the data we will be studying. It has a total of 10,826 rows and 11 columns.\n\n\n\n\nTable 1. Full Mangrove Data Set (head)\n \n  \n    plantation_year \n    plot \n    height_cm \n    canopy_width_1_cm \n    canopy_width_2_cm \n    crown_area_m2 \n    cd_chatting_m \n    chatting_agb_kg \n    chatting_agb_mg_ha \n    comley_mc_guinness_planted_bgb_kg \n    comley_mc_guinness_planted_bgb_mg_ha \n    total_tree_kg \n    total_tree_mg_ha \n    chatting_agc_kg_c \n    chatting_agc_mg_c_ha \n    comley_mc_guinness_planted_bgc_kg_c \n    comley_mc_guinness_planted_bgc_mg_c_ha \n    total_tree_kg_c \n    total_tree_mg_c_ha \n    plot_size_m2 \n  \n \n\n  \n    2017 \n    8 \n    112 \n    59 \n    57 \n    0.264 \n    0.58 \n    0.494 \n    0.032 \n    0.456 \n    0.030 \n    0.950 \n    0.062 \n    0.237 \n    0.015 \n    0.178 \n    0.012 \n    0.415 \n    0.027 \n    153.938 \n  \n  \n    2017 \n    2_new \n    280 \n    90 \n    101 \n    0.716 \n    0.96 \n    1.436 \n    0.571 \n    1.326 \n    0.527 \n    2.762 \n    1.099 \n    0.689 \n    0.274 \n    0.517 \n    0.206 \n    1.206 \n    0.480 \n    25.133 \n  \n  \n    2017 \n    2_new \n    275 \n    114 \n    119 \n    1.066 \n    1.17 \n    2.198 \n    0.874 \n    2.028 \n    0.807 \n    4.226 \n    1.681 \n    1.055 \n    0.420 \n    0.791 \n    0.315 \n    1.846 \n    0.734 \n    25.133 \n  \n  \n    2017 \n    2_new \n    274 \n    95 \n    102 \n    0.762 \n    0.99 \n    1.534 \n    0.611 \n    1.416 \n    0.564 \n    2.951 \n    1.174 \n    0.737 \n    0.293 \n    0.552 \n    0.220 \n    1.289 \n    0.513 \n    25.133 \n  \n  \n    2017 \n    2_new \n    264 \n    133 \n    104 \n    1.103 \n    1.19 \n    2.279 \n    0.907 \n    2.104 \n    0.837 \n    4.383 \n    1.744 \n    1.094 \n    0.435 \n    0.820 \n    0.326 \n    1.914 \n    0.762 \n    25.133 \n  \n  \n    2017 \n    2_new \n    264 \n    133 \n    104 \n    1.103 \n    1.19 \n    2.279 \n    0.907 \n    2.104 \n    0.837 \n    4.383 \n    1.744 \n    1.094 \n    0.435 \n    0.820 \n    0.326 \n    1.914 \n    0.762 \n    25.133 \n  \n\n\n\n\n\n\n\n\n\nTable 2. Total mangrove Dataset summary\n \n  \n    plantation_year \n    Plot Count \n    Sample Count \n    Total Carbon (mg/ha) \n    Total Carbon Variance (mg/ha) \n  \n \n\n  \n    2010 \n    7 \n    746 \n    45.725 \n    0.1072349 \n  \n  \n    2017 \n    27 \n    4126 \n    242.993 \n    0.1302739 \n  \n  \n    2019 \n    8 \n    998 \n    22.994 \n    0.0185163 \n  \n  \n    2020 \n    46 \n    4956 \n    87.824 \n    0.0159968 \n  \n\n\n\n\n\nAs we can see in the table above, the 2017 plantation year has an exciting number of samples, different sample plot sizes, and high variation of total carbon in Mg per hectare. We will focus on this year to answer our questions.\nThe table #3 contains only information from the 2017 plantation year. We will use this filter to analyse the different questions, and interpret our results.\n\n\nShow code\nmangrove_2017 <- mangrove_df |> \n  filter(plantation_year == 2017) |> \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha)\nmangrove_2017 <- mangrove_2017[!(mangrove_2017$total_tree_mg_c_ha== 1.224),]\n\n#Statistical Table with the selected variables for question #1\nmangrove_plots_2017 <- mangrove_2017 |> \n  group_by(plot_size_m2) |> \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |> \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nmangrove_plots_2017 %>% \n  kbl(align=rep('c', 5),\n      caption =\"Table 3. Plot summary of the 2017 plantation year\",\n      position = \"left\") %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nTable 3. Plot summary of the 2017 plantation year\n \n  \n    plot_size_m2 \n    tot_samples \n    tot_plot_count \n    mean_tree_c_ha \n    sd_tree_c_ha \n  \n \n\n  \n    25m2 \n    323 \n    4 \n    0.3708173 \n    0.3035103 \n  \n  \n    77m2 \n    336 \n    5 \n    0.0624196 \n    0.0651300 \n  \n  \n    154m2 \n    3466 \n    18 \n    0.0291466 \n    0.0297558 \n  \n\n\n\n\n\nShow code\n#Statistical table of each unique plot to analyse variability in the question #2\nplots_2017 <- mangrove_2017 |> \n  group_by(plot, plot_size_m2) |> \n  summarise(sample_count = n(),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha))\n\n\nNow let us take a closer look at the effect of sample plot size and the total tree carbon (Mg/ha).\n\n\nShow code\n#Boxplot\nboxplot_2017 <- mangrove_2017 |>\n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_boxplot() +\n  labs(title = \"Tree Carbon vs Plot Type\",\n       subititle = \"Plantation Year - 2017\",\n       x = \"Plot Size (m2)\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") \n\nggplotly(boxplot_2017)"
  },
  {
    "objectID": "posts/stats_final/index.html#question-1",
    "href": "posts/stats_final/index.html#question-1",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 1",
    "text": "Question 1\nThe graph above shows a visual difference in carbon means between the 25m2 sample plot and the other two sample plots. So let us analyze the relationship between the 77m2 and the 154m2, with a hypothesis test and see if the means in the total carbon per tree (Mg/ha) are different.\nAnalysis: Hypothesis Testing between 77m2 vs 154m2\nThe Null Hypothesis - The Total Carbon per Tree (Mg/ha) mean in the Sample Size of 77m2 is no different from the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} = 0\\]\nAlternative Hypothesis- The Total Carbon per Tree (Mg/ha) in the sample size of 77m2 is different from the one the 154m2.\n\\[H_0: \\mu_{77m2} - \\mu_{154m2} \\neq 0\\]\nCalculate the Point Estimate and the Standard Error\n\\[SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\\]\n\n\nShow code\n#Calculate Point Estimate\npoint_est_2 = (mangrove_plots_2017$mean_tree_c_ha[2] - mangrove_plots_2017$mean_tree_c_ha[3])\n\n#Define the Standard Error\nn_77m = mangrove_plots_2017$tot_samples[2]\ns_77m =  mangrove_plots_2017$sd_tree_c_ha[2]\nn_154m = mangrove_plots_2017$tot_samples[3]\ns_154m =  mangrove_plots_2017$sd_tree_c_ha[3]\n\nprint(paste(\"Point Estimate:\", round(point_est_2,5)))\n\n\n[1] \"Point Estimate: 0.03327\"\n\n\nShow code\nSE_2 = as.numeric(sqrt(s_77m^2/n_77m + s_154m^2/n_154m))\nprint(paste(\"Standard Error:\", round(SE_2,5)))\n\n\n[1] \"Standard Error: 0.00359\"\n\n\nCalculate the Z-Score \\[z_{score}=\\frac{\\text { point estimate }-\\text { null value }}{S E}\\]\n\n\nShow code\nz_score_2 <- (point_est_2 - 0) / SE_2\nz_score_2\n\n\n[1] 9.271106\n\n\nThe Z-Score will tell us that the the observed difference between a sample plot of 77m2 and 154m2 is 9.3 standard deviations above our null hypothesis of “zero difference” of our dependent variable in Tree Carbon (Mg/ha).\nCalculate the p-value and run a t - test\n\n\nShow code\noption2_ttest <- t.test(mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969], mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938])\noption2_ttest\n\n\n\n    Welch Two Sample t-test\n\ndata:  mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 76.969] and mangrove_2017$total_tree_mg_c_ha[mangrove_2017$plot_size_m2 == 153.938]\nt = 9.2711, df = 348.68, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.02621446 0.04033169\nsample estimates:\n mean of x  mean of y \n0.06241964 0.02914657 \n\n\n\nResults:\nWith this results we can reject the null as the p-value is telling us there it exists a statistical significant difference between the means of carbon storage between the sample plot size of 77m2 to the sample size of 154m2.\nWe are 95% confident that the true value of the difference in tree carbon across the two plot sizes lies between 0.0262 - 0.0403 Mg per hectare.\nThis graph summarizes all the important take away.\n\n\nShow code\ncrit_val_2 = qnorm(0.025, lower.tail = F)\nci_lower_2 = round(point_est_2 - crit_val_2*SE_2, 3)\nci_upper_2 = round(point_est_2 + crit_val_2*SE_2, 2)\n\nmangrove_2017 |>\n  filter(plot_size_m2 %in% c(76.969, 153.938)) |> \n  ggplot(aes(x=plot_size_m2, y = total_tree_mg_c_ha,color = plot_size_m2)) +\n  geom_point(alpha = 0.5) +\n  stat_summary(fun= \"mean\", aes(shape= \"mean\"),color = \"darkblue\", geom = \"pointrange\",size = 1.5) +\n  labs(title = \"Tree mg C vs Plot Type\",\n       x = \"Plot Size\",\n       y = \"Total Tree Carbon (Mg/ha)\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_hline(aes(yintercept = 0.026, linetype = \"Lower CI\"), color = \"gray50\", size = .5) +\n  geom_hline(aes(yintercept = 0.04, linetype = \"Upper CI\"), color = \"gray10\", size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Confidence Intervals\", values = c(2, 2), \n                        guide = guide_legend(override.aes = list(color = c(\"gray50\", \"gray10\"))))\n\n\n\n\n\n\n\nImportant take away from Question 1\nAs we can see in the graph, both means are close to each other, but they are statistically significantly different. With this data analysis and results, we can help the actual client better understand their plot sampling sizes, concluding that smaller plot sizes are getting a larger carbon estimate."
  },
  {
    "objectID": "posts/stats_final/index.html#question-2",
    "href": "posts/stats_final/index.html#question-2",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 2:",
    "text": "Question 2:\nLet analyze why the smaller plots are getting a larger estimate of total carbon per hectare.\nFor this purpose, we will create a new table and graph to easily visualize what the density per plot type (Number of trees per m2) looks like:\n\n\nShow code\n#Changing the column to numeric\nmangrove_df$plot_size_m2_num <- as.numeric(mangrove_df$plot_size_m2)\n\ndensity_df <- read_csv(\"/Users/javipatron/Documents/MEDS/Courses/eds222/homework/eds222-finalproject/data/clean_monitoring_data_javier.csv\") |> \n  clean_names() |> \n  filter(plantation_year == 2017) |> \n  select(height_cm,plot, plot_size_m2, cd_chatting_m,total_tree_kg_c,total_tree_mg_c_ha) |> \n  group_by(plot, plot_size_m2) |> \n  summarise(sample_count = n(),\n            mean_hight = mean(height_cm),\n            mean_tree_c = mean(total_tree_kg_c),\n            mean_carbon_mg_ha = mean(total_tree_mg_c_ha),\n            sd_tree_c = sd(total_tree_kg_c),\n            sd_tree_c_ha = sd(total_tree_mg_c_ha)) |> \n  mutate(density = sample_count/plot_size_m2)\n\nhead(density_df) %>% \n  kbl(align=rep('c', 5),\n      caption =\"Table 4. Density Table per Plot\",\n      position = \"left\") %>%\n  kable_paper(\"hover\", full_width = F)\n\n\n\n\nTable 4. Density Table per Plot\n \n  \n    plot \n    plot_size_m2 \n    sample_count \n    mean_hight \n    mean_tree_c \n    mean_carbon_mg_ha \n    sd_tree_c \n    sd_tree_c_ha \n    density \n  \n \n\n  \n    1_new \n    25.133 \n    95 \n    154.35789 \n    0.8619895 \n    0.3429368 \n    0.9612535 \n    0.3824683 \n    3.779891 \n  \n  \n    12 \n    153.938 \n    298 \n    96.80872 \n    0.3211477 \n    0.0208591 \n    0.2032983 \n    0.0132004 \n    1.935844 \n  \n  \n    13 \n    153.938 \n    183 \n    84.13115 \n    0.3955847 \n    0.0257268 \n    0.2680318 \n    0.0174041 \n    1.188790 \n  \n  \n    14 \n    153.938 \n    160 \n    108.31875 \n    0.5805625 \n    0.0377312 \n    0.3058017 \n    0.0198459 \n    1.039379 \n  \n  \n    16 \n    153.938 \n    288 \n    107.60069 \n    0.3394236 \n    0.0220590 \n    0.3061588 \n    0.0198987 \n    1.870883 \n  \n  \n    17 \n    153.938 \n    223 \n    105.67265 \n    0.3120404 \n    0.0202825 \n    0.2867950 \n    0.0186237 \n    1.448635 \n  \n\n\n\n\n\nShow code\ndensity_graph <- ggplot(density_df, aes(x= as.factor(plot_size_m2), y= density, fill = as.factor(plot_size_m2))) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Density of trees per plot size\",\n       x= \"Plot Size (m2)\",\n       y = \"Density (Tree/m2)\",\n       col = \"Plot Size (m2)\") +\n  scale_fill_discrete(name = \"Plot Type\")\n\nggplotly(density_graph)\n\n\n\n\n\n\nAs we can see in this graph, the plot sizes significantly differ in density (Number of trees per m2). Plots of 25 m2 have a density of 3.2 trees per m2, and plots of 77 m2 have a density of 0.873 trees per m2. To avoid possible bias, this is a point to highlight for future projects.\nNow, to visually show the effect of plot sizes on the total carbon per tree, let us take a deeper look at these two histograms.\nThis Histogram shows how much variance we have between our plots.\n\n\nShow code\nggplot(plots_2017, aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density)), color = \"gray20\", fill = \"darkolivegreen\", alpha = 0.7) +\n  geom_density(col = \"gray30\", alpha = 0.15, fill = \"green\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"All Plots\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Denisity Plots\")\n\n\n\n\n\nWe have a skew right histogram with a long tail. Now let us analyze this same plot, but separating per plot type, to see who is responsible for those outliers.\n\n\nShow code\nhistogram_plot <- plots_2017 |> \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |> \nggplot(aes(x=sd_tree_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Per Plot\",\n       x = \"Tree Carbon Variance (Mg/ha)\",\n       y = \"Density Plots\")\n\nggplotly(histogram_plot)\n\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 25m2\nmangrove_2017 |> \n  filter(plot_size_m2 == 25.133) |> \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"red\", color = \"gray30\", alpha = 0.6) +\n  geom_density(col = \"gray30\", alpha = 0.3, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"25m2\")\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 77m2\nmangrove_2017 |> \n  filter(plot_size_m2 == 76.969) |> \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightgreen\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"77m2\")\n\n\n\n\n\n\n\nShow code\n#Total Tree Carbon (mg/ha)\n# Just the 154m2\nmangrove_2017 |> \n  filter(plot_size_m2 == 153.938) |> \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density)), fill = \"lightblue\", color = \"gray30\") +\n  geom_density(col = \"gray30\", alpha = 0.5, fill = \"gray10\") +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"154m2\")\n\n\n\n\n\n\nImportant take away from Question 2:\nThe 25m2 plots have the highest carbon per hectare, a higher density of trees per m2, and a higher variance in their calculations. Overall, the plot of 25m2 has a more considerable amount of carbon stock per hectare, however, with high variability, which is not very confident for the methodologies and the Verified Carbon Standard protocols.\nAn important take away to ask and further analyse with Silvestrum, is why the 25m2 samples are having the highest trees records and the highest tree density in comparison to the 77m2 and 154m2."
  },
  {
    "objectID": "posts/stats_final/index.html#question-3",
    "href": "posts/stats_final/index.html#question-3",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Question 3:",
    "text": "Question 3:\nAs we can see a significant effect of sample plot size in the calculated Total Carbon per Tree, it would be helpful to put our shoes in the field, and estimate an appropriate sample size to determine the amount of carbon storage in this same area.\nFirst, does a larger number of samples decrease the variance in our calculations?\n\n\nShow code\nvariance_graph <- ggplot(plots_2017, aes(x= sample_count, y = (sd_tree_c_ha^2))) +\n  geom_point(aes(color = plot_size_m2)) +\n  geom_smooth(method = lm,\n              color = \"cyan4\",\n              se = F) +\n  labs(title = \"Tree Carbon variance per Sample plot\",\n       x = \"Number of samples per Plot\",\n       y = \"Variance of Carbon in each plot\")\n\nggplotly(variance_graph)\n\n\n\n\n\n\nThis graph shows that a larger number of samples decrease the variance in our calculations. But is there a way we can define our “sweet spot” of sample sizes? In the case of the 154m2 the 3,466 did lower the variance, but it sounds like a lot of work!\nFurthermore, to correctly create a statistical approach, define an “ideal” sample count, and help guide sample sizes in future projects. We will use this data, and the pwr()` package to estimate the number of samples needed to obtain a high power of confidentiality. In other words, depending on the means overlaps for each plot size, we will need more or less samples to get the 95% confidence that we are looking for.\nFirst, we need to understand and visualize the overlaps that we currently have between our sample plots.\n\n\nShow code\n#Total Tree Carbon (mg/ha)\nhistogram_all <- mangrove_2017 |> \n  filter(plot_size_m2 %in% c(25.133, 76.969, 153.938)) |> \nggplot(aes(x=total_tree_mg_c_ha)) +\n  geom_histogram(aes(y = stat(density), fill = plot_size_m2),color = \"gray30\", alpha = 0.3) +\n  geom_density(aes(fill = plot_size_m2), col = \"gray30\", alpha = 0.8) +\n  labs(title = \"Histogram of Carbon per Tree (Mg/ha)\",\n       subtitle = \"2017\")\n\nggplotly(histogram_all)\n\n\n\n\n\n\nThe histogram above shows the density between all tree measurements per plot size. The important part is to see the overlap between the sample types to calculate the power.\nAnother way to visualize this distribution of a single continuous variable of total tree carbon (Mg/ha) is by dividing the plot sizes into bins and using the frequency polygons geom_freqpoly()` function to display the counts with lines. Frequency polygons are more suitable when you want to compare the distribution across the levels of a categorical variable.\n\n\nShow code\n#Plotting just the lines\nggplot(mangrove_2017, aes(x=total_tree_mg_c_ha, color = plot_size_m2)) +\n  geom_freqpoly(aes(y = stat(density))) +\n  labs(title = \"Frequency Polygon\",\n    subtitle = \"Invisible Histogram to see overlaps between Sample Size\",\n       x = \"Total Carbon per Tree (Mg/ha)\",\n       y = \"Density\")\n\n\n\n\n\nThanks to the Frequency polygon graph, we can see clearly the overlap between 77m2 and 154m2 plot types, which are way more correlated than the overlap with 25m2.\nThirdly, we will use the pwr.t.test function to estimate the number of counts we need from each sample plot to have a power of 90%.\n\n\nShow code\n# Create the objects with the sd of each group\nsd_25m = mangrove_plots_2017[[1,5]]\nsd_77m = mangrove_plots_2017[[2,5]]\nsd_154m =  mangrove_plots_2017[[3,5]]\nmean_25m = mangrove_plots_2017[[1,4]]\nmean_77m = mangrove_plots_2017[[2,4]]\nmean_154m = mangrove_plots_2017[[3,4]]\n\n\n\nA) Power between 25m2 and 154m2.\n\n\nShow code\nmean_difference_a = mean_25m - mean_154m\nd25_154 = as.numeric(sqrt(sd_25m^2/2 + sd_154m^2/2))\n\neffect_size_a = mean_difference_a / d25_154\n\npower_test_1 <- pwr.t.test(d = round(effect_size_a,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_1)\n\n\n\n     Two-sample t test power calculation \n\n              n = 11.46789\n              d = 1.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBecause the means between 25m2 and 154m2 are so different, we will only need twelve samples to have a power of 95%.\n\n\nB) Power between 77m2 and 154m2\n\n\nShow code\nmean_difference_b = mean_77m - mean_154m\nd77_154 = as.numeric(sqrt(sd_77m^2/2 + sd_154m^2/2))\neffect_size_b = mean_difference_b / d77_154\n\n\npower_test_2 <- pwr.t.test(d = round(effect_size_b,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_2)\n\n\n\n     Two-sample t test power calculation \n\n              n = 60.64108\n              d = 0.66\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nShow code\nplot(power_test_2)\n\n\n\n\n\nIn this case, as the means between 77m2 and 154m2 are closer together, we can estimate that with Sixty samples, we could have a power of 95%."
  },
  {
    "objectID": "posts/stats_final/index.html#conclusion",
    "href": "posts/stats_final/index.html#conclusion",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "Conclusion",
    "text": "Conclusion\nFurther research needs to be conducted with other mangrove reforestation sites to increase our confidentiality regarding the correct selection of plot sizes and tree measurements.\nIn order to increase our probabilities and reduce possible bias for future projects, its important to follow the VCS methodologies regarding uncertainty. A random stratified sampling could be a better approach than measuring every single tree in the reforestation. Nevertheless, this study supports the efforts put in place with the 77m2 and 154m2 sample plots as they show a general low variance in their calculations, and a significant lower amount of total measurement specially for the 77m2 plot, saving valuable time in the field. Assuming that all calculation are right in the initial provided data, this study successfully supports the initial question of what is an “ideal” number of samples to reduce extra effort, and still be certain of your estimates (see results of Question 3).  \nOverall, Mangrove forests are an incredible biological ecosystem; this type of small analysis and efforts definitely help the nature-based solutions get closer to unimaginable changes in future projects. As this analysis has proved, there are some questions being answered, but there are still some other important opportunities to be discovered regarding the structure of sampling according to the methodologies, which they seem to be the key factor for certain results.\nPS. Thanks Silvestrum Climate Associates for sharing this collection of data samples data from one of your real reforestation sites!"
  },
  {
    "objectID": "posts/stats_final/index.html#references",
    "href": "posts/stats_final/index.html#references",
    "title": "An Statistical Mangrove Analysis Post",
    "section": "References",
    "text": "References\n\nVerraadmin (2020) First Blue Carbon Conservation Methodology expected to scale up finance for Coastal Restoration & Conservation Activities, Verra. Available at: https://verra.org/first-blue-carbon-conservation-methodology-expected-to-scale-up-finance-for-coastal-restoration-conservation-activities (Accessed: December 8, 2022). \nVM0033 methodology for tidal wetland and Seagrass Restoration, v2.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0033-methodology-for-tidal-wetland-and-seagrass-restoration-v2-0/ (Accessed: December 8, 2022). \nVM0032 methodology for the adoption of sustainable grasslands through adjustment of fire and grazing, v1.0 (2022) Verra. Available at: https://verra.org/methodologies/vm0032-methodology-for-the-adoption-of-sustainable-grasslands-through-adjustment-of-fire-and-grazing-v1-0/ (Accessed: December 8, 2022). \nMcGuinness, K. (2005) Above- and below-ground biomass, and allometry, of four common northern ..., Research Gate. Available at: https://www.researchgate.net/publication/248899546_Above-_and_below-ground_biomass_and_allometry_of_four_common_northern_Australian_mangroves (Accessed: December 9, 2022).\nKauffman, and Donato, (2012) Center for International Forestry Research - CIFOR, Cifor. Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nCenter for International Forestry Research - CIFOR (no date). Available at: https://www.cifor.org/publications/pdf_files/WPapers/WP86CIFOR.pdf (Accessed: December 9, 2022).\nThe more you know: Mangroves (2020) Ripley’s Aquarium of Canada. Available at: https://www.ripleyaquariums.com/canada/the-more-you-know-mangroves/ (Accessed: December 8, 2022).\n\n\nSupporting Figures\n\nTest the power results with random samples\nHere, we have created a power table with the means and SD of your random table and run a power analysis with the selected random samples to test my results\nTest your results 25m2 vs 154m2 by changing the number of samples!\n\n\nShow code\nsamples = 12\nrandom_table_test <- mangrove_2017 |> \n  group_by(plot_size_m2) |> \n  slice_sample(n = samples)\n\npower_table_test <- random_table_test |> \n  group_by(plot_size_m2) |> \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |> \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_25m = power_table_test[[1,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_25m = power_table_test[[1,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_a = (random_mean_25m - random_mean_154m) / (as.numeric(sqrt(random_sd_25m^2/2 + random_sd_154m^2/2)))\n\n\nrandom_power_test <- pwr.t.test(d = random_effect_size_a, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\nprint(effect_size_a)\n\n\n[1] 1.584427\n\n\nShow code\nprint(random_effect_size_a)\n\n\n[1] 2.055626\n\n\nShow code\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = mean_difference_a, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = (random_mean_25m - random_mean_154m), \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_a,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_25m - random_mean_154m),2), \"(Mg/ha)\", \"\\nPower:\", round(random_power_test$power,2)))\n\n\n\n\n\nTest your results 77m2 vs 154m2 with 50 samples\n\n\nShow code\nsamples = 60\nrandom_table_test <- mangrove_2017 |> \n  group_by(plot_size_m2) |> \n  slice_sample(n = samples)\n\npower_table_test <- random_table_test |> \n  group_by(plot_size_m2) |> \n  summarise(tot_samples = n(),\n            tot_plot_count = length(unique(plot)),\n            mean_tree_c_ha_mg = mean(total_tree_mg_c_ha),\n            sd_tree_c_ha_mg = sd(total_tree_mg_c_ha)) |> \n  mutate(plot_size_m2 = c(\"25m2\", \"77m2\", \"154m2\"))\n\nrandom_mean_77m = power_table_test[[2,4]]\nrandom_mean_154m = power_table_test[[3,4]]\nrandom_sd_77m = power_table_test[[2,5]]\nrandom_sd_154m = power_table_test[[3,5]]\n\n#Using all the samples\n#Real Difference \nrandom_effect_size_b = (random_mean_77m - random_mean_154m) / (as.numeric(sqrt(random_sd_77m^2/2 + random_sd_154m^2/2)))\n\nrandom_power_test_b <- pwr.t.test(d = random_effect_size_b, \n                           n = samples, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\n\n\nprint(effect_size_b)\n\n\n[1] 0.6571471\n\n\nShow code\nprint(random_effect_size_b)\n\n\n[1] 0.7051203\n\n\nShow code\nggplot(random_table_test, aes(x= plot_size_m2, y = total_tree_mg_c_ha, col = plot_size_m2)) +\n  geom_point() +\n  geom_hline(aes(yintercept = effect_size_b, \n                 linetype = \"real\"), \n             color = \"gray50\", \n             size = .5) +\n  geom_hline(aes(yintercept = random_effect_size_b, \n                 linetype = \"random\"), \n             color = \"gray10\", \n             size = .5) +\n  scale_shape_manual(\"\", values= c(\"mean\" = \"+\")) +\n  scale_linetype_manual(name = \"Means Difference\", \n                        values = c(2, 2), \n                        guide = guide_legend(\n                          override.aes = list(\n                            color = c(\"gray50\", \"gray10\")))) +\n  labs(title = \"Total Carbon per Tree (Mg/ha)\",\n       subtitle = \"Random sample size\",\n       x = \"Plot Type\",\n       y = \"Total Tree Carbon (mg/ha)\",\n       caption = paste(\"Difference between the real sample means is:\",round(mean_difference_b,2),\"(Mg/ha)\" ,\"\\nDifference between the randomly relected samples mean is:\", round((random_mean_77m - random_mean_154m),2), \"(Mg/ha)\",\"\\nPower:\", round(random_power_test_b$power,2)))\n\n\n\n\n\n\n\nC) Power of just 154m2\n\n\nShow code\nd_154 = as.numeric(sqrt(sd_154m^2/2))\neffect_size_c = mean_154m / d77_154\n\n\npower_test_c <- pwr.t.test(d = round(effect_size_c,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_c)\n\n\n\n     Two-sample t test power calculation \n\n              n = 78.23115\n              d = 0.58\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nShow code\nplot(power_test_c)\n\n\n\n\n\n\n\nD) Power of just 77m2\n\n\nShow code\nd_77 = as.numeric(sqrt(sd_77m^2/2))\neffect_size_d = mean_77m / d_77\n\n\npower_test_d <- pwr.t.test(d = round(effect_size_d,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_d)\n\n\n\n     Two-sample t test power calculation \n\n              n = 15.08404\n              d = 1.36\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nE) Power of just 25m2\n\n\nShow code\nd_25 = as.numeric(sqrt(sd_25m^2/2))\neffect_size_e = mean_25m / d_25\n\n\npower_test_e <- pwr.t.test(d = round(effect_size_e,2), \n                           power = 0.95, \n                           sig.level = 0.05,\n                           type=\"two.sample\",\n                           alternative=\"two.sided\")\nprint(power_test_e)\n\n\n\n     Two-sample t test power calculation \n\n              n = 9.759221\n              d = 1.73\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "posts/texas_outage/index.html",
    "href": "posts/texas_outage/index.html",
    "title": "Texas Outage",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nThe tasks in this post are: - Estimating the number of homes in Houston that lost power as a result of the first two storms\n- Investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau.\n\n\n\nLoad vector/raster data\n\nSimple raster operations\n\nSimple vector operations\n\nSpatial joins"
  },
  {
    "objectID": "posts/texas_outage/index.html#data",
    "href": "posts/texas_outage/index.html#data",
    "title": "Texas Outage",
    "section": "Data",
    "text": "Data\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\n\n\nRoads\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shape file of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouses\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "posts/texas_outage/index.html#assignment",
    "href": "posts/texas_outage/index.html#assignment",
    "title": "Texas Outage",
    "section": "Assignment",
    "text": "Assignment\n\nFind locations of blackouts\nFor improved computational efficiency and easier interoperability with sf, we will use the stars package for raster handling.\n\n\n\nShow code\n# Setting my filepaths\nrootdir <- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndatatif <- file.path(rootdir,\"data\",\"VNP46A1\")\ndata <- file.path(rootdir,\"data\")\n\n#Creating the names for each file\nnightlight1 <- 'VNP46A1.A2021038.h08v05.001.2021039064328.tif' \nnightlight2 <- 'VNP46A1.A2021038.h08v06.001.2021039064329.tif' \nnightlight3 <- 'VNP46A1.A2021047.h08v05.001.2021048091106.tif'\nnightlight4 <- 'VNP46A1.A2021047.h08v06.001.2021048091105.tif'\n  \n#Downloading the raster data to a star object\none <- read_stars(file.path(datatif, nightlight1))\ntwo <- read_stars(file.path(datatif, nightlight2))\nthree <- read_stars(file.path(datatif,nightlight3))\nfour <- read_stars(file.path(datatif,nightlight4))\n\n\n\n\nShow code\n#Combine tiles to have the full size\nlights_off <- st_mosaic(one,two)\nlights_on <- st_mosaic(three,four)\n\nplot(lights_off, main= \"Satellite Image of Houston Feb 7th\")\n\n\ndownsample set to 6\n\n\n\n\n\nShow code\nplot(lights_on, main= \"Satellite Image of Houston Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nCreate a blackout mask\n\nFind the change in night lights intensity (presumably) caused by the storm\nResponse: The change on the mean from the second date (All lights) goes from 13.86 down 12.13939, on the first date (Outrage)\nReclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nAssign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\n\n\n\nShow code\n#Create a new raster that has the difference of the values from the Feb 7th (Lights Off) raster, and the 16th (Lights On) raster. This will just have the difference of the attribute value on each pixel\n\nraster_diff <-  lights_off - lights_on\n\nplot(raster_diff, main= \"Difference in light intensity from Feb 7th and Feb 16th\")\n\n\ndownsample set to 6\n\n\n\n\n\n\n\nVectorize the mask\n\nUsing st_as_sf() to vectorize the blackout mask\nFixing any invalid geometries by using st_make_valid\n\n\n\nShow code\n# Converts the non-spatial star object (.tif) file to an sf object. An sf object will have an organizing structure that will have the geom column and the layer as attribues\nblackout <- st_as_sf(raster_diff)\nsummary(blackout)\n\n\n VNP46A1.A2021038.h08v05.001.2021039064328.tif          geometry    \n Min.   :  200.0                               POLYGON      :24950  \n 1st Qu.:  262.0                               epsg:4326    :    0  \n Median :  359.0                               +proj=long...:    0  \n Mean   :  543.8                                                    \n 3rd Qu.:  563.0                                                    \n Max.   :65512.0                                                    \n\n\n\n\nCrop the vectorized map to our region of interest.\n\nDefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nTurn these coordinates into a polygon using st_polygon\nConvert the polygon into a simple feature collection using st_sfc() and assign a CRS\n\nhint: because we are using this polygon to crop the night lights data it needs the same CRS\n\nCrop (spatially subset) the blackout mask to our region of interest \nRe-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nShow code\n#Create vectors for the desired CRS\nlon = c(-96.5, -96.5, -94.5, -94.5,-96.5)\nlat = c(29, 30.5, 30.5, 29, 29)\n\n\n#Create an array or matrix with those vectors \ncoordinates_array <-cbind(lon,lat)\n\n#Creating a polygon with the st_polygon function but the coordinates_array has to be in the form of a list so the st_polygon can read it.\nhouston_polygon <- st_polygon(list(coordinates_array))\n\n# Create a simple feature geometry list column, and add coordinate reference system so you can \"speak\" the same language than your recent blackout object\nhouston_geom <- st_sfc(st_polygon(list(coordinates_array)), crs = 4326)\n\n# Indexing or Cropping the blackout sf object with just the houston geometery polygon. \n\nhouston_blackout_subset <- blackout[houston_geom,]\n\n# Re-project the cropped blackout dataset with a new CRS (EPSG:3083) (NAD83 / Texas Centric Albers Equal Area)\nhouston_projection <- st_transform(houston_blackout_subset,\"EPSG:3083\")\n\n\n\n\nShow code\nplot(houston_blackout_subset, main = \"Blackout pixels in Houston\")\n\n\n\n\n\n\n\nExclude highways from blackout mask\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\n\nDefine SQL query\nLoad just highway data from geopackage using st_read\nReproject data to EPSG:3083\nIdentify areas within 200m of all highways using st_buffer\nFind areas that experienced blackouts that are further than 200m from a highway\n\n\n\nShow code\n#Reading the data with format .gpkg\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways <- st_read(file.path(data, \"gis_osm_roads_free_1.gpkg\"), query = query)\n\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'' from data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n\nShow code\n#Transform that new sf object with the CRS we have been using, so we stay in the same space.\nhighways_3083 <- st_transform(highways, \"EPSG:3083\")\n\n\n\n\nShow code\n#Create a buffer that contains all the roads, including the 200 meters across the roads.\n# BUT, to make a buffer object we need to first create a union of all those rows (highways), so we can use the st_buffer function...\nhighways_union <- st_union(highways_3083)\nhighways_buffer <- st_buffer(x = highways_union,\n          dist = 200)\n\nplot(highways_buffer, main = \"Highways in Houston\")\n\n\n\n\n\nShow code\n# Use the buffer to subtract those entire pixels or \"rows\" from the houston projection sf object.\nhouston_out <- houston_projection[highways_buffer, op = st_disjoint]\nplot(houston_out,\n     main= \"Houston Without the Roadlights\")\n\n\n\n\n\n\n\nFind homes impacted by blackouts\n\nLoad buildings dataset using st_read and the following SQL query to select only residential buildings\nhint: reproject data to EPSG:3083\n\n\n\nShow code\n#Read the data\nquery2 <- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\nbuildings <- st_read(file.path(data, \"gis_osm_buildings_a_free_1.gpkg\"), query = query2)\n\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')' from data source `/Users/javipatron/Documents/MEDS/Courses/eds223/data/gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\n\n\nFind homes in blackout areas\n\nFilter to homes within blackout areas.\nTip: You can use [ ] option to subtract those geometries from the building sf object that do not correspond to the Houston area. Or you can use the st_filter.\nCount number of impacted homes.\nTip: The total number of impacted houses when using st_filter or [ ] is 139,148. This could vary depending on how the st_filter considers the limits and borders of the geometries. The default method when using st_filter is st_intersects.\n\n\n\nShow code\n#Count number of houses\ndim(homes_blackout_join)[1]\n\n\n[1] 139148\n\n\n\n\nInvestigate socioeconomic factors\nLoad ACS data\n\nUse st_read() to load the geodatabase layers\nGeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\nIncome data is stored in the X19_INCOME layer\nSelect the median income field B19013e1\nhint: reproject data to EPSG:3083\n\n\n\n\nShow code\n#Read the data and understand the data layers\nst_layers(file.path(data, \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"))\n\n\nDriver: OpenFileGDB \nAvailable layers:\n                            layer_name geometry_type features fields crs_name\n1                      X01_AGE_AND_SEX            NA     5265    719     <NA>\n2                             X02_RACE            NA     5265    433     <NA>\n3        X03_HISPANIC_OR_LATINO_ORIGIN            NA     5265    111     <NA>\n4                         X04_ANCESTRY            NA     5265    665     <NA>\n5         X05_FOREIGN_BORN_CITIZENSHIP            NA     5265   1765     <NA>\n6                   X06_PLACE_OF_BIRTH            NA     5265   1221     <NA>\n7                        X07_MIGRATION            NA     5265   1793     <NA>\n8                        X08_COMMUTING            NA     5265   2541     <NA>\n9  X09_CHILDREN_HOUSEHOLD_RELATIONSHIP            NA     5265    263     <NA>\n10      X10_GRANDPARENTS_GRANDCHILDREN            NA     5265    373     <NA>\n11    X11_HOUSEHOLD_FAMILY_SUBFAMILIES            NA     5265    781     <NA>\n12      X12_MARITAL_STATUS_AND_HISTORY            NA     5265    759     <NA>\n13                       X13_FERTILITY            NA     5265    399     <NA>\n14               X14_SCHOOL_ENROLLMENT            NA     5265    779     <NA>\n15          X15_EDUCATIONAL_ATTAINMENT            NA     5265    715     <NA>\n16         X16_LANGUAGE_SPOKEN_AT_HOME            NA     5265    871     <NA>\n17                         X17_POVERTY            NA     5265   3941     <NA>\n18                      X18_DISABILITY            NA     5265    893     <NA>\n19                          X19_INCOME            NA     5265   3045     <NA>\n20                        X20_EARNINGS            NA     5265   2185     <NA>\n21                  X21_VETERAN_STATUS            NA     5265    565     <NA>\n22                     X22_FOOD_STAMPS            NA     5265    243     <NA>\n23               X23_EMPLOYMENT_STATUS            NA     5265   1625     <NA>\n24         X25_HOUSING_CHARACTERISTICS            NA     5265   4415     <NA>\n25                X27_HEALTH_INSURANCE            NA     5265   1593     <NA>\n26       X28_COMPUTER_AND_INTERNET_USE            NA     5265    385     <NA>\n27           X29_VOTING_AGE_POPULATION            NA     5265     35     <NA>\n28                      X99_IMPUTATION            NA     5265    783     <NA>\n29             X24_INDUSTRY_OCCUPATION            NA     5265   2107     <NA>\n30                  X26_GROUP_QUARTERS            NA     5265      3     <NA>\n31                 TRACT_METADATA_2019            NA    35976      2     <NA>\n32         ACS_2019_5YR_TRACT_48_TEXAS Multi Polygon     5265     15    NAD83\n\n\n\n\nDetermine which census tracts experienced blackouts.\n\nJoin the income data to the census tract geometries\nhint: make sure to join by geometry ID\nSpatially join census tract data with buildings determined to be impacted by blackouts\nFind which census tracts had blackouts\n\n\n\nShow code\n#Create a big sf that has the income information and its geometry\nincome_geom <- left_join(texas_geom, texas_income, by = \"GEOID_Data\")\n\n#Create an new sf that adds the income information to just the blackout sf object that we previously had. In this step I learned that if you use st_join will give you a different number and result than st_filter or [ ]\n\n#blackout_income_join <- st_join(homes_blackout_join, income_geom)\nblackout_join <- income_geom[homes_blackout_join,]\n\n#Print those Census Tracts a had blackouts. Im printing the number and the name\nlength(unique(blackout_join$NAMELSAD))\n\n\n[1] 711\n\n\nShow code\n#unique(blackout_join$NAMELSAD)\n\n\n\n\nCompare incomes of impacted tracts to unimpacted tracts.\n\nCreate a map of median income by census tract, designating which tracts had blackouts\nPlot the distribution of income in impacted and unimpacted tracts\nWrite approx. 100 words summarizing your results and discussing any limitations to this study\n\n\n\nShow code\n#Creating a list of tracts and counties that were affected by the blackout\nblackout_tracts <- unique(blackout_join$TRACTCE)\nblackout_counties <- unique(blackout_join$COUNTYFP)\n\n#Creating a Data Frames that includes only the rows that have the county affected by using the geom from blackout_tracts. One with the Counties and the other one with the Tracts.\ntracts_affected <- income_geom |> \n  filter(TRACTCE %in% blackout_tracts)\n\ncounties_affected <- income_geom |> \n  filter(COUNTYFP %in% blackout_counties)\n\n#Create a map were the base is the counties of Houston, then fill the color with the income_median column we created with \"B19013e1\", and then highlight the counties that were had building affected from our dataset\nmap <- tm_shape(counties_affected) +\n  tm_fill(col = \"income_median\", palette = \"BrBG\") +\n  tm_borders() +\n  tm_shape(blackout_join) +\n  tm_fill(col = \"pink\", alpha= 0.3) +\n  tm_layout(legend.outside = T,\n            main.title = \"Median Income by Census Tract\",\n            frame = T,\n            title = \"*Affected areas in pink*\") +\n  tm_compass(type = \"arrow\", \n             position = c(\"left\", \"top\")) +\n  tm_scale_bar()\n\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nShow code\nmap\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\n\n\n\nShow code\n#Finding the difference in income between the affected tracts and unaffected.\n\n#First we need create two sf data frames to categorize if they were impacted or not by the blackout\n\nnot_impacted <- anti_join(tracts_affected , as.data.frame(blackout_join)) |> \n  mutate(impacted = \"no\")\n\n\nJoining, by = c(\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"GEOID\", \"NAME\", \"NAMELSAD\",\n\"MTFCC\", \"FUNCSTAT\", \"ALAND\", \"AWATER\", \"INTPTLAT\", \"INTPTLON\", \"Shape_Length\",\n\"Shape_Area\", \"GEOID_Data\", \"income_median\", \"Shape\")\n\n\nShow code\nimpacted <- blackout_join |> \n  mutate(impacted = \"yes\")\n\n#Second, we need to create a new data frame were the \"key\" is the same column name, so we will have which tracts were impacted and which ones were not.\ncombination <- rbind(not_impacted, impacted) |> \n  dplyr::select(income_median, impacted)\n\nsummary(combination)\n\n\n income_median      impacted                   Shape    \n Min.   : 13886   Length:741         MULTIPOLYGON :741  \n 1st Qu.: 43749   Class :character   epsg:3083    :  0  \n Median : 60414   Mode  :character   +proj=aea ...:  0  \n Mean   : 71330                                         \n 3rd Qu.: 89796                                         \n Max.   :250001                                         \n NA's   :25                                             \n\n\nShow code\n#Third, create a histogram and a box plot of that new column and analyse the income median \n\nggplot(combination, aes(x = income_median, fill= impacted)) +\n  geom_histogram()  +\n    labs(title = \"Distribution of Income\",\n       x = \"Income Median\",\n       y = \"Count\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 25 rows containing non-finite values (stat_bin).\n\n\n\n\n\nShow code\nggplot(combination, aes(y = income_median, fill = impacted)) +\n  geom_boxplot() +\n  labs(title = \"Blackout Impact by Income\",\n       x = \"Impacted\",\n       y = \"Income Median\") +\n   theme(\n    panel.background = element_rect(fill = \"gray91\"),\n    panel.grid.major = element_line(colour = \"grey70\", size = 0.2),\n    panel.grid.minor = element_line(color = \"gray88\"))\n\n\nWarning: Removed 25 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nShow code\n# Statistical comparison\nsummary(impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  13886   43749   60642   71462   90013  250001       3 \n\n\nShow code\nsummary(not_impacted$income_median)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25755   42793   53442   59586   63670  134500      22 \n\n\nShow code\nt.test(impacted$income_median, not_impacted$income_median)\n\n\n\n    Welch Two Sample t-test\n\ndata:  impacted$income_median and not_impacted$income_median\nt = 0.9696, df = 7.2121, p-value = 0.3636\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -16915.65  40668.75\nsample estimates:\nmean of x mean of y \n 71462.30  59585.75"
  },
  {
    "objectID": "posts/texas_outage/index.html#summary",
    "href": "posts/texas_outage/index.html#summary",
    "title": "Texas Outage",
    "section": "Summary",
    "text": "Summary\nThe results show a high number people affected by this winter storm. According to the data, almost 140,000 houses/ building were impacted within the designated area of Houston showing an almost equal impact between high income census tracts and low income census tracts. Looking in detail at the plots, the distribution plot (Histogram), is a right skew, showing a higher density around the low income median, nevertheless in the summary results you can see that the median income for the not impacted census tracts is lower that the impacted census tract, showing that the effect of the storm affected almost equally everyone in the zone, regardless of the income. It is important to consider that we are not weighting each tract by impacted houses. Which could affect final results, and conclusions."
  },
  {
    "objectID": "posts/eez_spatial/index.html",
    "href": "posts/eez_spatial/index.html",
    "title": "Marine Aquaculture",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, bottom depth .2\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters.\nBased on previous research, we know that oysters needs the following conditions for optimal growth:\n\nSea surface temperature: 11-30 °C\nDepth: 0-70 meters below sea level\n\n\n\n\nCombining vector/raster data\nResampling raster data\nMasking raster data\nMap algebra"
  },
  {
    "objectID": "posts/eez_spatial/index.html#data",
    "href": "posts/eez_spatial/index.html#data",
    "title": "Marine Aquaculture",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).3\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\n\n\nPrepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\n\nLoad necessary packages and set path\n\nI recommend using the here package\n\nRead in the shape file for the West Coast EEZ (wc_regions_clean.shp)\nRead in SST rasters\n\naverage_annual_sst_2008.tif\naverage_annual_sst_2009.tif\naverage_annual_sst_2010.tif\naverage_annual_sst_2011.tif\naverage_annual_sst_2012.tif\n\nCombine SST rasters into a raster stack\nRead in bathymetry raster (depth.tif)\nCheck that data are in the same coordinate reference system\n\nReproject any data not in the same projection\n\n\n\n\n\nShow code\n# Setting my filepaths\nrootdir <- (\"/Users/javipatron/Documents/MEDS/Courses/eds223\")\ndata <- file.path(rootdir,\"data\",\"assignment4\")\n\n# Creating the names for each file\nsst_2008 <- 'average_annual_sst_2008.tif' \nsst_2009 <- \"average_annual_sst_2009.tif\"\nsst_2010 <- \"average_annual_sst_2010.tif\"\nsst_2011 <- \"average_annual_sst_2011.tif\"\nsst_2012 <- \"average_annual_sst_2012.tif\"\ndepth <- \"depth.tif\"\n  \n# Downloading the raster data to a star object\nsst_2008 <- rast(file.path(data, sst_2008))\nsst_2009 <- rast(file.path(data, sst_2009))\nsst_2010 <- rast(file.path(data, sst_2010))\nsst_2011 <- rast(file.path(data, sst_2011))\nsst_2012 <- rast(file.path(data, sst_2012))\nwc_regions <- st_read(file.path(data, \"wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `/Users/javipatron/Documents/MEDS/Courses/eds223/data/assignment4/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nShow code\ndepth <- rast(file.path(data,depth))\n\n# Stack all the raster\nall_sst <- c(sst_2008, sst_2009, sst_2010, sst_2011, sst_2012)\n\n\n# Check coordinate reference system\n#st_crs(wc_regions)\n#st_crs(depth)\n#st_crs(all_sst)\n\n\n# Set the new CRS\nall_sst <- project(all_sst, \"EPSG:4326\")\nall_sst\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 5  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : memory \nnames       : averag~t_2008, averag~t_2009, averag~t_2010, averag~t_2011, averag~t_2012 \nmin values  :      278.8167,      278.0800,      279.9200,      278.8600,      278.1920 \nmax values  :      301.4321,      301.4475,      300.9486,      307.2733,      309.8502 \n\n\nPrint the West Cost Polygon Vector data to see how it looks like:\n\n\nShow code\ntm_shape(wc_regions) +\n  tm_polygons(col=\"rgn\",\n              palette= \"RdYlBu\",\n              legend.reverse = T,\n              title = \"EEZ West Coast Regions\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass()\n\n\nSome legend labels were too wide. These labels have been resized to 0.44, 0.45, 0.49. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger.\n\n\n\n\n\nShow code\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing"
  },
  {
    "objectID": "posts/eez_spatial/index.html#process-data",
    "href": "posts/eez_spatial/index.html#process-data",
    "title": "Marine Aquaculture",
    "section": "Process Data",
    "text": "Process Data\nNext, we need process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions. We don’t want to change the underlying depth data, so we will need to resample to match the SST data using the nearest neighbor approach.\n\nFind the mean SST from 2008-2012\nConvert SST data from Kelvin to Celsius\nCrop depth raster to match the extent of the SST raster\n\nnote: the resolutions of the SST and depth data do not match\n\nResample the depth data to match the resolution of the SST data using the nearest neighbor approach.\nCheck that the depth and SST match in resolution, extent, and coordinate reference system. Can the rasters be stacked?\n\n\n\nShow code\n#Finding the mean \nmean_sst <- terra::app(all_sst, mean)\n\n#Converting to Celsius\nmean_sst_c <- (mean_sst - 273.15)\n\n#Cropping the Depth to just the area of SST\ncrop_depth <- crop(depth, mean_sst)\n\nclass(crop_depth)\n\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nShow code\n# Re-sample the depth with the needed resolution\nnew_depth <- terra::resample(crop_depth, mean_sst_c, method = \"near\")\n\n# Stack both rasters and see if they match\nstack_depth_sst <- c(mean_sst_c, new_depth)\n\n\n\nFind suitable locations\nIn order to find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\n\nReclassify SST and depth data into locations that are suitable for Lump sucker fish\n\nhint: set suitable values to 1 and unsuitable values to NA\n\nFind locations that satisfy both SST and depth conditions\n\nhint: create an overlay using the lapp() function multiplying cell values\n\n\n\n\n\nShow code\n#Create the matrix for Temperature between 11°C and 30°C\ntemp_vector <- c(-Inf, 11, NA, \n                   11, 30, 1,\n                   30, Inf, NA)\n\ntemp_oysters_matrix <- matrix(temp_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of °C\ntemp_oysters <- classify(mean_sst_c, temp_oysters_matrix)\n\n\n#Create the matrix for Depth between 0 & -70\ndepth_vector <- c(-Inf, -70, NA, \n                   -70, 0, 1,\n                   0, Inf, NA)\n\ndepth_oysters_matrix <- matrix(depth_vector, ncol= 3, byrow = T)\n\n# Reclassify the SST raster of depth\ndepth_oysters <- classify(new_depth, depth_oysters_matrix, include.lowest = T)\n\n# Combine the two raster\nmatrixes <- c(depth_oysters, temp_oysters)\n\n# Rename the matrixes attributes\nnames(matrixes) <- c(\"temp_matx\",\"depth_matx\")\n\n#Combine both matrixes\ncombined_matrix_stack <- c(matrixes, stack_depth_sst)\n\n\n\n\nShow code\n#Find the locations where the oysters have a 1 in the pixel.\ncheck_condition <- function(x,y){\n  return(x * y)\n   }\n\ntemp_conditions <- lapp(combined_matrix_stack[[c(1,3)]], fun = check_condition)\ndepth_conditions <- lapp(combined_matrix_stack[[c(2,4)]], fun = check_condition)\n\ntm_shape(temp_conditions) +\n  tm_raster(title = \"Sea Temp °C\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\n\n\n\n\nShow code\ntm_shape(depth_conditions) +\n  tm_raster(title = \"Depth (Meters)\") +\n  tm_graticules(alpha = 0.3) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE,\n            frame = T)\n\n\nCompass not supported in view mode.\n\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\nNow lets create the mask of both conditions\n\n\nShow code\nsuitable_conditions <- lapp(matrixes[[c(1,2)]], fun = check_condition)\nprint(suitable_conditions)\n\n\nclass       : SpatRaster \ndimensions  : 480, 408, 1  (nrow, ncol, nlyr)\nresolution  : 0.04165905, 0.04165905  (x, y)\nextent      : -131.9848, -114.9879, 29.99208, 49.98842  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : memory \nname        : lyr1 \nmin value   :    1 \nmax value   :    1"
  },
  {
    "objectID": "posts/eez_spatial/index.html#determine-the-most-suitable-eez",
    "href": "posts/eez_spatial/index.html#determine-the-most-suitable-eez",
    "title": "Marine Aquaculture",
    "section": "Determine the most suitable EEZ",
    "text": "Determine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nSelect suitable cells within West Coast EEZs\nFind area of grid cells\nFind the total suitable area within each EEZ\n\nhint: it might be helpful to rasterize the EEZ data\n\nFind the percentage of each zone that is suitable\n\nhint it might be helpful to join the suitable area by region onto the EEZ vector data\n\n\n\n\nShow code\ncell_ezz <- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n\nrast_ezz <- rasterize(wc_regions, suitable_conditions, field= 'rgn')\nmask_ezz <-  mask(rast_ezz, suitable_conditions)\nsuitable_area <- zonal(cell_ezz, mask_ezz, sum )\n\njoined_area <-  left_join(wc_regions, suitable_area, by = 'rgn') |> \n  mutate(area_suitkm2 = area,\n         percentage = (area_suitkm2 / area_km2) * 100,\n         .before = geometry)"
  },
  {
    "objectID": "posts/eez_spatial/index.html#visualize-results",
    "href": "posts/eez_spatial/index.html#visualize-results",
    "title": "Marine Aquaculture",
    "section": "Visualize results",
    "text": "Visualize results\nNow that we have results, we need to present them!\nCreate the following maps:\n\nTotal suitable area by region\nPercent suitable area by region\n\n\n\nShow code\ntm_shape(joined_area) +\n  tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Total Suitable area per EEZ for: Oysters\"),\n            frame = T)\n\n\n\n\n\n\n\n\n\nShow code\ntm_shape(joined_area) +\n  tm_polygons(col = \"percentage\", palette = \"RdYlBu\", legend.reverse = T,\n              title = \"Percent (%)\") +\n  tm_shape(wc_regions) +\n  tm_polygons(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            main.title.size = 1,\n            main.title = paste(\"Suitable Area per EEZ for: Oysters \"),\n            frame = T)"
  },
  {
    "objectID": "posts/eez_spatial/index.html#conclusion",
    "href": "posts/eez_spatial/index.html#conclusion",
    "title": "Marine Aquaculture",
    "section": "Conclusion",
    "text": "Conclusion\nAs you can see in the maps above the most suitable areas for the oysters are the northerner regions with a 3 to 3.5 % of the entire EEZ region. As the assignments states the sea surface temperature that the oysters like are between the 11-30 °C, and for the depth is between 0-70 meters below sea level, which are pretty specific. Thus, this results can give us hints on how the Economic Exclusive Zones help those species, and we can compare to other EEZ or other species of the US West Coast."
  },
  {
    "objectID": "posts/eez_spatial/index.html#broaden-your-workflow",
    "href": "posts/eez_spatial/index.html#broaden-your-workflow",
    "title": "Marine Aquaculture",
    "section": "Broaden your workflow!",
    "text": "Broaden your workflow!\nNow that you’ve worked through the solution for one group of species, let’s update your workflow to work for other species. Please create a function that would allow you to reproduce your results for other species. Your function should be able to do the following:\n\n\nAccept temperature and depth ranges and species name as inputs\n\nCreate maps of total suitable area and percent suitable area per EEZ with the species name in the title\n\n\nRun your function for a species of your choice! You can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\n\n\nShow code\nfind_my_happy_place <- function(species = \"NAME\", min_temp = 5, max_temp = 30, min_depth = 0, max_depth = -5468) {\n  temp_vector <- c(-Inf, min_temp, NA, min_temp, max_temp, 1,max_temp, Inf, NA)\n  temp_matrix <- matrix(temp_vector, ncol= 3, byrow = T)\n  temp_condition <- classify(mean_sst_c, temp_matrix)\n  depth_vector <- c(-Inf, max_depth, NA, max_depth, min_depth, 1, min_depth, Inf, NA)\n  depth_matrix <- matrix(depth_vector, ncol= 3, byrow = T)\n  depth_condition <- classify(new_depth, depth_matrix, include.lowest = T)\n  mix_rasters <- c(depth_condition, temp_condition)\n  suitable_conditions <- lapp(mix_rasters[[c(1,2)]], fun = check_condition)\n  cell_ezz <- cellSize(suitable_conditions, unit = 'km', transform = TRUE)\n  rast_ezz <- rasterize(wc_regions, suitable_conditions, field= 'rgn')\n  mask_ezz <-  mask(rast_ezz, suitable_conditions)\n  suitable_area <- zonal(cell_ezz, mask_ezz, sum )\n  joined_area <-  left_join(wc_regions, suitable_area, by = 'rgn') |>\n    mutate(happy_area_km2 = area,\n           \"happy_(%)\" = (happy_area_km2 / area_km2) * 100,\n           .before = geometry) |> \n    arrange(desc(happy_area_km2))\n  map <- tmap_arrange(tm_shape(joined_area) +\n                        tm_polygons(col = \"area\", palette = \"RdYlBu\", legend.reverse = T, title = \"Area (km2)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Total Suitable area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T),\n                        tm_shape(joined_area) +\n                        tm_polygons(col = \"happy_(%)\", palette = \"RdYlBu\", legend.reverse = T,\n                                    title = \"Percent (%)\") +\n                        tm_shape(wc_regions) +\n                        tm_polygons(alpha = 0.5) +\n                        tm_layout(legend.outside = TRUE,\n                                  main.title = paste(\"Suitable Area per EEZ for:\", species),\n                                  main.title.size = 1,\n                                  frame = T))\n  print(joined_area[c(\"rgn\", \"rgn_key\", \"area_km2\", \"happy_area_km2\", \"happy_(%)\",\"geometry\")])\n  print(paste(\"*Conclusion:* For the species\", species, \"the most suitable region is\", joined_area$rgn[1], \"with\", round(joined_area$happy_area_km2[1],2), \"km2 of 'happy' area.\"))\n  \n  map\n}\n\n\n\nNow Test your function\nREMEMBER:\n1. The species name has to be in quotes. The Default value is “NAME”.\n2. The depth has to include the negative sign for the maximum depth.\nDefault; Min: 0, Max: -5468.\n3. The Temperature is in °C .\nDefault; Min: 5, Max: 30).\n\n\nShow code\nfind_my_happy_place()\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2 happy_(%)\n1 Southern California    CA-S 206860.78      205730.95  99.45382\n2  Central California    CA-C 202738.33      201478.95  99.37882\n3              Oregon      OR 179994.06      178650.46  99.25353\n4 Northern California    CA-N 164378.81      162975.61  99.14636\n5          Washington      WA  66898.31       64695.95  96.70790\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-123.4318 4...\n4 MULTIPOLYGON (((-124.2102 4...\n5 MULTIPOLYGON (((-122.7675 4...\n[1] \"*Conclusion:* For the species NAME the most suitable region is Southern California with 205730.95 km2 of 'happy' area.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow code\n# Try your species!! \nfind_my_happy_place(\"Turtle\", 13, 28, 0, -290)\n\n\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key  area_km2 happy_area_km2  happy_(%)\n1 Southern California    CA-S 206860.78     10890.3024 5.26455645\n2  Central California    CA-C 202738.33       360.9648 0.17804468\n3          Washington      WA  66898.31        29.1610 0.04359004\n4              Oregon      OR 179994.06             NA         NA\n5 Northern California    CA-N 164378.81             NA         NA\n                        geometry\n1 MULTIPOLYGON (((-120.6505 3...\n2 MULTIPOLYGON (((-122.9928 3...\n3 MULTIPOLYGON (((-122.7675 4...\n4 MULTIPOLYGON (((-123.4318 4...\n5 MULTIPOLYGON (((-124.2102 4...\n[1] \"*Conclusion:* For the species Turtle the most suitable region is Southern California with 10890.3 km2 of 'happy' area.\""
  },
  {
    "objectID": "posts/blog_example_1/index.html",
    "href": "posts/blog_example_1/index.html",
    "title": "Blog Post Title",
    "section": "",
    "text": "Hi this my first class blog post, and this is my first graph in R.\nSHARK\n\n\n\n\n\nIn this page will also help me visualize my references, cites, changes, and edits to official posts in my webpage.\nHere is some more text. Cite: (Csik 2022)\nWhen you want to reference stuff. Then in this same pager click in “Insert v”, then click in citation… then copy and paste the url\n\n\n\n\nReferences\n\nCsik, Samantha. 2022. “Adding a Blog to Your Existing Quarto Website.” October 24, 2022. https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/.\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Javier Patrón},\n  title = {Blog {Post} {Title}},\n  date = {2022-10-24},\n  url = {https://javipatron.github.io/posts/blog_example_1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJavier Patrón. 2022. “Blog Post Title.” October 24, 2022.\nhttps://javipatron.github.io/posts/blog_example_1/."
  },
  {
    "objectID": "posts/ethics-post/index.html",
    "href": "posts/ethics-post/index.html",
    "title": "Project Chaac Dives into the Pool of Ethical Sense",
    "section": "",
    "text": "Data is a new language in which we read, write, and transform the figures our eyes cannot naturally see, hidden in our surrounding reality. However, thanks to the technological revolution, those “natural numbers” can now be deciphered and manipulated by people who are fortunate and deft enough to speak and understand such language. \nRephrasing a definition of data coined by Stefania Milan “Data is seen as an avenue to revert or challenge our dominant understandings of the world, (re)creating conditions of possibility for counter-imaginaries and social justice claims to emerge.” With that in mind, In this blog post, I will discuss how data experts would be able to generate more consciousness on ethics and bias while developing a more sensitive touch in their daily work as environmental data scientists. To help express these thoughts on paper, I will use an actual environmental project held in the Yucatán Peninsula of Mexico and discuss how responsible and equitable data scientists should behave when exercising the power of data to design a world case example out of this project. \nMy musings expressed in this post are meant to awaken present and future “data evangelists”, motivating them to nourish a better sense of community within their field, and understanding the substantial change that both environment and society will experience through Project Chaac. This reflection will be based on the skills and foundations I have acquired throughout the course of Ethics and Bias in Environmental Data Science at the Bren School of Environmental Science & Management. \nAt its core, Project Chaac seeks to protect, conserve, and restore a territory of 42,676 hectares of degraded mangroves in the southern Mexican state of Yucatán. The project involves a significant amount of team management that must be aligned with overall environmental impact. Worldwide, most of the lands and territories targeted for greenhouse gas (GHG) mitigation action overlap with areas customarily held by indigenous groups, local communities, and Afro-descendant peoples. \nMoreover, most of the lands and forests targeted for nature-based greenhouse gas removal and offset, like Project Chaac, are located within areas where the State’s customary rights of communities have yet to be recognized (Reference two). Therefore, I will briefly describe some difficulties and potential roadblocks involving ethics and bias that the project might face in Mexico, since its complexity resides in the entangled path of multiple factors and circumstances at constant play. Metaphorically speaking, this project is a living, growing vine, in which the roots are essential; biodiversity stands for the open minds involved; key nutrients represent human values, and the environment as a whole is the high frequency vibe of all the passionate individuals that will provide a firm and rich soil to plant the seed of change. Finally, we will analyze how this tiny seed can become a fully grown, jade-green turquoise plant, as astonishing as the Strongylodon Macrobotrys.\n\n\n\nImage 1: How does a seed germinate?\n\n\nAs with all plants, a careful assessment of the seed germination and the creation of concise strategies will have an exponential impact on the growth and direction of the project. Here are the three axes on which I foresee true potential for building solid bridges between ethics and bias resides:\n\nData Justice.- Satellite images are used to select the pieces of land with the highest potential for mangrove restoration in the area. Therefore an equitable allocation of carbon credits rights, land use, and business strategy must be well cemented among all players involved: Federal and local authorities, as well as the private sector, along with the rural communities called ejidos, as well as the territories inhabited by Afro-descendants. (Reference four)\nProject Development.- The actual value of the land comes when you dip your hands in the dirt while sweating big fat droplets on the rich soil, learning and adapting to your surroundings. In this case, the project implies complicated engineering analysis to build an efficient network of hydrological channels in the near future, in order to embrace a healthy and protected ecosystem in the restored mangrove lands.\n\n\n\n\nImage 2: Hydrological construction and rehabilitation of canals. Photo: Primary Production Laboratory (CINVESTAV- IPN)\n\n\n\nLegal Feasibility.- Carbon accounting in developing countries is facing difficult but also exciting times. Agreements regarding carbon rights will encounter critical roadblocks, and hence need to be exceptionally well-planned. Additionally, with climate change hot on our heels and the Paris Agreement goals swiftly approaching, countries and corporations are increasing their participation to meet their emission reduction targets and net-zero commitments 5. Keeping that in mind, embracing and understanding Mexican laws will become a crucial element for the success of the project.\n\nNow, I would like to define what Blue Carbon is and why it is so important. A newly developed concept, it refers to the carbon stocks sequestered in any coastal ecosystem, like mangroves, sea grasses, and salt marshes. Mangroves are considered one of the most productive and biologically complex ecosystems on earth; that is why they are also regarded as Nature’s “superheroes” fighting against climate change. \nHowever, over the last few years, mangroves have been deforested at an alarming rate, due to increased coastal development and other anthropogenic factors. Thus, conservation and restoration of these coastal ecosystems are essential both to sustain the natural environment and to assure their decisive impact on our world. As with coral reefs, mangrove forests allow the development of true environmental health by fostering fisheries and healthy coastal ecosystems, as well as by providing significant protection from natural disasters.\nNext, I will introduce the topic of Data Justice and why it is relevant to this project and blog post. Data Justice happens whenever data relies on insufficient recognition to understand real community needs. It is an approach that addresses new ways of data collection and dissemination of crucial facts which have been invisibilized in the past, thus harming marginalized communities (Reference six). Therefore, it plays an essential role in carbon-credit adjudications. In environmental science, carbon stocks represent the total amount of organic carbon stored within an environmental system, so the correct manipulation of data is key to quantifying such processes. Hence, data scientists play a vital role in elegant calculations in the pursuit of data justice.\nRegarding carbon rights in Mexico, I am presenting two tables to describe some general carbon trade legal agreements. Carbon rights are used to describe a number of different tradeable GHG prerogatives. In other words, it is the given right to benefit from sequestered carbon and/or reduced greenhouse gas emissions.” (Reference two)\n\n\nShow code\n# Add the tables in a nice format\n\n\n\nTable 1. Legal frameworks to support carbon-linked transactions.\n\n\n\n\n\n\n\n\n\n\nCountry\nLaws for securing community tenure to forests?\nEstablished a national legal framework for carbon trade?\nDefined carbon rights?\nAre carbon rights linked to tenure?\nDo communities have carbon rights?\n\n\n\n\nMexico\nYes\nPartial\nInconclusive or undefined\nInconclusive or undefined\nInconclusive or undefined\n\n\n\n\nTable 2. Legal recognition of community land.\n\n\n\n\n\n\n\n\nCountry\nCountry area where rights of Indigenous  peoples, local communities, and Afro-descendants are legally recognized\nCountry area where rights of Indigenous peoples, local communities, and Afro-descendants are not legally recognized\nTotal percent of land held by Indigenous peoples, local communities, and Afro-descendants\n\n\n\n\nMexico\n52 %\n0.50%\n52.5 %\n\n\n\nAs the tables clearly depict, Mexico has an inconclusive law on carbon rights. However, almost half of its territory is legally recognized when referring to its indigenous peoples, local communities, and Afro-descendants, which is today a leading example among developing countries.\nSo now we have an overview of the project’s objectives. However, how and where does data manipulation impact its success? \nCurrently, the CINVESTAV (The Center for Research and Advanced Studies of the National Polytechnic Institute), a non-governmental Mexican research institution, has settled and defined the actual conditions of the mangroves by using satellite imagery. After collecting images from the Sentinel-2A 7 from March 2021 to June 2022, and by using a powerful environmental data science tool called NDVI (Normalized Difference Vegetation Index), they were able to quantify and define the areas with higher and lower vegetation density health. In other words, they designated the patches of land in specific areas of interest, where vegetation reflects green back to space (meaning healthy mangroves), comparing such areas against those showing degraded or deforested vegetation, (which reflected longer color wavelengths).\n\n\n\n\nImage 3: NDVI Formula\n\n\nBy using the NDVI tool, a language that only a few people know about (remember “potential bias”?), the CINVESTAV team defined those sections of land with a higher probability of mangrove reforestation along with the potential for the development of a potential carbon accounting business. As you can imagine, such results designate an important plant tutor, which sets the standard for launching critical collaborative agreements with land owners, state governments, private companies, and ejidos.\nNowadays, technology has become a potent influential tool across the globe, but it can also be scary and threatening to those who do not understand it. Consequently, I invite all involved parties, especially data scientists, to think outside the box from the very beginning and bring the terminology of Data Justice into play. \nIn contrast, if a project lacks a strategy of environmental data justice or fails to recognize an equitable carbon right among indigenous people and/or local communities, it will surely generate a negative domino effect with huge implications, leaving long-lasting scars for all future environmental nature-based initiatives.\nThroughout the course of Ethics and Bias, I learned the importance of becoming data-sensitive. I am one of those scientists who are fortunate enough to speak the language of data, having developed a sense for reading and hearing his surroundings in order to find a deeper connection with their heart and soul, ensuring that the local wisdom moves from one generation to the next, honoring its ancient knowledge as an invaluable heritage. I wish to become a key player in communicating the importance of this work, quickly generating new allies within the communities. Undoubtedly, data sensitivity is the vital foundation for building a solid ethical environment.\nUnfortunately, only some countries explicitly recognize community carbon rights, and even fewer have tested established rules’ operational and political feasibility. However, Project Chaac has all the tools to become a world case example of a well-organized project, although a topnotch level of data sensitivity, knowledge on carbon rights, data justice, and firm management must be assured to bring all the pieces together.\nOverall, the success of this mangrove restoration project depends heavily on a vigorous commitment between all players involved: Collaborators, government agents, lawyers, field engineers, data scientists, and ejido leaders, among others. These 40,000 hectares of land are heavily threatened by tourism and pressure from the governmental instances. Yet, I am certain that Project Chaac represents a golden opportunity to show the world how a nature-based solution with a well-cemented ethical culture is definitely possible. All of the aforementioned factors will empower our incipient vine to evolve into an emerald wonder that will make the world’s eyes widen in awe, aiming to the skies.\n\n\n\nImage 4: The beautiful blue jade vine (Strongylodon Macrobotrys)\n\n\nReferences:\n1. Stefania, S. (2019) Full article: Exploring Data Justice: Conceptions, applications and ..., Exploring Data Justice: Available at: https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1606268 (Accessed: December 8, 2022).\n2. Initiative, A.R.and R. (2020) Rights-based conservation: The path to preserving Earth’s biological and cultural diversity?, Rights + Resources. Available at: https://rightsandresources.org/publication/rights-based-conservation/ (Accessed: December 7, 2022).\n3. Strongylodon macrobotrys (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Strongylodon_macrobotrys (Accessed: December 7, 2022).\n4. Ejido (2022) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Ejido (Accessed: December 7, 2022).\n5. Hood, C. (2020) Completing the Paris ‘rulebook’: Key article 6 issues, Center for Climate and Energy Solutions. Available at: https://www.c2es.org/document/completing-the-paris-rulebook-key-article-6-issues/ (Accessed: December 7, 2022).\n6. Taylor, L. (2017). What is Data Justice? The case for connecting digital rights and freedoms globally. Big Data & Society.https://doi.org/10.1177/2053951717736335.\n7. Home (no date) Sentinel. Available at: https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a (Accessed: December 7, 2022).\n\n\n\n\n\nCitationBibTeX citation:@online{patrón2022,\n  author = {Javier Patrón},\n  title = {Project {Chaac} {Dives} into the {Pool} of {Ethical} {Sense}},\n  date = {2022-12-19},\n  url = {https://github.com/javipatron},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJavier Patrón. 2022. “Project Chaac Dives into the Pool of Ethical\nSense.” December 19, 2022. https://github.com/javipatron."
  },
  {
    "objectID": "posts/global_fishing_watch/index.html",
    "href": "posts/global_fishing_watch/index.html",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "",
    "text": "Javier Patrón jpatron@ucsb.edu\nJessica French jfrench@bren.ucsb.edu\nPol Carbó Mestre pcarbomestre@ucsb.edu\n\n\n\n\n\nPurpose(#purpose)\nDataset Description(#overview)\nData I/O(#io)\nMetadata Display and Basic Visualization(#display)\nUse Case Examples(#usecases)\nCreate Binder Environment(#binder)\nReferences\n\n\n\n\nThe purpose of this notebook is to explore Global Fishing Watch’s dataset showing daily fishing effort as inferred fishing hours daily. This notebook will show how to read in the dataset, visualize the data using google earth engine, and give an overview of how the data can be used to explore differences in fishing effort within and outside Peru’s EEZ and how fishing effort is impacted by El Niño Southern Oscillation (ENSO) events.\n\n\n\nThe Global Fishing Watch (GFW) provides an open platform to access Automatic Identification System (AIS) data from commercial fishing activities. The AIS is a tracking system that uses transceivers on ships to broadcast vessel information such as unique identification, position, course, and speed. AIS is integrated into all classes of vessels as a collision avoidance tool. However, the GFW collects and processes raw AIS data related to fishing activities to improve records and assign additional information, such as the distance from shore, depth, etc. Then, with the use of machine learning models, they characterize vessels and fishing activities, which constitute some of the products available in their API.\n\n\n\n\n\nOne of the most interesting products that the GFW API offers is estimates of fishing effort derived from AIS data. GFW uses machine learning models to classify fishing vessels and predict when they are fishing. First, they identify fishing vessels in the AIS system. Then the vessel is characterized using the best available information and their fleet registry data combined with a characterization model trained on 45,441 marine vessels (both fishing and non-fishing) matched to official fleet registries. Then, GFW estimates the vessel’s fishing time and location based on its movement patterns. To do so, a fishing detection model was trained on AIS data from 503 ships and identified fishing activity with >90% accuracy. The model predicts a score for every AIS position in their database to distinguish fishing positions from non-fishing positions (i.e., navigation time). When the model identifies fishing activity, the time associated with that AIS position is registered as apparent fishing activity. More details about the model can be found on the following GitHub repository (link).\n\n\n\n\n\n\n\n\n\n\nOnce the fishing vessels are identified and their AIS positions have been assigned as fishing positions, the apparent fishing effort can be calculated for any area by summarizing the fishing hours for all fishing vessels in that area. The resulting maps of apparent fishing effort are created by rasterizing all AIS positions into a grid and calculating the total fishing activity in each grid cell. For the present project we will access this type processed data.\nPre-processed AIS data can be accessed from their R package “gfwr” or downloaded from their website as .cvs files. For this project, we will use some of their existing products related to fishing effort. The data can be accessed from Google Big Query in a less processed format and through Google Earth Engine (GEE) for two data subproducts daily fishing hours and daily vessel hours. For accessibility reasons, we will focus on the GEE data related to fishing hours.\nEach image in the collection contains daily rasters of fishing effort measured in hours of inferred fishing activity per square kilometer. Data is available for a given flag state and day, over a 5 years period (2012-2017), where each band represent a fishing gear type. The following figure summarizes the image resolution and the available bands.\nThe data used belongs to the first global assessment of commercial fishing activity, published in Science by GFW (2018).\n\n\n\nLoad necessary packages.\n\n\nCode\n#1\n# Import packages\nimport ee\nimport geemap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nfrom itertools import chain\nimport os\n\n\nAuthenticate and initialize google earth engine.\n\n\nCode\n#2\n# Authenticate google earth engine\n#ee.Authenticate()\n# Initialize google earth engine \nee.Initialize()\n#Read in the data from google earth engine and filter metadata to include all countries."
  },
  {
    "objectID": "posts/global_fishing_watch/index.html#metadata-display",
    "href": "posts/global_fishing_watch/index.html#metadata-display",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Metadata Display",
    "text": "Metadata Display\nWe can look at the metadata and bands in a couple different ways. The code below is a little overwhelming and difficult to search through.\n\n\nCode\n#7\n# Extract the first image so we can look at info about the data in general. \nimage_test = dataset.first()\ninfo = image_test.getInfo()\nprint(info)\n\n\n{'type': 'Image', 'bands': [{'id': 'drifting_longlines', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'fixed_gear', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'other_fishing', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'purse_seines', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'squid_jigger', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}, {'id': 'trawlers', 'data_type': {'type': 'PixelType', 'precision': 'float'}, 'dimensions': [36000, 16500], 'crs': 'EPSG:4326', 'crs_transform': [0.01, 0, -180, 0, -0.01, 85]}], 'version': 1518594814620050.0, 'id': 'GFW/GFF/V1/fishing_hours/ARG-20120101', 'properties': {'system:time_start': 1325376000000, 'country': 'ARG', 'system:footprint': {'type': 'LinearRing', 'coordinates': [[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]]}, 'system:time_end': 1325462400000, 'system:asset_size': 731, 'system:index': 'ARG-20120101'}}\n\n\nThis code allows you to see only the band names and numbers in a more readable format.\n\n\nCode\n#8\ndef get_image_ids(gee_snipet):\n  x = ee.ImageCollection(gee_snipet)\n  first_image = x.first()\n  bands_list = first_image.bandNames()\n  lista = bands_list.getInfo()\n  for i, val in enumerate(lista):\n    print(i,val)\n    \n#Example\nprint(get_image_ids('GFW/GFF/V1/fishing_hours'))\n\n\n0 drifting_longlines\n1 fixed_gear\n2 other_fishing\n3 purse_seines\n4 squid_jigger\n5 trawlers\nNone\n\n\nTo read on the metadata we will first create an object image from the collection and index their properties. To built this first image we will create and use simple methods over the original data set of GEE.\n\n\nCode\n#9\n# Creating an image with some filters on time and space\nfishing_image = dataset \\\n    .filterBounds(aoi_2) \\\n    .first()\n\n# See image properties with their names and values\nfishing_props = geemap.image_props(fishing_image)\nfishing_props.getInfo()\n\n# Index by country\ncountry = fishing_props.get('country')\nprint('Country:', country.getInfo())\n\n#Represent the image properties with propertyNames()\nproperties = fishing_image.propertyNames()\nprint('Metadata properties:' , properties.getInfo())\n\n\nCountry: ARG\n\n\nMetadata properties: ['system:time_start', 'country', 'system:footprint', 'system:time_end', 'system:version', 'system:id', 'system:asset_size', 'system:index', 'system:bands', 'system:band_names']"
  },
  {
    "objectID": "posts/global_fishing_watch/index.html#metadata-csv-description",
    "href": "posts/global_fishing_watch/index.html#metadata-csv-description",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Metadata CSV Description:",
    "text": "Metadata CSV Description:\nThe GFW website allows us to access individual data sets of AIS-based fishing effort and vessel presence. These data sets are provided in daily .csv file format and include the same data that is available through the GFW’s Google Earth Engine (GEE) API. By examining these files, we can gain insight into the GEE API metadata, such as the number of fishing hours per cell and day, the fishing state flag, and the type of gear used. This information can help us understand how the data is structured on the GEE fishing hours data set.\n\n\nCode\n#10\n#Reading the CSV file with the attributes of the metadata\nraw_fishing_vessels = pd.read_csv('2016-01-01.csv')\nraw_fishing_vessels.head()\n\n\n\n\n\n\n  \n    \n      \n      date\n      cell_ll_lat\n      cell_ll_lon\n      flag\n      geartype\n      hours\n      fishing_hours\n      mmsi_present\n    \n  \n  \n    \n      0\n      2016-01-01\n      -4.00\n      -144.82\n      USA\n      fishing\n      0.2005\n      0.0\n      1\n    \n    \n      1\n      2016-01-01\n      -4.25\n      -144.92\n      USA\n      fishing\n      0.0750\n      0.0\n      1\n    \n    \n      2\n      2016-01-01\n      5.25\n      -138.54\n      USA\n      fishing\n      1.4925\n      0.0\n      1\n    \n    \n      3\n      2016-01-01\n      -5.75\n      -145.45\n      USA\n      fishing\n      0.0500\n      0.0\n      1\n    \n    \n      4\n      2016-01-01\n      -6.50\n      -164.41\n      USA\n      fishing\n      0.0333\n      0.0\n      1\n    \n  \n\n\n\n\nFrom that file, we can group, count, and sort the data to see which countries and gear types are most highly represented for that day.\n\n\nCode\n#11\n# Check for even representation of vessels\nprint(raw_fishing_vessels['flag'].value_counts().sort_values(ascending=False).head())\nprint(raw_fishing_vessels['geartype'].value_counts().sort_values(ascending=False).head())\n\n\nCHN    39022\nTWN     6297\nRUS     6164\nJPN     6130\nUSA     5213\nName: flag, dtype: int64\ntrawlers              38067\ndrifting_longlines    21721\nfishing               13557\nset_longlines          6244\ntuna_purse_seines      3758\nName: geartype, dtype: int64"
  },
  {
    "objectID": "posts/global_fishing_watch/index.html#peruvian-fisheries-use-case-example",
    "href": "posts/global_fishing_watch/index.html#peruvian-fisheries-use-case-example",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Peruvian Fisheries Use Case Example",
    "text": "Peruvian Fisheries Use Case Example\nPeru is the second largest producer of wild caught sea food in the world landing 5,658,917 tonnes in 2020 (FAO, 2022). The marine fishing sector is responsible for 232,000 jobs and $3.2 billion in GDP as of 2005 (Christensen et al., 2014). Peruvian fishers were some of the first people to recognize El Niño events through their impact on fisheries (El Niño | National Geographic Society). El Niño Southern Oscillation causes warmer ocean temperatures and a reduction in upwelling, reducing the number of anchoveta and fisheries production.\nOur case study focuses on using Global Fishing Watch (GFW) data on fishing hours to explore fishing activities inside and outside of Peru’s economic exclusive zone (EEZ). The Peruvian fisheries serve as a valuable example of how international law, including the 1982 United Nations Convention on the Law of the Sea, enables developing countries to retain control over their marine resources and maintain food sovereignty. This is especially critical as foreign fleets from across the globe often take advantage of the rich fishing resources and high productivity of Peruvian waters, which extend beyond the boundaries of the EEZ. By studying the data on fishing hours, we can gain a better understanding of the overall health and sustainability of the Peruvian fisheries. This information can provide valuable insights into the importance of the EEZ for Peru and how it helps to protect the country’s marine resources from being exploited by foreign fleets.\nIn addition to providing insights into the importance of the EEZ, our analysis will also investigate whether trends in fishing efforts can be used as a proxy to evaluate the impacts of El Niño events on the Peruvian fisheries. Although this constitutes a secondary aspect of our study, we will comment on the fishing fluctuations and whether they may be influenced by the El Niño Southern Oscillation (ENSO). El Niño events can be identified using the Ocean Niño Index (ONI), which is a 3-month running mean of ERSST.v5 sea surface temperature anomalies in the Niño 3.4 region (between 5 degrees North and South of the equator and between 120 and 170 degrees West). For this study, we will focus on the 2015 El Niño event and compare it to the data from the previous year. More information is available here\n\nVisualizing Peruvian fisheries\nThis Visualization map allows us to understand the significance of Exclusive Economic Zones, particularly in the case of Peru where the fishing industry contributes significantly to the country’s GDP. By using this map, we can compare the total fishing hours within and outside of Peru’s EEZ without considering the flag or gear type. This information is crucial for understanding the impact Peru’s EEZ for managing and regulating this sector.\n\n\nCode\n#12\n# Global fishing effort:\n## Aggregate 2016 collection to single image of global fishing effort\neffort_all = fishing_effort_ImgCollection.sum()\n## Sum bands to get total effort across gear types\neffort_all = effort_all.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_all = effort_all.mask(effort_all.gt(0))\n\n# Fishing effort in Peru's EEZ\neffort_EEZ = fishing_effort_ImgCollection.filterBounds(aoi_1).map(lambda image: image.clip(aoi_1))\n## Aggregate 2016 collection to single image of global fishing effort\neffort_EZZ = effort_EEZ.sum()\n## Sum bands to get total effort across gear types\neffort_EZZ = effort_EZZ.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_EZZ = effort_EZZ.mask(effort_EZZ.gt(0))\n\n# Visualization parameters\ntrawlersVis = {\n  'palette': ['0C276C', '3B9088', 'EEFF00', 'ffffff']\n}\n\n# Comparing the two maps\nleft_layer = geemap.ee_tile_layer(effort_EZZ, trawlersVis, name = \"All 2016\", opacity = 0.9)\nright_layer = geemap.ee_tile_layer(effort_all, trawlersVis, name = \"Effort EZZ\", opacity = 0.9)\nmap_comparison = geemap.Map(center = [-15, -83],zoom = 5)\nmap_comparison.centerObject(aoi_1, 5)\nmap_comparison.setOptions(\"HYBRID\")\nmap_comparison.addLayer(aoi_1, {\"color\": \"white\", \"width\": 1.5}, \"EEZ of Perú\");\nmap_comparison.split_map(left_layer, right_layer)\nmap_comparison\n\n\n\n\n\n\n\nGenerating a Gif of Monthly Fishing Effort off of Peru’s Coast.\nAnother aspect worth exploring is the evolution of the fishing effort across the years. The following code allows us to create an animated image of fishing activity in front of Peru. To do this, we will first aggregate the daily data into monthly fishing hour estimates. Then, we will sum up all band values to obtain information about total fishing. With the resulting images, we will create a .gif to visualize the temporal and spatial evolution of the fisheries.\n\n\nCode\n#13\n# monthly sum of fishing effort\ndef monthly_Sum (collection, years, months):\n  effort_sum = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('system:time_start',{'month': month, 'year': year})\n      Monthly_sum = Monthly_sum.mask(Monthly_sum.gt(0))\n      Monthly_sum = SRTM.blend(Monthly_sum)\n      effort_sum.append (Monthly_sum)\n  return ee.ImageCollection.fromImages(effort_sum)\n\n# list of images\ndef monthly_images (collection, years, months):\n  effort_images = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('system:time_start',{'month': month, 'year': year})\n      effort_images.append (Monthly_sum)\n  return effort_images\n\n\n\n\nCode\n#14\n# create list years and months for use in the function\nyears = range(2012, 2017)\nmonths = range(1, 13)\n\n\n\n\nCode\n#15\n# Create an image collection of fishing effort where each image is a sum of the fishing effort for that month. \neffort_collection = monthly_Sum(fishing_effort_ImgCollection, years, months)\n# Creates a list of the images from the image collection. \neffort_list = monthly_images(fishing_effort_ImgCollection, years, months)\nlen(effort_list)\n\n# Creates an empty list to be populated with month and year of images. \ndate_list = []\n\n# Populates the date_list with the month and year of each image. This will be used to annotate the gif with a time stamp. \nfor i in effort_list:\n  date = i.get('system:time_start').getInfo()\n  date_list.append(date)\n    \nlen(date_list)\n\n\n60\n\n\n\n\nCode\n#16\nsaved_gif = os.path.join(os.path.expanduser('~'), \"\".join([os.getcwd(),\"/img/monthly_fishing.gif\"]))\ngeemap.download_ee_video(effort_collection, videoArgs, saved_gif)\n\n\nGenerating URL...\n\n\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/74d674bad7b2624f834ab73885feff9d-5f3e2208107def4395d6994ef7e51323:getPixels\nPlease wait ...\n\n\nThe GIF image has been saved to: /Users/javipatron/Documents/MEDS/Personal/personal_webpage/javipatron.github.io/posts/global_fishing_watch/img/monthly_fishing.gif\n\n\n\n\nCode\n#17\nout_gif = os.path.join(os.path.expanduser('~'), \"\".join([os.getcwd(),\"/img/montly_fishing.gif\"]))\n\n\n\n\nCode\n#18\ngeemap.add_text_to_gif(\n    saved_gif,\n    out_gif,\n    xy=('3%', '5%'),\n    text_sequence=date_list,\n    font_size=20,\n    font_color='#ffffff',\n    duration=600\n)\n\n\n\n\nCode\n#19\ngeemap.show_image(out_gif)\n\n\n\n\n\nIn the previous .gif, we can observe a seasonal pattern where fishing grounds migrate from the north to the south along the coast and the EEZ borders, with months experiencing minimal activity. However, one limitation of the image the difficulty to discern trend changes across years, making it challenging to determine if ENSO events may be diminishing fishing activity.\n\n\nQuantifying fishing hours inside and outside the EEZ\nIn order to further analyze the observed fishing differences between the Exclusive Economic Zone (EEZ) and the region immediately next to it, we will extract temporal series of total fishing hours inside and outside the EEZ. This will provide us with a quantifiable measure to compare the fishing efforts in these two areas. Additionally, the temporal series may enable us to identify any patterns or trends in fishing effort that may be influenced by the 2015 El Niño event.\nSince the GEE dataset exceeds the allowed extraction size, we have created a function that calls for the data of interest in order to work with it in pandas. The following function allow us to agregate all daily fishing hours values per month. With it we get first a dataset with monthly data instead of daily. Once we have temporarilly aggregated the data, we apply a reducer to sum up all fishing gears in one band and get a unique value representing the total fishing hours.\n\n\nCode\n#20\n# Loop ranges already defined for the gif. \n# Function to extract data of interest:\n## .sum() Aggregates collections to single monthly image of global fishing effort\n## .reduce(ee.Reducer.sum()) Sum bands to get total effort across all gear types\n\ndef monthly_Sum (collection, years, months):\n  effort_sum = []\n  for year in years:\n    for month in months:\n      Monthly_sum = collection.filter(ee.Filter.calendarRange(year, year, 'year')) \\\n                              .filter(ee.Filter.calendarRange(month, month, 'month')) \\\n                              .sum() \\\n                              .reduce(ee.Reducer.sum()) \\\n                              .set('year', year).set('system:time_start', ee.Date.fromYMD(year, 1, 1))\\\n                              .set('month', month).set('system:time_start', ee.Date.fromYMD(year, month, 1).millis())                                                     \n      effort_sum.append (Monthly_sum)\n  return ee.ImageCollection.fromImages(effort_sum)\n\n\n\n\nCode\n#21\nbyMonth = monthly_Sum(fishing_effort_ImgCollection,years,months)\ntype(byMonth)\n\n\nee.imagecollection.ImageCollection\n\n\nOur updated image collection now includes monthly fishing data for all gears combined. In order to effectively analyze the time series of this data, we will need to design a new function that allows us to summarize the information within our designated areas of interest (AOIs). This function will take the AOI as a parameter, enabling us to conduct time series analysis on each AOI individually. The upcoming sections will focus on conducting time series analysis for each AOI.\n\nFishing inside the EEZ\nThe following time series correspond to the fishing data contained inside the EEZ. previously saved as aoi_1.\n\n\nCode\n#22\n# Function to summarize fishing monthly data:\n## Extracting all image values in the collection by the AOI relative to the EEZ\ndef aoi_sum(img):\n    sum = img.reduceRegion(reducer=ee.Reducer.sum(), geometry=aoi_1, # EEZ area of interest\n                           scale=1113.2) # Resolution of the GFW product\n    return img.set('time', img.date().millis()).set('sum',sum)\n\naoi_reduced_imgs = byMonth.map(aoi_sum)\nnested_list = aoi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['time','sum']).values().get(0)\n\n\n\n\nCode\n#23\n# Converting GEE data to Pandas and rearranging it for its representation\ndf = pd.DataFrame(nested_list.getInfo(), columns=['time','sum'])\ndf['sum'] = df['sum'].apply(lambda x: x.get('sum'))\ndf.rename(columns = {'sum':'total_fishing_hours'}, inplace = True)\ndf[\"id\"] = df.index \nfirst_column = df.pop('id')\ndf.insert(0, 'id', first_column)\n\n# Setting time format for representation purposes\ndf['datetime'] = pd.to_datetime(df['time'], unit='ms')\ndf['datetime'] = pd.to_datetime(df['datetime'],format=\"%Y%m%d\")\n#df.head()\n\n\n\n\nCode\n#24\nplt.figure(figsize=(10, 6), dpi=300)   # create a new figure, set size and resolution (dpi)\nplt.fill_between(df['datetime'],df['total_fishing_hours'])   # add data to the plot\nplt.title(\"Fishing hours inside Peru's EEZ\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\n\n\nText(0, 0.5, 'Total hours')\n\n\n\n\n\n\n\nFishing outside the EEZ\nTo characterize the fishing activity outside of the EEZ, we need to define a new area of interest. In this case, we will focus on a square region with the same latitude boundaries as the EEZ but extending towards the west in longitude (aoi_2). The first step is to quantify the number of fishing hours over that region. Then we will subtract the EEZ fishing hours previously calculated from it to graphically compare effort from inside and outside.\nThe map below shows the new area of interest and the data across a the 5-year time period that will be included in the time series analysis.\n\n\nCode\n#25\n# Fishing effort in Peru's EEZ\neffort_AOI2 = fishing_effort_ImgCollection.filterBounds(aoi_2).map(lambda image: image.clip(aoi_2))\n## Aggregate 2016 collection to single image of global fishing effort\neffort_AOI2 = effort_AOI2.sum()\n## Sum bands to get total effort across gear types\neffort_AOI2 = effort_AOI2.reduce(ee.Reducer.sum())\n## Mask out pixels with no effort\neffort_AOI2 = effort_AOI2.mask(effort_AOI2.gt(0))\n\n# Add the total fishing effort layer\nMap = geemap.Map(zoom = 2)\nMap.centerObject(aoi_2, 5)\nMap.setOptions('HYBRID');\nMap.addLayer(aoi_2, {'color': 'white','width': 1.5}, \"Sampling area\");\nMap.addLayer(effort_AOI2,trawlersVis);\nMap\n\n\n\n\n\n\n\nCode\n#26\n# Function to summarize fishing monthly data inside and outside the EEZ\ndef aoi_sum(img):\n    sum = img.reduceRegion(reducer=ee.Reducer.sum(), geometry=aoi_2, \n                           scale=1113.2)\n    return img.set('time', img.date().millis()).set('sum',sum)\n\naoi_reduced_imgs = byMonth.map(aoi_sum)\nnested_list = aoi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['time','sum']).values().get(0)\n\n# Converting GEE data to Pandas and rearranging it for its representation\ndf2 = pd.DataFrame(nested_list.getInfo(), columns=['time','sum'])\ndf2['sum'] = df2['sum'].apply(lambda x: x.get('sum'))\ndf2.rename(columns = {'sum':'total_fishing_hours'}, inplace = True)\ndf2[\"id\"] = df2.index \nfirst_column = df2.pop('id')\ndf2.insert(0, 'id', first_column)\ndf2['datetime'] = pd.to_datetime(df2['time'], unit='ms')\ndf2['datetime'] = pd.to_datetime(df2['datetime'],format=\"%Y%m%d\")\n\n# Ploting time series\nplt.figure(figsize=(10, 6), dpi=300)   \nplt.fill_between(df2['datetime'],df2['total_fishing_hours'])\nplt.title(\"Fishing hours inside the second AOI\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\n\n\nText(0, 0.5, 'Total hours')\n\n\n\n\n\nTo compare fishing efforts within and outside, we can subtract the total fishing hours from the previous data frames. This will give us the actual fishing effort outside the EEZ, which we can then represent alongside fishing within the EEZ in an area chart for easy visualization and comparison.\n\n\nCode\n#27\n# Combining both df\ndf[\"total_fishing_hours_outside\"] = abs(df[\"total_fishing_hours\"] - df2[\"total_fishing_hours\"])\ndf.rename(columns = {'total_fishing_hours':'total_fishing_hours_inside'}, inplace = True) \ntotal_fishing_df = df[['id', 'time', 'datetime','total_fishing_hours_inside','total_fishing_hours_outside']]\ntotal_fishing_df.head()\n\n\n\n\n\n\n  \n    \n      \n      id\n      time\n      datetime\n      total_fishing_hours_inside\n      total_fishing_hours_outside\n    \n  \n  \n    \n      0\n      0\n      1325376000000\n      2012-01-01\n      0.000000\n      160.185273\n    \n    \n      1\n      1\n      1328054400000\n      2012-02-01\n      29.597059\n      442.994487\n    \n    \n      2\n      2\n      1330560000000\n      2012-03-01\n      0.683282\n      512.164755\n    \n    \n      3\n      3\n      1333238400000\n      2012-04-01\n      7.479013\n      2290.269927\n    \n    \n      4\n      4\n      1335830400000\n      2012-05-01\n      11.036600\n      3353.737642\n    \n  \n\n\n\n\n\n\nCode\n#28\nplt.figure(figsize=(10, 6), dpi=300)   \nplt.stackplot(total_fishing_df['datetime'],\n              [total_fishing_df['total_fishing_hours_inside'], total_fishing_df['total_fishing_hours_outside']],\n              labels=['Inside the EEZ', 'Outside the EEZ'],\n              alpha=0.8)\nplt.title(\"Fishing hours in front of the coast of Perú\", fontsize=16)\nplt.xlabel('Date', fontsize=14)\nplt.ylabel('Total hours', fontsize=14)\nplt.legend(loc=2, fontsize='large')\nplt.show()\n\n\n\n\n\nThe orange region of the graph indicates the amount of fishing occurring in international waters, while the blue area represents the fishing that takes place within Peru’s exclusive economic zone (EEZ). It is likely that this fishing is carried out by Peru’s own fleet or by other countries with which Peru has bilateral fishing agreements. From the data, it is clear that the majority of fishing effort occurs outside of Peru’s EEZ, with some instances exceeding 30,000 hours. This highlights the importance of the EEZ, as it protects Peru’s resources from being over exploited by foreign fleets. Without the EEZ, foreign fleets would be able to legally access and deplete Peru’s waters.\nOn the other hand, there is a distinct pattern of fishing activity that tends to peak during the summer and fall months, and decrease during the winter. This trend is consistent both inside and outside the Exclusive Economic Zone (EEZ). Additionally, it appears that there has been an increase in fishing hours when comparing the first two years of data with the last three years. However, this could be attributed to better availability of AIS data over the years rather than any actual changes in fishing patterns. Furthermore, there is no clear evidence of an influence of ENSO events on fishing activity. In fact, values for 2014 and 2015 are relatively similar, indicating that fishing hours may not be the most effective way to measure the impact of atmospheric and oceanic oscillations on fisheries.\n\n\n\nImproving our analysis\nOverall, the notebook has successfully introduced the GFW datasets and its access through GEE. However, continuing the analysis could provide further insights into the study case.\nWe could improve our analysis in several ways. For instance, we could investigate the contribution of different fishing flags to the total fishing effort both inside and outside the EEZ. This would allow us to identify which international fleets are fishing in Peru’s waters and evaluate the countries that have fishing agreements with Peru. We could also compare total navigation hours (a data set also available from GEE) with fishing hours to establish how busy the waters are within our areas of interest.\nFurthermore, we could delve deeper into exploring the effects of ENSO. Initially, we could have included a time series of the ENSO indicator from the 3-month mean of ERSST.v5 sea surface temperature anomalies in the Niño 3.4 region, which we did not incorporate as we did not detect any influence. In this regard, we could have explored additional GFW data sets (accessible through Google BigQuery) representing fishing activities using indicators influenced by El Niño."
  },
  {
    "objectID": "posts/global_fishing_watch/index.html#binder-environment",
    "href": "posts/global_fishing_watch/index.html#binder-environment",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "Binder Environment",
    "text": "Binder Environment\nThe content of the present notebook can be accessed using Binder’s services for building and sharing reproducible and interactive computational environments from online repositories.\nClick this link to launch the notebook in your browser:\n\n\n\nBinder"
  },
  {
    "objectID": "posts/global_fishing_watch/index.html#references",
    "href": "posts/global_fishing_watch/index.html#references",
    "title": "Analyzing the Economic Exclusive Zone in Perú",
    "section": "References",
    "text": "References\n\nChristensen, V., de la Puente, S., Sueiro, J. C., Steenbeek, J., & Majluf, P. (2014). Valuing seafood: The Peruvian fisheries sector. Marine Policy, 44, 302–311. https://doi.org/10.1016/j.marpol.2013.09.022\nEl Niño | National Geographic Society. (n.d.). Retrieved November 29, 2022, from https://education.nationalgeographic.org/resource/el-nino\nFAO. Fishery and Aquaculture Statistics. Global capture production 1950-2020 (FishStatJ). 2022. In: FAO Fisheries and Aquaculture Division [online]. Rome. Updated 2022.\nGFW (global fishing watch) daily fishing hours | Earth Engine Data catalog | google developers (no date) Google. Google. Available at: https://developers.google.com/earth-engine/datasets/catalog/GFW_GFF_V1_fishing_hours (Accessed: November 30, 2022).\nGlobal Fishing Watch Application Programming Interfaces (API) Documentation (https://globalfishingwatch.org/our-apis/documentation#introduction)\nKroodsma, David A., Juan Mayorga, Timothy Hochberg, Nathan A. Miller, Kristina Boerder, Francesco Ferretti, Alex Wilson et al. “Tracking the global footprint of fisheries.” Science 359, no. 6378 (2018): 904-908. DOI:10.1126/science.aao5646."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Javier Patrón",
    "section": "",
    "text": "Data Science Master´s Student\n\n\nEnvironmental Engineer"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blogs",
    "section": "",
    "text": "Analyzing the Economic Exclusive Zone in Perú\n\n\n\nMEDS\n\n\nPython\n\n\nOcean\n\n\n\nAssessing Trends in Fishing Effort Inside and Outside Peru’s EEZ Using AIS Data from Global Fishing Watch.\n\n\n\nJavier Patrón\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Chaac Dives into the Pool of Ethical Sense\n\n\n\nMEDS\n\n\nMangroves\n\n\n\nAn Ethics and Bias thought\n\n\n\nJavier Patrón\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Aquaculture\n\n\n\nMEDS\n\n\nSpatial\n\n\nOcean\n\n\nR\n\n\n\nDetermining which Exclusive Economic Zones (EEZ) on the US West Coast are best suited to developing marine aquaculture for ocean species like Oysters.\n\n\n\nJavier Patrón\n\n\nDec 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Statistical Mangrove Analysis Post\n\n\n\nMEDS\n\n\nStatistics\n\n\nR\n\n\nMangroves\n\n\n\nA blue carbon analysis based on the skills learned from the course of Statistics for Environmental Data Science at the Bren School of Environmental Science & Management\n\n\n\nJavier Patrón\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas Outage\n\n\n\nMEDS\n\n\nSpatial\n\n\nR\n\n\n\nAnalyzing the Effects of the Texas February 2021 Storm on the Houston metropolitan Area\n\n\n\nJavier Patrón\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Title\n\n\n\nMEDS\n\n\nR\n\n\nPython\n\n\nOcean\n\n\n\nWhat my first post looks like\n\n\n\nJavier Patrón\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "BIO",
    "section": "",
    "text": "After graduating from Universidad Iberoamericana as a Mechanical and Electrical Engineer in Mexico City, I started my professional career as a Field Engineer at General Electric. I was the pioneer commissioner and field engineer for the smart lighting product development in the Latin America region. After 5 years, the GE Current smart controls and energy management software grew significantly and I ended up as the team leader for the global commissioning and tech-support team.\nThen in 2021, I shifted my career to become the team headman and navigator of a racing sailboat to cross the Pacific. During this time, I volunteered in a coral restoration project in Fiji, where the collection of data was proven to be key for effective restoration practices. This ocean crossing adventure was a life-changing experience and fueled my enthusiasm to apply for the Masters of Environmental Data Science at the Bren School UCSB.\nI´m an energetic and proactive individual. My vision as an Environmental Data Scientist is to empower high-end technology companies by improving their monitoring practices on data collection and drawing actionable insights to foster a positive environmental impact. Ultimately, I aspire to work on natural climate solutions, carbon offset, and MPA restoration and conservation.\n\n\n\nImage: Coral Farming. Mamanuca Islands, Fiji"
  }
]